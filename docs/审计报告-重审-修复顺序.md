# LLMux 重审报告（链路重建 + 修复顺序）

基线：`main@4c4f0e5`（2026-01-14）  
目标标准：企业级开源、单体/分布式双模式的高性能 LLM 网关（运维友好、开发者友好、高可用、默认安全）。

本报告不复述 `docs/审计报告-未重新审计.md` 的全部验证笔记，而是在“怀疑原结论是否成立/是否值得修”的前提下：
- 重新梳理真实代码链路（admin/data plane、AuthN/AuthZ、缓存/路由/观测、上游请求）。
- 对旧审计项逐条复核“是否仍存在”。
- 给出从先到后的修复顺序，并标注已落地/暂不修复项。

---

## 0. 结论摘要（先讲结论）

LLMux 的系统性风险不在“零散 bug”，而在“默认边界/默认策略是否足够安全 + 多租户隔离是否贯穿状态组件”。截至 `main@4c4f0e5`：
- 管理面/控制面/静态 UI 已从数据面分离：`server.admin_port=0` 时不再把管理路由注册到 data mux（默认安全姿态显著提升）。
- 管理面已具备最低可用的 AuthZ 门禁（KeyTypeManagement / 管理角色），OIDC 与 API Key middleware 的冲突也已解除。
- 多租户共享状态组件的核心风险已落地修复：缓存 key、路由 stats、RR key TTL、遗留 Prometheus 高基数标签等。
- 仍需要产品/文档决策的项主要集中在：插件安全边界、外部 callback 的默认采集策略、双重限流职责边界、UI 认证闭环等。

---

## 1. 代码链路（重建）

### 1.1 入口与路由分面
- `cmd/server/main.go`：加载 `internal/config` → 构建 `llmux.Client` → 构建 data/admin mux（`cmd/server/routes.go`） → 组装中间件（`cmd/server/middleware.go`）。
- `cmd/server/routes.go`：仅当 `server.admin_port>0` 才创建 `Admin` mux 并注册管理/控制/UI；否则仅注册 data routes。
- 管理路由清单：`internal/api/routes.go`（含 `/key/* /team/* /user/* /organization/* /spend/* /audit/* /control/* /invitation/* /mcp/*`）。

### 1.2 中间件栈（AuthN / AuthZ / 限制 / 观测）
- `cmd/server/middleware.go`：
  - management body size 限制（DoS 基础护栏）。
  - 管理面 AuthZ：要求 `AuthContext` +（管理 role 或 `KeyTypeManagement`），并支持 bootstrap 初始化路径。
- `internal/auth/middleware.go`：API Key 认证（enabled 才启用），并在已存在 `AuthContext` 时直接放行（OIDC 可独立工作）。
- `internal/auth/oidc.go`：OIDC 认证；与 API Key middleware 组合后不再互相“二次拦截”。
- `internal/observability/requestid.go`：对 `X-Request-ID` 做长度/字符集约束，不再完全信任客户端输入。

### 1.3 Client 核心链路（缓存 / streaming / 上游请求）
- 缓存：`client.go: (*Client).generateCacheKey` 现已包含租户 scope + 更完整请求字段覆盖，并使用 SHA-256（降低跨租户命中与投毒面）。
- Streaming：`stream.go` 使用独立 `streamHTTPClient`（避免 global Timeout 截断）；支持 recovery，并已增加 recovery backoff + 内存上限。
- 上游请求：provider 构造已做 URL escape；BaseURL 默认阻断私网/loopback（允许显式放开）。

---

## 2. 旧审计项复核：是否仍存在？

### 2.1 `审计报告-未重新审计.md`（R1-R8）
- R1 管理面落在 data port：已修复（`admin_port=0` 不再把管理路由注册到 data mux）。
- R2 KeyType 定义未 enforce：已修复（管理面 AuthZ 要求 `KeyTypeManagement` 或管理角色）。
- R3 “distributed 配置默认禁用 governance”：原结论不严谨（审计引用的配置样本不具代表性）；真实问题是“治理/访问控制的落地路径与文档/默认值一致性”。建议降级为 P2（见第 4 节）。
- R4 OIDC 与 API Key middleware 冲突：已修复（API Key middleware 检测到 `AuthContext` 时放行）。
- R5 Streaming 16KB chunk 上限：已修复（scanner buffer 上限已提高）。
- R6 Streaming 被全局 Timeout 截断：已修复（stream 使用专用 HTTP client，不设置全局 Timeout）。
- R7 双重限流（governance + client）：偏产品设计（租户级 vs provider 级），建议明确职责边界与可配置开关（见第 4 节）。
- R8 Invitation endpoints 不可达：已修复（生产 server 已接入 invitation 路由，并补齐 Postgres 持久化与迁移）。

### 2.2 本轮新增/补充项（已落地）
- Streaming recovery：增加 backoff（避免重试风暴）+ 增加 `stream.max_accumulated_bytes`（避免累积内容无限增长）。
- Legacy Prometheus 指标：对 `model` label 做规范化/截断/字符集收敛，降低 label 高基数 DoS 风险。

---

## 3. 修复顺序（从先到后）

本仓库已按“先 P0 再 P1/P2”的顺序落地了关键修复；以下以“企业级默认安全”为准给出顺序与状态：

### P0（必须修复；已落地）
1) 管理面/data plane 分离（`admin_port=0` 不注册管理路由）。
2) 管理面 AuthZ 门禁（KeyType/role/bootstrap 初始化路径）。
3) 多租户隔离：缓存 key tenant scope + SHA-256 + 覆盖字段完善。
4) 多租户隔离：路由 stats / RR key TTL 等共享状态的租户化与治理。
5) Invitation 路由接入生产链路 + Postgres 持久化迁移（`004_invitation_links.sql`）。
6) Streaming：不再被全局 Timeout 截断；提高 scanner buffer；recovery 增加 backoff + 累积内容上限。
7) 安全输出：非 LLMError 不再把 `err.Error()` 直接回传给客户端。
8) Legacy metrics：`model` label 收敛（规避高基数 DoS 的最低成本护栏）。

### P1（建议继续；多为默认策略/运维友好性）
1) 明确“双重限流”的职责边界与最佳实践（文档 + 配置示例），避免误配置导致双倍限流/容量评估错误。
2) 外部 callback（Langfuse/Datadog 等）默认采集策略：建议默认不上传 messages/response，改为显式 opt-in（需产品取舍）。
3) UI：补齐鉴权闭环（当前 UI 侧更像“管理 API 客户端壳”，需明确登录/会话策略）。
4) 迁移治理：为 Postgres schema 维护一条“强一致迁移序列”的运行手册（避免 drift 回归）。

### P2（暂不修复；需要更大架构/产品决策）
1) 插件安全边界：插件是同进程可执行代码，不可能做强沙箱；建议把“可信插件”写入威胁模型与文档，并逐步引入只读 ContextView/受控 capability。
2) 更激进的默认安全姿态（例如默认强制 `auth.enabled=true` 才能启用管理面）：会影响现有用户使用方式，建议在大版本变更窗口处理。

---

## 4. 本轮落地的关键提交（便于追踪）
- Invitation 生产接入 + Postgres 持久化：`c245bd0`
- Streaming recovery backoff + 累积上限：`a738508`
- Legacy metrics model label 收敛：`4c4f0e5`

