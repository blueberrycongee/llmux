# LLMux 重审报告（以质疑态度重建链路与修复顺序）

基线：`ed4f36149b3124d6be8f39caa7fedecf81b5b6a8`  
当前分支快照：`fix/admin-port-management`（截至 `a0d4744`）  
目标标准：企业级开源单体/分布式双模式 LLM 高性能网关（运维友好、开发者友好、高可用、默认安全）。

本报告不复述 `docs/审计报告-未重新审计.md` 的全部验证笔记，而是在“怀疑原结论是否成立/是否值得修”的前提下，重建真实代码链路与风险优先级，并给出**从先到后**的修复顺序（含“暂不修复”项）。

---

## 0. 结论摘要（先讲结论）

LLMux 的系统性风险主要来自“默认边界/默认策略”是否足够安全，而不是单点 bug：

1) **控制面/管理面与数据面边界**：`admin_port=0` 时管理/控制/UI 与数据面共享 mux，必须用明确的 AuthZ 门禁兜底；否则一旦被暴露在同一入口，后果是“默认裸奔”。  
2) **AuthN 与 AuthZ 必须分层**：API Key / OIDC 只能解决“你是谁”，管理端点还要解决“你能做什么”。  
3) **多租户隔离必须贯穿缓存/路由/观测**：分布式场景下任何“全局共享的 state”（缓存 key、router stats、RR key、metrics label）都会成为跨租户串扰与投毒入口。  
4) **观测默认低基数、低敏**：Prometheus 维度一旦引入 `user/end_user/api_key` 类字段，几乎必然演化成 DoS 与合规风险。

该分支已把“高置信 P0/P1”集中落地（见第 2 节），当前剩余项更多偏“产品策略与默认配置取舍”（仍需继续收敛）。

---

## 1. 代码链路重建（从入口到上游）

### 1.1 进程入口与路由分面

- 入口：`cmd/server/main.go`  
  - 读取配置（`internal/config/config.go`），创建 `llmux.Client`，创建网关 handler（`internal/api/client_handler.go`）。
  - `buildMuxes`（`cmd/server/routes.go`）在 `admin_port>0` 时创建 `Admin` mux；否则把管理/控制/UI 注册到 Data mux。
- Data mux：`registerDataRoutes`（`cmd/server/routes.go`）注册 `/v1/*`、`/health/*`、`/metrics`（若 `metrics.enabled`）。
- Admin mux：`registerAdminRoutes`（`cmd/server/routes.go`）注册管理/控制端点（`internal/api/routes.go`）+ UI 静态资源。

结论：**admin_port=0** 时 Data 与 Admin 合并，因此“管理面 AuthZ 兜底”必须视为生产必需能力。

### 1.2 中间件栈（Auth / OIDC / 管理面兜底 / metrics / request-id / CORS）

- 栈构建：`cmd/server/middleware.go`
  - 管理端点 body limit（`server: limit management request bodies`）
  - 管理端点 AuthZ 门禁（management 角色 / KeyTypeManagement / bootstrap token）
  - API Key middleware：`internal/auth/middleware.go`
  - OIDC middleware：`internal/auth/oidc.go`
  - 指标与可观测：`internal/metrics.Middleware`、`internal/observability.RequestIDMiddleware`、`corsMiddleware(...)`

关键修复点：
- API Key middleware 现已在存在 `AuthContext` 时直接放行，从而避免 OIDC 成功后被二次拦截（OIDC 可以独立工作）。
- `X-Request-ID` 现已做长度与字符集校验，拒绝 CRLF/超长等注入输入（见第 2 节）。
- 默认配置不再把 `/metrics` 放进 `auth.skip_paths`（auth 开启时默认保护 metrics）。

### 1.3 llmux.Client：缓存 / streaming / 上游请求

- 缓存 key：`client.go: (*Client).generateCacheKey`
  - 已升级为 `SHA-256`，并引入 `tenant_id`（来自 `AuthContext.APIKey.ID`）与更完整的请求字段覆盖（TopP/Stop/N/Tools/ResponseFormat/StreamOptions/Tags/Extra…），同时对 `Extra` 做稳定序列化。
- streaming：`client.go` 为 streaming 单独使用 `streamHTTPClient`（不设置全局 Timeout，仅设置 `ResponseHeaderTimeout`），避免长流被全局 timeout 中途杀死。

---

## 2. 修复顺序（从先到后；含已完成）

说明：这里按“可被真实攻击/真实事故触发的风险”排序；每项尽量对应可自动化验收点。  

### P0（必须修，已全部落地）

1) ✅ **管理端点 AuthZ 兜底 + 请求体限制**  
   - 管理端点默认要求管理权限（KeyTypeManagement / OIDC 管理角色）或 bootstrap token；并对管理请求体加统一大小限制。  
   - 相关提交：`057b1f9`, `cmd/server/middleware.go`。

2) ✅ **Auth：blocked/read-only 等字段真正 enforce**  
   - APIKey.Blocked、Team.Blocked、read-only key 的路由限制均已落地。  
   - 相关提交：`989f38d`, `bef0433`, `internal/auth/middleware.go`。

3) ✅ **RegenerateKey 避免“先删后建”导致 key 丢失**  
   - 改为原地更新/原子替换，避免无事务的丢 key 风险。  
   - 相关提交：`ccf5dbe`。

4) ✅ **Invitation 不再信任 `X-User-ID` 头**  
   - creator 从 AuthContext 推导，避免伪造 header 注入。  
   - 相关提交：`386d2d7`。

5) ✅ **DB schema 与迁移统一（解决 001_init.sql 漂移）**  
   - 标准化迁移脚本，避免“启动即报错”的硬失败。  
   - 相关提交：`125a2e3`，`internal/auth/migrations/`。

6) ✅ **缓存隔离与投毒面收敛（tenant + 字段覆盖 + SHA-256）**  
   - 缓存 key 引入租户 scope（默认 APIKey.ID）与完整字段覆盖，并保证稳定序列化与加密哈希。  
   - 证据：`client.go: generateCacheKey`，`client_cache_key_test.go`（`make check` 已覆盖）。

7) ✅ **streaming 可靠性修复（chunk 上限、超时策略、恢复行为）**  
   - 解除 16KB `bufio.Scanner` 限制（提升到 256KB 并在相关路径统一）。  
   - streaming 不再被 `http.Client.Timeout` 中途截断（使用专用 streaming client）。  
   - 相关提交：`0cd4751`，`stream.go`，`client.go`。

8) ✅ **include_usage 语义：尊重用户设置**  
   - 不再强行覆盖 `include_usage=false`。  
   - 相关提交：`585a973`。

9) ✅ **Provider 配置生效与 URL 构造安全**  
   - `timeout/max_concurrent/headers` 正确下传；Azure/Gemini URL path/query 做 escape；BaseURL 增加校验并默认阻止私网/loopback（可显式放开）。  
   - 相关提交：`0c8fafb`, `0cda11f`, `36d3e14`, `247dc12`。

10) ✅ **分布式路由 stats 按租户隔离**  
   - Redis stats key 引入租户 scope（从 ctx 传入），避免串租户冷却/投毒。  
   - 相关提交：`5756ca8`。

11) ✅ **Prometheus 默认降维（去高基数/PII labels）**  
   - 移除 `user/end_user/hashed_api_key/...` 等高基数维度，避免 Prometheus 爆炸与隐私泄露。  
   - 相关提交：`26d37cb`。

12) ✅ **Request-ID 输入不再全信任 + /metrics 默认不跳过鉴权**  
   - `X-Request-ID` 做 sanitize；默认不再把 `/metrics` 放入 auth skip paths（auth 开启时 metrics 默认受保护）。  
   - 相关提交：`a0d4744`。

### P1（仍建议继续收敛，但更偏“默认策略/产品取舍”）

13) ⏳ **配置示例与加载器能力一致性（避免误导部署）**  
   - 目前 loader 仅支持 `${VAR_NAME}`，但示例配置出现 `${DB_HOST:localhost}` 这类“带默认值语法”，会误导用户。  
   - 同时示例仍把 `/metrics` 放在 `auth.skip_paths` 中，与“默认保护 metrics”目标冲突。  
   - 建议：统一示例配置语法，移除默认值语法；并让示例默认不跳过 `/metrics`。  
   - 证据：`internal/config/config.go: LoadFromFile`，`config/config.example.yaml`。

14) ⏳ **默认安全姿态（示例默认不应“裸奔”）**  
   - 建议在 `config/config.example.yaml`（以及 distributed/min/dev）把 `auth.enabled`、`admin_port`、`bootstrap_token` 的安全姿态写成“默认安全、显式放开”。  
   - 说明：是否把“代码默认值”也改为更严格（例如默认仅监听 localhost / 默认必须配置 auth）需要项目层面决策，但示例与文档至少应先安全。

### P2（暂不修复：需要更大产品/架构决策）

15) **插件机制的安全边界**：插件是同进程可执行代码，不可能做强沙箱；建议文档明确“仅可信代码可作为插件”，并逐步引入只读 ContextView。  
16) **UI 的认证闭环与安全响应头**：在后端管理面授权闭环稳定后，再决定 UI 认证方式（cookie session vs OIDC），并统一 CSP/Frame-ancestors 等安全头。  

---

## 3. 对原报告的“质疑点”与更正（避免误修）

1) “ModelAccessMiddleware 未接入 == 模型访问控制没落地”需要更正：  
   - middleware 版本未接入并不等同于“完全没控制”，治理引擎内已有 model access 检查；真正风险在于默认配置/禁用路径导致退化，以及 model 规范化一致性。  

2) “RateLimitMiddleware 死代码”需要更精确：  
   - HTTP middleware 形态未挂载并不等同于 limiter 没被使用；但 client 侧与 governance 侧的双重限流职责边界需要明确（偏产品/文档层）。

---

## 4. 建议的验收方式（让修复可证明）

- 管理面兜底：无管理权限访问 `/key/*` `/control/*` 等应返回 401/403；bootstrap token 仅用于紧急/初始化。  
- OIDC/ApiKey：Bearer JWT 能独立通过（在需要的路由上），不会再被 APIKey middleware 二次拦截。  
- 缓存隔离：不同 API key 的同请求应生成不同 cache key；仅改变 `tools/top_p/stop/extra` 等字段 key 必须变化。  
- Router stats：相同 deployment 下，不同租户 scope 的冷却/失败统计互不影响。  
- Prometheus label：默认不包含用户维度；`end_user` 随机值不会导致 label 爆炸。  
- Metrics 鉴权：`auth.enabled=true` 时 `/metrics` 默认需要鉴权（除非显式在 skip_paths 放开）。  
- Request-ID：传入超长/含 CRLF 的 `X-Request-ID` 不会被回显；服务端会生成新的安全 request id。  

