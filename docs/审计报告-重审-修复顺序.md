# LLMux 重审报告（以质疑态度重建链路与修复顺序）

基线：`ed4f36149b3124d6be8f39caa7fedecf81b5b6a8`  
目标标准：企业级开源单体/分布式双模式 LLM 高性能网关（运维友好、开发者友好、高可用、默认安全）。

本报告不复述 `docs/审计报告-未重新审计.md` 的全部验证笔记，而是在“怀疑原结论是否成立/是否值得修”的前提下，重建真实代码链路与风险优先级，并给出**从先到后**的修复顺序（含“暂不修复”项）。

---

## 0. 结论摘要（先讲结论）

LLMux 当前最大的系统性问题不是“某几个点 bug”，而是：

1) **控制面/管理面与数据面边界不清 + 默认配置不安全**：默认 `admin_port=0`、`auth.enabled=false`、`metrics.enabled=true`，并且管理/控制/UI 路由会被注册到同一 mux，形成“默认裸奔”的企业级红线。  
2) **鉴权体系只有 AuthN（认证）几乎没有 AuthZ（授权）**：KeyType/角色在类型中存在，但管理端点缺少显式授权门禁。  
3) **多租户隔离与“模型规范化”不一致**：路由层对 model 做“带 `/` 时剥离前缀”的兼容行为，但治理/缓存/限流/指标并未统一 canonical model，造成绕过、污染与高基数 DoS。  
4) **观测默认高敏/高基数**：Prometheus callback 默认启用，且把 `end_user/user/hashed_api_key/team` 等高基数字段做 label；同时 `end_user` 可由请求体注入。  
5) **若启用缓存/分布式（enterprise 常态），存在跨租户错命中/投毒风险**：缓存 key 不含租户维度且字段覆盖不足，哈希使用 FNV64。

如果只修“单点 bug”（比如 `bufio.Scanner` 16KB），并不能把系统拉到企业级安全/高可用水位；应优先把“边界与默认策略”修正。

---

## 1. 代码链路重建（从入口到上游）

### 1.1 进程入口与路由分面

- 入口：`cmd/server/main.go`  
  - 读取配置（`internal/config/config.go`），创建 `llmux.Client`，再创建网关 Handler（`internal/api/client_handler.go`）。
  - `buildMuxes`（`cmd/server/routes.go`）在 `admin_port>0` 时创建 `Admin` mux；否则把管理/控制/UI 直接注册到 Data mux。
- Data mux：`registerDataRoutes`（`cmd/server/routes.go`）注册 `/v1/*`、`/health/*`、`/metrics`（如果 `metrics.enabled`）。
- Admin mux：`registerAdminRoutes`（`cmd/server/routes.go`）注册管理/控制端点 `internal/api/routes.go` + UI 静态资源 `http.FileServer`。

这意味着：**admin_port=0** 时，Data 与 Admin 完全合并（包含 UI `/` 与 `/key/*` `/control/*` 等）。

### 1.2 中间件栈（Auth / OIDC / metrics / request-id / CORS）

- 栈构建：`cmd/server/middleware.go`
  - 若 `auth.enabled=true`：启用 API Key middleware（`internal/auth/middleware.go: Authenticate`）。
  - 若配置 OIDC：启用 OIDC middleware（`internal/auth/oidc.go: OIDCMiddleware`）。
  - 之后固定加：`internal/metrics.Middleware`、`internal/observability.RequestIDMiddleware`、`corsMiddleware(...)`。

关键细节：OIDC 包在 API Key middleware 外层（OIDC -> APIKey -> handler）。但 APIKey middleware **不检查**已有 `AuthContext`，导致 OIDC 成功后仍被强制要求 API key，OIDC 不能独立工作。

### 1.3 治理（Governance）与网关 Handler

- 网关 handler：`internal/api/client_handler.go`
  - 读取并限制请求体大小（Chat 请求默认 10MB，管理端点无此限制）。
  - 生成/透传 RequestID（`internal/observability/requestid.go`）。
  - `evaluateGovernance()` -> `internal/governance/engine.go: Evaluate`。
- 治理引擎：`cmd/server/governance.go` + `internal/governance/engine.go`
  - model access、预算、租户限流（tenant rate limiter）均在这里发生。
  - 重要：这些检查大多依赖 `AuthContext`；当 `auth.enabled=false` 时，治理会退化为几乎不生效（尤其是 model access / budget）。

### 1.4 llmux.Client：路由/缓存/（另一套）限流/streaming

- 核心执行：`client.go`
  - **Client 侧也有一套 rate limit**（`checkRateLimit` + `WithRateLimiterConfig`），会与 governance 的租户限流叠加（双重限流）。
  - 缓存：`generateCacheKey` 只使用 `{model,messages,temperature,max_tokens}` 且 FNV64；不含租户维度。
  - HTTP client：`http.Client{Timeout: cfg.Timeout}`，对 streaming 同样生效，会截断长流。
- Streaming：`stream.go`
  - SSE 用 `bufio.Scanner` 解析，最大 token 16KB（`scanner.Buffer(..., 4096*4)`）。
  - 失败恢复会累积已输出内容到 `strings.Builder`，没有 size cap。

### 1.5 Router：model 前缀剥离、副作用与分布式存储

- Router：`routers/base.go`
  - 当找不到 model 时，若包含 `/`，取最后一个 `/` 后的后缀再匹配（`LastIndex` 剥离）。
- RoundRobin：`routers/round_robin.go` + `routers/round_robin_store.go`
  - counter key 直接用用户输入的 `model` 字符串。
  - Redis RR store 的 key 永不过期（`INCR` 无 TTL）。
- 分布式 stats：`routers/redis_stats_store.go`
  - key 空间按 deploymentID 聚合（`deploymentKeyPrefix(deploymentID)`），是全局健康/失败统计；这在多租户场景可能被“恶意租户投毒”。

---

## 2. 重审后的修复顺序（从先到后）

下面按 **P0（必须立刻）、P1（本周/近期）、P2（暂不修复/仅文档）** 给出顺序。  
每条都给出“为什么要先修它”与“证据定位”。

### P0（立刻修：默认安全 + 控制面边界 + 多租户隔离）

#### P0-1 管理/控制/UI 默认裸奔：必须改为显式启用且强制隔离

**为什么先修**：这是企业级网关的“单点致命失败”。它让攻击者不需要任何技巧，只要能访问端口就能管理系统。  
**证据定位**：
- `internal/config/config.go: DefaultConfig()`：`Server.AdminPort=0`、`Auth.Enabled=false`、`Metrics.Enabled=true`  
- `cmd/server/routes.go: buildMuxes()`：`AdminPort==0` 时 `registerAdminRoutes(dataMux, ...)`  
- `internal/api/routes.go: (*ManagementHandler).RegisterRoutes()`：注册大量写接口 `/key/* /team/* /user/* /control/*`
**建议修复（方向）**：
- 把管理/控制/UI 的启用做成显式开关（例如 `server.management.enabled` / `server.ui.enabled`），默认关闭；
- 或者强制：只要注册管理路由，就必须 `admin_port>0` 且 `auth.enabled=true`（否则启动失败）；
- `/metrics` 同理：在企业基线下要么独立端口，要么需要可选鉴权/网络白名单。

#### P0-2 管理 API 缺失 AuthZ：KeyType/角色必须真正 enforce

**为什么先修**：即便你打开了 `auth.enabled`，目前也只有“认证”，没有“授权”。任意 key 仍可做破坏性管理操作。  
**证据定位**：
- `internal/auth/types.go`：存在 `KeyTypeManagement` 等定义  
- `internal/api/management.go`：各端点无 `auth.GetAuthContext()` 校验，无 role/key_type 校验  
**建议修复（方向）**：
- 引入“管理面授权中间件”（至少：要求 `KeyTypeManagement` 或 `UserRoleProxyAdmin`）并在管理 mux 统一挂载；
- 将“读操作”和“写操作”分级（viewer vs admin）；
- 配合 P0-1：管理面默认不暴露到 data port。

#### P0-3 OIDC 与 API Key middleware 冲突：修复后再谈 SSO/企业登录

**为什么先修**：当前配置上“看似支持 OIDC”，实际上很容易被迫关掉 auth 才能跑通 UI/SSO，反向扩大风险。  
**证据定位**：
- `cmd/server/middleware.go`：OIDC 包在 APIKey 外层  
- `internal/auth/oidc.go: OIDCMiddleware()`：OIDC 成功后写入 `AuthContext` 并调用 next  
- `internal/auth/middleware.go: Authenticate()`：不检查已有 `AuthContext`，仍强制 API key
**建议修复（方向）**：
- APIKey middleware 在读取 header 前先判断 `GetAuthContext(r.Context()) != nil` 则放行；
- 或调整中间件顺序/策略：允许“APIKey 或 OIDC 二选一”，并能对 admin/data 分别配置。

#### P0-4 缓存的跨租户泄露/投毒：必须引入租户 scope + 完整字段 + 强哈希

**为什么先修**：分布式/企业配置里通常会启用 Redis/dual cache。当前实现会让不同租户命中同一 key（泄露）或用“忽略字段差异”投毒。  
**证据定位**：
- `client.go: generateCacheKey()`：key 仅含 `model/messages/temperature/max_tokens`，hash=FNV64  
- `config/config.distributed.yaml`：`cache.enabled: true`, `type: dual`
**建议修复（方向）**：
- key 结构：`<globalNamespace>:<tenantScope>:chat:<sha256(canonicalModel + stableRequestFields)>`  
  - tenantScope 默认用 `authCtx.APIKey.ID`（可配置 team/org，但必须来自服务端 authctx，不能来自请求体）  
  - stableRequestFields 至少覆盖 `TopP/Stop/N/Tools/ToolChoice/ResponseFormat/StreamOptions/Tags/Extra/...`  
- 哈希改为 `SHA-256`（或 BLAKE3）并对字段做稳定序列化；
- 明确“缓存只对同租户生效”的产品语义，并补回归测试。

#### P0-5 Model 规范化必须全链路一致（路由/治理/限流/缓存/指标/round-robin）

**为什么先修**：这是导致“绕过/高基数/Redis key 无限增长/错误投毒”的根因。  
**证据定位**：
- `routers/base.go: snapshotDeployments()`：`LastIndex("/")` 剥离  
- `pkg/types/model.go: SplitProviderModel()`：只按第一个 `/` 切分（与 router 不一致）  
- `routers/round_robin_store.go`：RR key=用户 model，Redis 无 TTL  
- `client.go: buildRateLimitKey()`：key 直接拼接原 model，放大高基数  
**建议修复（方向）**：
- 定义单一 canonical 规则（建议只支持 `provider/model` 单斜杠；多斜杠直接 400）；
- 在进入治理/路由/缓存/限流/指标前统一 canonicalize，且上游请求使用 canonical model；
- 若要支持“指定 provider”语义，应该显式解析 provider 前缀并参与路由选择，而不是“找不到就剥离后缀”。

#### P0-6 Prometheus 默认高敏/高基数：必须降维，避免 DoS 与合规风险

**为什么先修**：指标是默认打开并且默认不鉴权的；高基数会炸 Prometheus，也会把 PII/tenant 信息暴露到观测面。  
**证据定位**：
- `internal/observability/config.go: DefaultObservabilityConfig()`：Prometheus 默认 enabled  
- `internal/observability/prometheus_callback.go: payloadToMetrics()`：labels 填 `EndUser/User/HashedAPIKey/Team/...`  
- `internal/api/client_handler.go: applyAuthContext()`：`EndUser` 允许来自请求体 `req.User`
**建议修复（方向）**：
- 默认 labels 仅保留低基数：`provider/model/status/deployment`（必要时再加“tenant_id hash（低基数桶）”）；  
- 将 `end_user/user/api_key` 这类维度移到 logs/traces（并做脱敏/采样/截断）；  
- 给 `/metrics` 提供可选鉴权或默认不在 `auth.skip_paths`。

#### P0-7 请求对象池跨请求污染：Reset() 必须清理 StreamOptions

**为什么先修**：这是确定的跨请求状态污染，属于“高置信 P0 bug”。  
**证据定位**：
- `pkg/types/request.go: (*ChatRequest).Reset()` 未清理 `StreamOptions`
**建议修复（方向）**：
- `Reset()` 补齐 `r.StreamOptions = nil`，并加回归测试覆盖“请求 A 设置 stream_options 后请求 B 不继承”。

#### P0-8 错误信息泄露：禁止把任意 err.Error() 直接回包

**为什么先修**：错误字符串可能包含内部地址、库错误细节、甚至配置片段；企业环境会被当作信息泄露缺陷。  
**证据定位**：
- `internal/api/client_handler.go: writeError()`：非 `*LLMError` 时 `NewInternalError(..., err.Error())`  
**建议修复（方向）**：
- 对外统一 `internal server error`（可带 `request_id`），详细 error 只写日志；  
- 对 `*LLMError` 也做分级（哪些 message 可透传，哪些需要替换）。

---

### P1（近期修：稳定性、正确性、可运维）

#### P1-1 Streaming 解析与超时策略：不要让全局 Timeout 杀掉长流

**证据定位**：
- `client.go: http.Client{Timeout: cfg.Timeout}`  
- `stream.go: scanner.Buffer(..., 4096*4)`（16KB 上限）
**建议修复（方向）**：
- streaming 使用 `http.Client.Timeout=0` + per-request context deadline（或更合理的 `ResponseHeaderTimeout` / `IdleConnTimeout`）；  
- 替换 `bufio.Scanner` 为自定义 SSE reader（按 `\n\n` 分帧）或把 buffer 提升并加上上限与错误处理；  
- 对 `accumulated strings.Builder` 加 size cap 或按 token/字节上限截断。

#### P1-2 gateway 配置未生效：providers 的 timeout/max_concurrent/headers 没有传到 llmux.Client

**证据定位**：
- `cmd/server/main.go: buildClientOptions()`：构造 `llmux.ProviderConfig` 未填 `Timeout/MaxConcurrent/Headers`
**建议修复（方向）**：
- 补齐字段传递，并在 README/config 示例中说明其语义；  
- 对 `headers` 增加 denylist（避免注入危险 hop-by-hop header）。

#### P1-3 管理端点输入防护：MaxBytesReader、Content-Type、超时、幂等与事务

**证据定位**：
- `internal/api/management.go` 多处 `json.NewDecoder(r.Body).Decode()` 无 body 限制  
- `internal/api/management.go: RegenerateKey()` 删除旧 key + 创建新 key 无事务
**建议修复（方向）**：
- 管理 mux 增加统一的 body limit（例如 1MB）与 `Content-Type: application/json` 强制；  
- `RegenerateKey` 通过 store 层提供事务方法（Postgres `BEGIN ... COMMIT`），避免 key 丢失。

#### P1-4 DB schema 与代码漂移：确定“唯一权威迁移”并在文档中落地

**证据定位**：
- `internal/auth/postgres.go` 查询列包含 `blocked/organization_id/key_alias/...`  
- `internal/auth/migrations/001_init.sql` 不包含这些列（但仓库里仍存在）  
**建议修复（方向）**：
- 明确：生产必须使用 `002_full_schema.sql` + `003_enterprise_features.sql`（或合并成单一迁移），并把 `001_init.sql` 标注废弃/删除；  
- 提供最小可执行的“建库脚本 + 版本校验”，避免启动即报错。

#### P1-5 URL 拼接安全构造（可靠性/兼容性）

**证据定位**：
- `providers/azure/azure.go`：deploymentName 直接拼 URL path  
- `providers/gemini/gemini.go`：token 直接拼 query
**建议修复（方向）**：
- 使用 `net/url` + `url.Values` + `path.Join`/`PathEscape` 构造 URL，避免字符破坏语义。

---

### P2（暂不修复：更像“产品决策/文档边界/非关键技术债”）

这些点在“企业级网关”里**确实值得做**，但考虑投入产出或需要更大的架构决策，建议先文档化/打标签，暂不在第一波修复中阻塞：

1) **SSRF/BaseURL 校验**：当前 base_url 完全信任配置（例如允许 localhost）。在“配置可被不可信方修改”威胁模型下是 P0，但在典型企业部署里配置应属于可信运维域。建议先在文档/配置校验里提示风险，后续再加 allowlist/deny private IP 的可选校验。  
2) **Router stats 按租户隔离**：全局健康统计是否应按租户隔离属于产品决策；隔离会显著放大 Redis key 空间与计算。短期更现实的是修复“model 规范化”后，减少投毒入口，并在治理层做“无效模型快速拒绝”。  
3) **插件机制安全边界**：插件本质是“可执行代码”，不能指望在同进程里做强沙箱。建议在文档明确“仅可信代码可作为插件”，并提供最小能力接口（只读视图）作为后续演进方向。  
4) **UI 安全头/真正登录**：目前 UI 本身更像“管理 API 的演示壳”。应先完成后端管理面授权闭环后，再决定 UI 的认证方式（cookie session vs OIDC）。安全响应头（CSP 等）可作为 UI 产品化的一部分处理。

---

## 3. 对原报告的“质疑点”与更正（避免误修）

1) “ModelAccessMiddleware 未接入 == 模型访问控制没落地”这一句需要更正：  
   - **middleware 版本**确实未接入，但 **governance 引擎**里已经做了 `checkModelAccess()`（`internal/governance/engine.go`）。  
   - 真正的缺口是：当 `auth.enabled=false` 或 `governance.enabled=false` 时，这套控制会退化；以及 model canonicalization 不一致会造成 per-model 策略绕过。

2) “RateLimitMiddleware 死代码”需要更精确：  
   - `TenantRateLimiter.RateLimitMiddleware` 确实没挂到 HTTP 栈；  
   - 但 limiter 本体在 governance 中通过 `trl.Check()` 被使用（`cmd/server/governance.go` -> `internal/governance/engine.go: checkRateLimit`）。  
   - 真问题是“client 侧也有一套 rate limiting”，导致双重限流语义不清。

---

## 4. 建议的验收方式（让修复可证明）

为避免“修了但不敢上线”，建议每个 P0/P1 都有可自动化的验收点：

- 管理面隔离：`admin_port=0` 时访问 `/key/list` 应 404 或 403；`admin_port>0` 且无管理权限应 403。  
- OIDC/ApiKey：仅带 Bearer JWT 的请求能通过（在需要的路由上），不会再被 APIKey middleware 拦截。  
- 缓存隔离：两把不同 API key 同请求应生成不同 cache key，不互相命中；仅改变 `tools/top_p/stop/extra` 等字段应改变 key。  
- model 规范化：`provider/model` 只生成一个 RR key/RateLimit key/metrics label，不随前缀变化；多斜杠直接拒绝。  
- Prometheus label：默认不包含用户维度；`end_user` 随机值不会导致 label 爆炸。  
- Stream：单个 SSE chunk >16KB 不应导致 `token too long`；长 stream 不被全局 client timeout 杀死。  
- Reset 污染：复用对象不会继承 `StreamOptions`。

