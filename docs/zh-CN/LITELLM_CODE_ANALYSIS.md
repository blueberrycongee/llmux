# LiteLLM ä»£ç æ·±åº¦åˆ†ææŠ¥å‘Š

## ä¸€ã€é¡¹ç›®æ•´ä½“æ¶æ„

```
litellm/
â”œâ”€â”€ litellm/                    # æ ¸å¿ƒåº“
â”‚   â”œâ”€â”€ llms/                   # ğŸ”¥ Provider é€‚é…å±‚ (100+ ä¸ª)
â”‚   â”‚   â”œâ”€â”€ base_llm/           # åŸºç±»å®šä¹‰
â”‚   â”‚   â”œâ”€â”€ openai/             # OpenAI å®ç°
â”‚   â”‚   â”œâ”€â”€ anthropic/          # Anthropic å®ç°
â”‚   â”‚   â”œâ”€â”€ azure/              # Azure OpenAI
â”‚   â”‚   â”œâ”€â”€ gemini/             # Google Gemini
â”‚   â”‚   â””â”€â”€ ...                 # å…¶ä»– 90+ providers
â”‚   â”œâ”€â”€ types/                  # ç±»å‹å®šä¹‰ (Pydantic models)
â”‚   â”œâ”€â”€ proxy/                  # Proxy Server (LLM Gateway)
â”‚   â”œâ”€â”€ router.py               # è·¯ç”± & è´Ÿè½½å‡è¡¡
â”‚   â”œâ”€â”€ router_strategy/        # è·¯ç”±ç­–ç•¥å®ç°
â”‚   â”œâ”€â”€ router_utils/           # è·¯ç”±å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ exceptions.py           # ç»Ÿä¸€å¼‚å¸¸å®šä¹‰
â”‚   â”œâ”€â”€ utils.py                # å·¥å…·å‡½æ•°
â”‚   â””â”€â”€ cost_calculator.py      # æˆæœ¬è®¡ç®—
â”œâ”€â”€ model_prices_and_context_window.json  # ğŸ’° ä»·æ ¼è¡¨ (ç›´æ¥å¯ç”¨)
â””â”€â”€ tests/                      # æµ‹è¯•ç”¨ä¾‹
```

## äºŒã€æ ¸å¿ƒè®¾è®¡æ¨¡å¼åˆ†æ

### 1. Provider Adapter æ¨¡å¼

LiteLLM ä½¿ç”¨ BaseConfig æŠ½è±¡åŸºç±»å®šä¹‰ç»Ÿä¸€æ¥å£ï¼Œæ¯ä¸ª Provider ç»§æ‰¿å¹¶å®ç°ï¼š

```python
# litellm/llms/base_llm/chat/transformation.py
class BaseConfig(ABC):
    
    @abstractmethod
    def get_supported_openai_params(self, model: str) -> list:
        """è¿”å›è¯¥ Provider æ”¯æŒçš„ OpenAI å‚æ•°"""
        pass

    @abstractmethod
    def map_openai_params(self, non_default_params, optional_params, model, drop_params) -> dict:
        """å°† OpenAI æ ¼å¼å‚æ•°æ˜ å°„ä¸º Provider ç‰¹å®šæ ¼å¼"""
        pass

    @abstractmethod
    def transform_request(self, model, messages, optional_params, litellm_params, headers) -> dict:
        """è½¬æ¢è¯·æ±‚ä½“"""
        pass

    @abstractmethod
    def transform_response(self, model, raw_response, model_response, ...) -> ModelResponse:
        """è½¬æ¢å“åº”ä½“ä¸ºç»Ÿä¸€æ ¼å¼"""
        pass

    @abstractmethod
    def validate_environment(self, headers, model, messages, ...) -> dict:
        """éªŒè¯ç¯å¢ƒå˜é‡ã€è®¾ç½® headers"""
        pass

    @abstractmethod
    def get_error_class(self, error_message, status_code, headers) -> BaseLLMException:
        """è¿”å›å¯¹åº”çš„å¼‚å¸¸ç±»"""
        pass
```

**Go è½¬æ¢ç­–ç•¥**ï¼šè¿™ä¸ªæ¥å£è®¾è®¡å¯ä»¥ç›´æ¥æ˜ å°„ä¸º Go interfaceï¼š

```go
type ProviderConfig interface {
    GetSupportedOpenAIParams(model string) []string
    MapOpenAIParams(params map[string]any, model string) map[string]any
    TransformRequest(model string, messages []Message, params map[string]any) (*Request, error)
    TransformResponse(raw *http.Response, model string) (*ModelResponse, error)
    ValidateEnvironment(headers map[string]string, apiKey string) error
    GetErrorClass(statusCode int, message string) error
}
```

### 2. ç±»å‹ç³»ç»Ÿåˆ†æ

**æ ¸å¿ƒç±»å‹å®šä¹‰ (litellm/types/utils.py)**ï¼š

```python
# è¯·æ±‚ç±»å‹
class AllMessageValues(TypedDict):  # OpenAI æ¶ˆæ¯æ ¼å¼
    role: str
    content: Union[str, List[ContentBlock]]
    
# å“åº”ç±»å‹  
class ModelResponse:
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choices]
    usage: Usage

class Choices:
    finish_reason: str
    index: int
    message: Message

class Usage:
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

# æµå¼å“åº”
class GenericStreamingChunk(TypedDict):
    text: str
    tool_use: Optional[ChatCompletionToolCallChunk]
    is_finished: bool
    finish_reason: str
    usage: Optional[ChatCompletionUsageBlock]
```

**Go è½¬æ¢**ï¼šè¿™äº› TypedDict å¯ä»¥ç›´æ¥è½¬ä¸º Go structï¼š

```go
type ModelResponse struct {
    ID      string   `json:"id"`
    Object  string   `json:"object"`
    Created int64    `json:"created"`
    Model   string   `json:"model"`
    Choices []Choice `json:"choices"`
    Usage   Usage    `json:"usage"`
}

type Choice struct {
    FinishReason string  `json:"finish_reason"`
    Index        int     `json:"index"`
    Message      Message `json:"message"`
}

type Usage struct {
    PromptTokens     int `json:"prompt_tokens"`
    CompletionTokens int `json:"completion_tokens"`
    TotalTokens      int `json:"total_tokens"`
}
```

### 3. å¼‚å¸¸ä½“ç³»

**ç»Ÿä¸€å¼‚å¸¸ç±»å‹ (litellm/exceptions.py)**ï¼š

| å¼‚å¸¸ç±» | HTTP Status | è¯´æ˜ |
|--------|-------------|------|
| AuthenticationError | 401 | API Key æ— æ•ˆ |
| NotFoundError | 404 | æ¨¡å‹ä¸å­˜åœ¨ |
| BadRequestError | 400 | è¯·æ±‚å‚æ•°é”™è¯¯ |
| RateLimitError | 429 | è§¦å‘é™æµ |
| Timeout | 408 | è¯·æ±‚è¶…æ—¶ |
| ServiceUnavailableError | 503 | æœåŠ¡ä¸å¯ç”¨ |
| InternalServerError | 500 | å†…éƒ¨é”™è¯¯ |
| ContextWindowExceededError | 400 | ä¸Šä¸‹æ–‡è¶…é™ |
| ContentPolicyViolationError | 400 | å†…å®¹è¿è§„ |

æ¯ä¸ªå¼‚å¸¸éƒ½åŒ…å«ï¼š
- status_code
- message
- llm_provider
- model
- litellm_debug_info
- max_retries / num_retries

**Go è½¬æ¢**ï¼šå®šä¹‰ç»Ÿä¸€ error ç±»å‹ï¼š

```go
type LLMError struct {
    StatusCode int
    Message    string
    Provider   string
    Model      string
    Retries    int
}

func (e *LLMError) Error() string { 
    return fmt.Sprintf("provider=%s model=%s code=%d: %s", 
        e.Provider, e.Model, e.StatusCode, e.Message)
}
```

### 4. Router è·¯ç”±ç­–ç•¥

**æ”¯æŒçš„è·¯ç”±ç­–ç•¥ (litellm/router_strategy/)**ï¼š

| ç­–ç•¥ | æ–‡ä»¶ | è¯´æ˜ |
|------|------|------|
| Simple Shuffle | simple_shuffle.py | éšæœºé€‰æ‹© |
| Least Busy | least_busy.py | æœ€å°‘å¹¶å‘ |
| Lowest Latency | lowest_latency.py | æœ€ä½å»¶è¿Ÿ |
| Lowest Cost | lowest_cost.py | æœ€ä½æˆæœ¬ |
| Lowest TPM/RPM | lowest_tpm_rpm.py | æœ€ä½ token/è¯·æ±‚ä½¿ç”¨ç‡ |
| Budget Limiter | budget_limiter.py | é¢„ç®—é™åˆ¶ |
| Tag Based | tag_based_routing.py | æ ‡ç­¾è·¯ç”± |

**Router æ ¸å¿ƒé€»è¾‘**ï¼š

```python
class Router:
    def __init__(self, model_list, routing_strategy="simple-shuffle", ...):
        self.model_list = model_list  # éƒ¨ç½²åˆ—è¡¨
        self.routing_strategy = routing_strategy
        self.cooldown_cache = CooldownCache()  # å†·å´ç¼“å­˜ï¼ˆç†”æ–­ï¼‰
        
    async def acompletion(self, model, messages, **kwargs):
        # 1. è·å–å¯ç”¨éƒ¨ç½²
        deployments = self._get_deployments(model)
        # 2. è¿‡æ»¤å†·å´ä¸­çš„éƒ¨ç½²
        deployments = await _async_get_cooldown_deployments(deployments)
        # 3. æ ¹æ®ç­–ç•¥é€‰æ‹©
        deployment = self._pick_deployment(deployments)
        # 4. è°ƒç”¨
        return await litellm.acompletion(...)
```

## ä¸‰ã€å…³é”®æ–‡ä»¶æ¸…å•ï¼ˆAI æ‰¹é‡è½¬æ¢ï¼‰

### ğŸ¤– å¯ç›´æ¥è®© AI è½¬æ¢çš„æ–‡ä»¶

| æ–‡ä»¶ | å†…å®¹ | è½¬æ¢éš¾åº¦ |
|------|------|----------|
| types/utils.py | æ ¸å¿ƒç±»å‹å®šä¹‰ | â­ ç®€å• |
| types/llms/openai.py | OpenAI ç±»å‹ | â­ ç®€å• |
| types/llms/anthropic.py | Anthropic ç±»å‹ | â­ ç®€å• |
| exceptions.py | å¼‚å¸¸å®šä¹‰ | â­ ç®€å• |
| model_prices_and_context_window.json | ä»·æ ¼è¡¨ | â­ ç›´æ¥å¤åˆ¶ |
| llms/openai/chat/gpt_transformation.py | OpenAI é€‚é… | â­â­ ä¸­ç­‰ |
| llms/anthropic/chat/transformation.py | Anthropic é€‚é… | â­â­ ä¸­ç­‰ |
| llms/azure/chat/gpt_transformation.py | Azure é€‚é… | â­â­ ä¸­ç­‰ |
| llms/gemini/chat/transformation.py | Gemini é€‚é… | â­â­ ä¸­ç­‰ |

### ğŸ§  éœ€è¦äººå·¥é‡å†™çš„éƒ¨åˆ†

| æ¨¡å— | åŸå›  |
|------|------|
| SSE æµå¼å¤„ç† | Python çš„ async generator æ¨¡å¼åœ¨ Go ä¸­å®Œå…¨ä¸åŒ |
| HTTP Client | Go ç”¨ net/http æˆ– fasthttpï¼Œéœ€è¦é‡æ–°è®¾è®¡è¿æ¥æ±  |
| Router å¹¶å‘æ§åˆ¶ | Go ç”¨ channel/semaphoreï¼Œä¸æ˜¯ Python asyncio |
| é…ç½®çƒ­é‡è½½ | Go ç”¨ fsnotify + atomic pointer |
| Metrics åŸ‹ç‚¹ | Go ç”¨ prometheus/client_golang |

## å››ã€Provider å‚æ•°æ˜ å°„è¡¨

### OpenAI â†’ Anthropic å‚æ•°æ˜ å°„
ä» anthropic/chat/transformation.py æå–ï¼š

| OpenAI å‚æ•° | Anthropic å‚æ•° | è¯´æ˜ |
|-------------|----------------|------|
| max_tokens | max_tokens | ç›´æ¥æ˜ å°„ |
| max_completion_tokens | max_tokens | åˆ«å |
| temperature | temperature | ç›´æ¥æ˜ å°„ |
| top_p | top_p | ç›´æ¥æ˜ å°„ |
| stop | stop_sequences | éœ€è¦è½¬ä¸ºæ•°ç»„ |
| tools | tools | éœ€è¦æ ¼å¼è½¬æ¢ |
| tool_choice | tool_choice | éœ€è¦æ ¼å¼è½¬æ¢ |
| response_format | è½¬ä¸º tool call | Anthropic ä¸ç›´æ¥æ”¯æŒ |
| stream | stream | ç›´æ¥æ˜ å°„ |
| user | metadata.user_id | åµŒå¥— |

### Tool Choice æ˜ å°„
```python
# OpenAI â†’ Anthropic
"auto"     â†’ {"type": "auto"}
"required" â†’ {"type": "any"}
"none"     â†’ {"type": "none"}
{"function": {"name": "xxx"}} â†’ {"type": "tool", "name": "xxx"}
```

## äº”ã€ä»·æ ¼è¡¨ç»“æ„

`model_prices_and_context_window.json` ç»“æ„ï¼š

```json
{
  "gpt-4o": {
    "litellm_provider": "openai",
    "mode": "chat",
    "max_input_tokens": 128000,
    "max_output_tokens": 16384,
    "input_cost_per_token": 0.0000025,
    "output_cost_per_token": 0.00001,
    "supports_function_calling": true,
    "supports_vision": true,
    "supports_response_schema": true,
    "supports_parallel_function_calling": true
  }
}
```

**Go ç»“æ„ä½“**ï¼š

```go
type ModelInfo struct {
    LiteLLMProvider              string   `json:"litellm_provider"`
    Mode                         string   `json:"mode"`
    MaxInputTokens               int      `json:"max_input_tokens"`
    MaxOutputTokens              int      `json:"max_output_tokens"`
    InputCostPerToken            float64  `json:"input_cost_per_token"`
    OutputCostPerToken           float64  `json:"output_cost_per_token"`
    SupportsFunctionCalling      bool     `json:"supports_function_calling"`
    SupportsVision               bool     `json:"supports_vision"`
    SupportsResponseSchema       bool     `json:"supports_response_schema"`
    SupportsParallelFunctionCall bool     `json:"supports_parallel_function_calling"`
}
```

## å…­ã€æµå¼å“åº”å¤„ç†åˆ†æ

### Python å®ç°æ¨¡å¼
```python
# litellm/llms/base_llm/base_model_iterator.py
class BaseModelResponseIterator:
    def chunk_parser(self, chunk: dict) -> ModelResponseStream:
        """è§£æå•ä¸ª chunk"""
        pass
        
    def __iter__(self):
        for chunk in self.streaming_response:
            yield self.chunk_parser(chunk)
```

### Go å®ç°å»ºè®®
```go
type StreamHandler struct {
    reader   *bufio.Reader
    buffer   *sync.Pool  // å¤ç”¨ buffer
    clientCtx context.Context
}

func (s *StreamHandler) Next() (*StreamChunk, error) {
    select {
    case <-s.clientCtx.Done():
        return nil, s.clientCtx.Err()  // client æ–­å¼€
    default:
        line, err := s.reader.ReadBytes('\n')
        if err != nil {
            return nil, err
        }
        return s.parseChunk(line)
    }
}
```

## ä¸ƒã€æˆ‘ä»¬éœ€è¦å®ç°çš„æ ¸å¿ƒæ¥å£

### 1. Provider Interface
```go
type Provider interface {
    // åŸºç¡€ä¿¡æ¯
    Name() string
    SupportedModels() []string
    
    // å‚æ•°è½¬æ¢
    MapParams(openaiParams map[string]any) map[string]any
    
    // è¯·æ±‚è½¬æ¢
    BuildRequest(ctx context.Context, req *ChatRequest) (*http.Request, error)
    
    // å“åº”è½¬æ¢
    ParseResponse(resp *http.Response) (*ChatResponse, error)
    ParseStreamChunk(chunk []byte) (*StreamChunk, error)
    
    // é”™è¯¯å¤„ç†
    MapError(statusCode int, body []byte) error
}
```

### 2. Router Interface
```go
type Router interface {
    // é€‰æ‹©éƒ¨ç½²
    Pick(ctx context.Context, model string) (*Deployment, error)
    
    // æ ‡è®°æˆåŠŸ/å¤±è´¥
    ReportSuccess(deployment *Deployment, latency time.Duration)
    ReportFailure(deployment *Deployment, err error)
    
    // ç†”æ–­çŠ¶æ€
    IsCircuitOpen(deployment *Deployment) bool
}
```

### 3. Config Interface
```go
type ConfigManager interface {
    // è·å–é…ç½®
    Get() *Config
    
    // çƒ­é‡è½½
    Watch(ctx context.Context) <-chan *Config
    
    // åŸå­æ›´æ–°
    Update(newConfig *Config) error
}
```

## å…«ã€å¼€å‘ä¼˜å…ˆçº§å»ºè®®

### Phase 1: éª¨æ¶ (Week 1-2)
- å®šä¹‰ Go æ¥å£ (Provider, Router, Config)
- å®ç° HTTP Server (net/http)
- å®ç° OpenAI Providerï¼ˆä½œä¸ºæ¨¡æ¿ï¼‰
- é›†æˆ Prometheus metrics
- é›†æˆ slog æ—¥å¿—

### Phase 2: AI æ‰¹é‡ç”Ÿæˆ (Week 3)
- è®© AI è½¬æ¢ types/ ä¸‹çš„æ‰€æœ‰ç±»å‹å®šä¹‰
- è®© AI è½¬æ¢ Anthropic/Azure/Gemini adapter
- è®© AI ç”Ÿæˆå‚æ•°æ˜ å°„è¡¨
- äººå·¥ review + ä¿®æ­£

### Phase 3: æµå¼ & é«˜å¯ç”¨ (Week 4-5)
- å®ç° SSE æµå¼è½¬å‘ï¼ˆäººå·¥ï¼‰
- é›†æˆ gobreaker ç†”æ–­å™¨
- å®ç° per-provider semaphore
- å®ç°ä¼˜é›…å…³é—­

### Phase 4: æ‰“åŒ… (Week 6)
- Distroless Docker é•œåƒ
- Helm Chart
- CI/CD

## ä¹ã€Prompt æ¨¡æ¿ï¼ˆç”¨äº AI æ‰¹é‡è½¬æ¢ï¼‰

### è½¬æ¢ç±»å‹å®šä¹‰
```
ä½ æ˜¯ä¸€ä¸ª Go ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ Python Pydantic/TypedDict ç±»å‹å®šä¹‰è½¬æ¢ä¸º Go structã€‚

è¦æ±‚ï¼š
1. ä½¿ç”¨æ­£ç¡®çš„ json tag
2. Optional å­—æ®µä½¿ç”¨æŒ‡é’ˆç±»å‹æˆ– omitempty
3. Union ç±»å‹ä½¿ç”¨ interface{} æˆ–å®šä¹‰å¤šä¸ªç±»å‹
4. æ·»åŠ  GoDoc æ³¨é‡Š

Python ä»£ç ï¼š
---
{paste code}
---
```

### è½¬æ¢ Provider Adapter
```
ä½ æ˜¯ä¸€ä¸ª Go ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹ LiteLLM Provider çš„ Python å®ç°è½¬æ¢ä¸º Goã€‚

è¦æ±‚ï¼š
1. å®ç° Provider interface
2. MapParams æ–¹æ³•å¤„ç†å‚æ•°æ˜ å°„
3. BuildRequest æ–¹æ³•æ„å»º HTTP è¯·æ±‚
4. ParseResponse æ–¹æ³•è§£æå“åº”
5. ä¿æŒä¸åŸ Python é€»è¾‘ä¸€è‡´

Python ä»£ç ï¼š
---
{paste code}
---

Go Provider Interfaceï¼š
---
type Provider interface {
    Name() string
    MapParams(params map[string]any) map[string]any
    BuildRequest(ctx context.Context, req *ChatRequest) (*http.Request, error)
    ParseResponse(resp *http.Response) (*ChatResponse, error)
}
---
```