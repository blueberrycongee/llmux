# LiteLLM ä»£ç æ·±åº¦åˆ†æï¼šç¿»è¯‘ vs é‡å†™

## ä¸€ã€å¯ä»¥å¿«é€Ÿç¿»è¯‘çš„éƒ¨åˆ†ï¼ˆç…§æŠ„é€»è¾‘ï¼‰

è¿™äº›ä»£ç é€»è¾‘ç®€å•ã€æ¨¡å¼å›ºå®šï¼Œå¯ä»¥è®© AI ç›´æ¥ä» Python ç¿»è¯‘æˆ Goï¼š

### 1. ç±»å‹å®šä¹‰ (Types)

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ | è¯´æ˜ |
|------|------|----------|------|
| types/utils.py | ModelResponse, Usage, Choices | â­ | çº¯æ•°æ®ç»“æ„ï¼Œç›´æ¥è½¬ struct |
| types/llms/openai.py | OpenAI è¯·æ±‚/å“åº”ç±»å‹ | â­ | TypedDict â†’ Go struct |
| types/llms/anthropic.py | Anthropic ç±»å‹ | â­ | åŒä¸Š |
| types/router.py | è·¯ç”±ç›¸å…³ç±»å‹ | â­ | åŒä¸Š |

**Python ç¤ºä¾‹**ï¼š

```python
class ModelResponse:
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choices]
    usage: Usage
```

**Go ç¿»è¯‘**ï¼š

```go
type ModelResponse struct {
    ID      string   `json:"id"`
    Object  string   `json:"object"`
    Created int64    `json:"created"`
    Model   string   `json:"model"`
    Choices []Choice `json:"choices"`
    Usage   *Usage   `json:"usage,omitempty"`
}
```

### 2. å¼‚å¸¸/é”™è¯¯å®šä¹‰ (Exceptions)

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| exceptions.py | ç»Ÿä¸€å¼‚å¸¸ç±»å‹ | â­ |

**Python**ï¼š

```python
class RateLimitError(openai.RateLimitError):
    def __init__(self, message, llm_provider, model, ...):
        self.status_code = 429
        self.message = f"litellm.RateLimitError: {message}"
        self.llm_provider = llm_provider
        self.model = model
```

**Go ç¿»è¯‘**ï¼š

```go
type RateLimitError struct {
    StatusCode int
    Message    string
    Provider   string
    Model      string
    Retries    int
    MaxRetries int
}

func (e *RateLimitError) Error() string {
    return fmt.Sprintf("RateLimitError: %s (provider=%s, model=%s)", 
        e.Message, e.Provider, e.Model)
}
```

### 3. å‚æ•°æ˜ å°„é€»è¾‘ (Parameter Mapping)

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| llms/anthropic/chat/transformation.py çš„ map_openai_params() | OpenAI â†’ Anthropic å‚æ•°è½¬æ¢ | â­â­ |
| llms/openai/chat/gpt_transformation.py çš„ map_openai_params() | OpenAI å‚æ•°è¿‡æ»¤ | â­ |

**Python**ï¼š

```python
def _map_tool_choice(self, tool_choice, parallel_tool_use):
    if tool_choice == "auto":
        return {"type": "auto"}
    elif tool_choice == "required":
        return {"type": "any"}
    elif tool_choice == "none":
        return {"type": "none"}
    elif isinstance(tool_choice, dict):
        return {"type": "tool", "name": tool_choice["function"]["name"]}
```

**Go ç¿»è¯‘**ï¼š

```go
func (c *AnthropicConfig) MapToolChoice(toolChoice any) map[string]any {
    switch v := toolChoice.(type) {
    case string:
        switch v {
        case "auto":
            return map[string]any{"type": "auto"}
        case "required":
            return map[string]any{"type": "any"}
        case "none":
            return map[string]any{"type": "none"}
        }
    case map[string]any:
        if fn, ok := v["function"].(map[string]any); ok {
            return map[string]any{"type": "tool", "name": fn["name"]}
        }
    }
    return nil
}
```

### 4. è¯·æ±‚/å“åº”è½¬æ¢ (Transform Request/Response)

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| llms/*/chat/transformation.py çš„ transform_request() | æ„å»ºè¯·æ±‚ä½“ | â­â­ |
| llms/*/chat/transformation.py çš„ transform_response() | è§£æå“åº”ä½“ | â­â­ |

**Python**ï¼š

```python
def transform_request(self, model, messages, optional_params, ...):
    return {
        "model": model,
        "messages": messages,
        **optional_params,
    }
```

**Go ç¿»è¯‘**ï¼š

```go
func (c *OpenAIConfig) TransformRequest(model string, messages []Message, params map[string]any) map[string]any {
    req := map[string]any{
        "model":    model,
        "messages": messages,
    }
    for k, v := range params {
        req[k] = v
    }
    return req
}
```

### 5. ä»·æ ¼è¡¨å’Œæ¨¡å‹ä¿¡æ¯

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| model_prices_and_context_window.json | ä»·æ ¼ã€token é™åˆ¶ | â­ ç›´æ¥å¤åˆ¶ |

**ç›´æ¥åµŒå…¥ Go**ï¼š

```go
//go:embed model_prices.json
var modelPricesJSON []byte

var ModelPrices map[string]ModelInfo

func init() {
    json.Unmarshal(modelPricesJSON, &ModelPrices)
}
```

### 6. è·¯ç”±ç­–ç•¥çš„é€‰æ‹©é€»è¾‘

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| router_strategy/simple_shuffle.py | éšæœºé€‰æ‹© | â­ |
| router_strategy/lowest_latency.py çš„ _get_available_deployments() | æœ€ä½å»¶è¿Ÿé€‰æ‹© | â­â­ |

**Python**ï¼š

```python
def _get_available_deployments(self, model_group, healthy_deployments, ...):
    # æŒ‰å»¶è¿Ÿæ’åº
    sorted_deployments = sorted(potential_deployments, key=lambda x: x[1])
    lowest_latency = sorted_deployments[0][1]
    # åœ¨ buffer èŒƒå›´å†…éšæœºé€‰
    valid = [x for x in sorted_deployments if x[1] <= lowest_latency + buffer]
    return random.choice(valid)[0]
```

**Go ç¿»è¯‘**ï¼š

```go
func (r *LowestLatencyRouter) Pick(deployments []*Deployment) *Deployment {
    sort.Slice(deployments, func(i, j int) bool {
        return deployments[i].AvgLatency < deployments[j].AvgLatency
    })
    lowest := deployments[0].AvgLatency
    buffer := r.LatencyBuffer * lowest
    
    var valid []*Deployment
    for _, d := range deployments {
        if d.AvgLatency <= lowest+buffer {
            valid = append(valid, d)
        }
    }
    return valid[rand.Intn(len(valid))]
}
```

### 7. å†·å´åˆ¤æ–­é€»è¾‘

| æ–‡ä»¶ | å†…å®¹ | ç¿»è¯‘éš¾åº¦ |
|------|------|----------|
| router_utils/cooldown_handlers.py çš„ _is_cooldown_required() | åˆ¤æ–­æ˜¯å¦éœ€è¦å†·å´ | â­â­ |
| router_utils/cooldown_handlers.py çš„ _should_cooldown_deployment() | åˆ¤æ–­æ˜¯å¦è§¦å‘å†·å´ | â­â­ |

**Python**ï¼š

```python
def _is_cooldown_required(model_id, exception_status):
    if exception_status >= 400 and exception_status < 500:
        if exception_status == 429:  # Rate Limit
            return True
        elif exception_status == 401:  # Auth Error
            return True
        elif exception_status == 408:  # Timeout
            return True
        else:
            return False  # å…¶ä»– 4xx ä¸å†·å´
    return True  # 5xx éƒ½å†·å´
```

**Go ç¿»è¯‘**ï¼š

```go
func IsCooldownRequired(statusCode int) bool {
    if statusCode >= 400 && statusCode < 500 {
        switch statusCode {
        case 429, 401, 408, 404:
            return true
        default:
            return false
        }
    }
    return true // 5xx éƒ½å†·å´
}
```

## äºŒã€éœ€è¦ç”¨ Go ç‰¹æ€§é‡å†™çš„éƒ¨åˆ†

è¿™äº›ä»£ç ä¾èµ– Python ç‰¹æ€§æˆ–æ€§èƒ½æ•æ„Ÿï¼Œå¿…é¡»ç”¨ Go çš„æ–¹å¼é‡æ–°å®ç°ï¼š

### 1. ğŸ”¥ SSE æµå¼è½¬å‘ï¼ˆæœ€å¤æ‚ï¼‰

**Python å®ç°é—®é¢˜**ï¼š
- ä½¿ç”¨ async for + yield çš„ generator æ¨¡å¼
- ä¾èµ– Python çš„ asyncio äº‹ä»¶å¾ªç¯
- å†…å­˜ç®¡ç†ä¸å¯æ§

**Go é‡å†™è¦ç‚¹**ï¼š

```go
type SSEForwarder struct {
    upstream   io.ReadCloser
    downstream http.ResponseWriter
    clientCtx  context.Context
    bufferPool *sync.Pool  // å¤ç”¨ buffer
}

func (f *SSEForwarder) Forward() error {
    buf := f.bufferPool.Get().([]byte)
    defer f.bufferPool.Put(buf)
    
    reader := bufio.NewReader(f.upstream)
    flusher, _ := f.downstream.(http.Flusher)
    
    for {
        select {
        case <-f.clientCtx.Done():
            // ğŸ”¥ å…³é”®ï¼šclient æ–­å¼€ç«‹å³å–æ¶ˆä¸Šæ¸¸
            return f.clientCtx.Err()
        default:
            line, err := reader.ReadBytes('\n')
            if err == io.EOF {
                return nil
            }
            if err != nil {
                return err
            }
            
            // å†™å…¥ä¸‹æ¸¸
            f.downstream.Write(line)
            flusher.Flush()
        }
    }
}
```

**å…³é”®å·®å¼‚**ï¼š

| Python | Go |
|--------|-----|
| async for chunk in stream | for { reader.ReadBytes('\n') } |
| GC è‡ªåŠ¨å›æ”¶ | sync.Pool æ‰‹åŠ¨å¤ç”¨ buffer |
| asyncio cancel | context.Context cancel |
| æ— èƒŒå‹æ§åˆ¶ | å¯ä»¥åŠ  chan åšèƒŒå‹ |

### 2. ğŸ”¥ HTTP Client è¿æ¥æ± 

**Python å®ç°**ï¼š
- ä½¿ç”¨ httpx.AsyncClient æˆ– aiohttp
- è¿æ¥æ± é…ç½®åˆ†æ•£åœ¨å¤šå¤„
- SSL é…ç½®å¤æ‚

**Go é‡å†™**ï¼š

```go
type ProviderClient struct {
    client    *http.Client
    transport *http.Transport
    semaphore chan struct{}  // å¹¶å‘æ§åˆ¶
}

func NewProviderClient(maxConns int, timeout time.Duration) *ProviderClient {
    transport := &http.Transport{
        MaxIdleConns:        maxConns,
        MaxIdleConnsPerHost: maxConns,
        IdleConnTimeout:     90 * time.Second,
        // ğŸ”¥ å…³é”®ï¼šè¿æ¥å¤ç”¨
        DisableKeepAlives:   false,
    }
    
    return &ProviderClient{
        client: &http.Client{
            Transport: transport,
            Timeout:   timeout,
        },
        semaphore: make(chan struct{}, maxConns),
    }
}

func (c *ProviderClient) Do(ctx context.Context, req *http.Request) (*http.Response, error) {
    // è·å– permit
    select {
    case c.semaphore <- struct{}{}:
        defer func() { <-c.semaphore }()
    case <-ctx.Done():
        return nil, ctx.Err()
    }
    
    return c.client.Do(req.WithContext(ctx))
}
```

### 3. ğŸ”¥ ç†”æ–­å™¨ (Circuit Breaker)

**Python å®ç°**ï¼š
- LiteLLM ç”¨ç®€å•çš„ cooldown cacheï¼ˆä¸æ˜¯çœŸæ­£çš„ç†”æ–­å™¨ï¼‰
- åŸºäºæ—¶é—´çª—å£çš„å¤±è´¥è®¡æ•°

**Go é‡å†™ï¼ˆä½¿ç”¨ gobreakerï¼‰**ï¼š

```go
import "github.com/sony/gobreaker"

type ProviderBreaker struct {
    breakers map[string]*gobreaker.CircuitBreaker  // per-provider
    mu       sync.RWMutex
}

func NewProviderBreaker() *ProviderBreaker {
    return &ProviderBreaker{
        breakers: make(map[string]*gobreaker.CircuitBreaker),
    }
}

func (pb *ProviderBreaker) GetBreaker(providerID string) *gobreaker.CircuitBreaker {
    pb.mu.RLock()
    if cb, ok := pb.breakers[providerID]; ok {
        pb.mu.RUnlock()
        return cb
    }
    pb.mu.RUnlock()
    
    pb.mu.Lock()
    defer pb.mu.Unlock()
    
    // Double check
    if cb, ok := pb.breakers[providerID]; ok {
        return cb
    }
    
    cb := gobreaker.NewCircuitBreaker(gobreaker.Settings{
        Name:        providerID,
        MaxRequests: 3,                    // åŠå¼€çŠ¶æ€æœ€å¤šæ”¾ 3 ä¸ªè¯·æ±‚
        Interval:    60 * time.Second,     // ç»Ÿè®¡çª—å£
        Timeout:     30 * time.Second,     // ç†”æ–­åå¤šä¹…å°è¯•æ¢å¤
        ReadyToTrip: func(counts gobreaker.Counts) bool {
            // ğŸ”¥ å…³é”®ï¼šå¤±è´¥ç‡ > 50% ä¸”è¯·æ±‚æ•° > 10 æ‰ç†”æ–­
            failureRatio := float64(counts.TotalFailures) / float64(counts.Requests)
            return counts.Requests >= 10 && failureRatio >= 0.5
        },
    })
    
    pb.breakers[providerID] = cb
    return cb
}
```

### 4. ğŸ”¥ é…ç½®çƒ­é‡è½½

**Python å®ç°**ï¼š
- LiteLLM éœ€è¦é‡å¯æ‰èƒ½æ›´æ–°é…ç½®
- æ²¡æœ‰çœŸæ­£çš„çƒ­é‡è½½

**Go é‡å†™**ï¼š

```go
type ConfigManager struct {
    config  atomic.Pointer[Config]
    watcher *fsnotify.Watcher
}

func (cm *ConfigManager) Watch(ctx context.Context, path string) error {
    watcher, _ := fsnotify.NewWatcher()
    cm.watcher = watcher
    watcher.Add(path)
    
    // é˜²æŠ–
    debounce := time.NewTimer(0)
    <-debounce.C
    
    for {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case event := <-watcher.Events:
            if event.Op&fsnotify.Write == fsnotify.Write {
                // é˜²æŠ–ï¼š500ms å†…å¤šæ¬¡å˜æ›´åªè§¦å‘ä¸€æ¬¡
                debounce.Reset(500 * time.Millisecond)
            }
        case <-debounce.C:
            newConfig, err := LoadConfig(path)
            if err != nil {
                log.Error("reload failed, keeping old config", "err", err)
                continue
            }
            // ğŸ”¥ å…³é”®ï¼šåŸå­æ›¿æ¢
            cm.config.Store(newConfig)
            log.Info("config reloaded")
        }
    }
}

func (cm *ConfigManager) Get() *Config {
    return cm.config.Load()
}
```

### 5. ğŸ”¥ ä¼˜é›…å…³é—­ (Graceful Shutdown)

**Python å®ç°**ï¼š
- ç®€å•çš„ signal handler
- æ²¡æœ‰ drain mode

**Go é‡å†™**ï¼š

```go
type Server struct {
    httpServer   *http.Server
    activeConns  sync.WaitGroup
    shuttingDown atomic.Bool
}

func (s *Server) Shutdown(ctx context.Context) error {
    s.shuttingDown.Store(true)
    
    // 1. åœæ­¢æ¥æ”¶æ–°è¯·æ±‚
    s.httpServer.SetKeepAlivesEnabled(false)
    
    // 2. ç­‰å¾…ç°æœ‰è¯·æ±‚å®Œæˆï¼ˆæœ€å¤š 60sï¼‰
    done := make(chan struct{})
    go func() {
        s.activeConns.Wait()
        close(done)
    }()
    
    select {
    case <-done:
        log.Info("all connections drained")
    case <-ctx.Done():
        log.Warn("drain timeout, forcing shutdown")
    }
    
    // 3. å…³é—­ HTTP server
    return s.httpServer.Shutdown(ctx)
}

// ä¸­é—´ä»¶ï¼šè¿½è¸ªæ´»è·ƒè¿æ¥
func (s *Server) TrackConnection(next http.Handler) http.Handler {
    return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        if s.shuttingDown.Load() {
            http.Error(w, "server shutting down", http.StatusServiceUnavailable)
            return
        }
        s.activeConns.Add(1)
        defer s.activeConns.Done()
        next.ServeHTTP(w, r)
    })
}
```

### 6. ğŸ”¥ Metrics åŸ‹ç‚¹

**Python å®ç°**ï¼š
- åˆ†æ•£åœ¨å„å¤„çš„ logging
- æ²¡æœ‰ç»“æ„åŒ–çš„ metrics

**Go é‡å†™**ï¼š

```go
var (
    requestsTotal = promauto.NewCounterVec(prometheus.CounterOpts{
        Name: "llm_requests_total",
        Help: "Total LLM requests",
    }, []string{"provider", "model", "status"})
    
    requestLatency = promauto.NewHistogramVec(prometheus.HistogramOpts{
        Name:    "llm_request_latency_seconds",
        Help:    "Request latency",
        Buckets: []float64{0.1, 0.5, 1, 2, 5, 10, 30, 60},
    }, []string{"provider", "model"})
    
    tokenUsage = promauto.NewCounterVec(prometheus.CounterOpts{
        Name: "llm_token_usage_total",
        Help: "Token usage",
    }, []string{"provider", "model", "type"})  // type: input/output
)

// ä¸­é—´ä»¶
func MetricsMiddleware(provider, model string) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            start := time.Now()
            
            // åŒ…è£… ResponseWriter æ•è·çŠ¶æ€ç 
            wrapped := &statusRecorder{ResponseWriter: w, status: 200}
            next.ServeHTTP(wrapped, r)
            
            // è®°å½• metrics
            status := strconv.Itoa(wrapped.status)
            requestsTotal.WithLabelValues(provider, model, status).Inc()
            requestLatency.WithLabelValues(provider, model).Observe(time.Since(start).Seconds())
        })
    }
}
```

### 7. ğŸ”¥ å¹¶å‘æ§åˆ¶ (Semaphore / Bulkhead)

**Python å®ç°**ï¼š
- æ²¡æœ‰ per-provider çš„å¹¶å‘é™åˆ¶
- å…¨å±€å…±äº«èµ„æº

**Go é‡å†™**ï¼š

```go
type Bulkhead struct {
    semaphores map[string]chan struct{}
    mu         sync.RWMutex
}

func NewBulkhead(limits map[string]int) *Bulkhead {
    b := &Bulkhead{semaphores: make(map[string]chan struct{})}
    for provider, limit := range limits {
        b.semaphores[provider] = make(chan struct{}, limit)
    }
    return b
}

func (b *Bulkhead) Acquire(ctx context.Context, provider string) error {
    b.mu.RLock()
    sem, ok := b.semaphores[provider]
    b.mu.RUnlock()
    
    if !ok {
        return nil // æ²¡æœ‰é™åˆ¶
    }
    
    select {
    case sem <- struct{}{}:
        return nil
    case <-ctx.Done():
        return fmt.Errorf("bulkhead: %s is full", provider)
    }
}

func (b *Bulkhead) Release(provider string) {
    b.mu.RLock()
    sem, ok := b.semaphores[provider]
    b.mu.RUnlock()
    
    if ok {
        <-sem
    }
}
```

## ä¸‰ã€æ€»ç»“å¯¹ç…§è¡¨

| æ¨¡å— | ç¿»è¯‘/é‡å†™ | å·¥ä½œé‡ | ä¼˜å…ˆçº§ |
|------|----------|--------|--------|
| ç±»å‹å®šä¹‰ | ç¿»è¯‘ | â­ | P0 |
| å¼‚å¸¸å®šä¹‰ | ç¿»è¯‘ | â­ | P0 |
| å‚æ•°æ˜ å°„ | ç¿»è¯‘ | â­â­ | P1 |
| è¯·æ±‚è½¬æ¢ | ç¿»è¯‘ | â­â­ | P1 |
| å“åº”è½¬æ¢ | ç¿»è¯‘ | â­â­ | P1 |
| ä»·æ ¼è¡¨ | ç›´æ¥å¤åˆ¶ | â­ | P1 |
| è·¯ç”±é€‰æ‹©é€»è¾‘ | ç¿»è¯‘ | â­â­ | P2 |
| å†·å´åˆ¤æ–­é€»è¾‘ | ç¿»è¯‘ | â­â­ | P2 |
| SSE æµå¼è½¬å‘ | é‡å†™ | â­â­â­â­â­ | P0 |
| HTTP Client | é‡å†™ | â­â­â­ | P0 |
| ç†”æ–­å™¨ | é‡å†™ | â­â­â­ | P1 |
| é…ç½®çƒ­é‡è½½ | é‡å†™ | â­â­â­ | P1 |
| ä¼˜é›…å…³é—­ | é‡å†™ | â­â­ | P1 |
| Metrics | é‡å†™ | â­â­ | P0 |
| å¹¶å‘æ§åˆ¶ | é‡å†™ | â­â­â­ | P1 |

## å››ã€å»ºè®®çš„å¼€å‘é¡ºåº

**Week 1: éª¨æ¶**
```
â”œâ”€â”€ å®šä¹‰ Go interface (Provider, Router)
â”œâ”€â”€ å®ç° HTTP Server
â”œâ”€â”€ å®ç° Metrics ä¸­é—´ä»¶
â””â”€â”€ å®ç° OpenAI Provider (æ‰‹å†™ï¼Œä½œä¸ºæ¨¡æ¿)
```

**Week 2: AI æ‰¹é‡ç”Ÿæˆ**
```
â”œâ”€â”€ è®© AI ç¿»è¯‘æ‰€æœ‰ç±»å‹å®šä¹‰
â”œâ”€â”€ è®© AI ç¿»è¯‘ Anthropic/Azure/Gemini adapter
â””â”€â”€ äººå·¥ review
```

**Week 3: æµå¼ (æœ€éš¾)**
```
â”œâ”€â”€ å®ç° SSE è½¬å‘æ ¸å¿ƒ
â”œâ”€â”€ sync.Pool buffer å¤ç”¨
â”œâ”€â”€ client æ–­å¼€æ£€æµ‹
â””â”€â”€ æµ‹è¯•å„ç§è¾¹ç•Œæƒ…å†µ
```

**Week 4: é«˜å¯ç”¨**
```
â”œâ”€â”€ é›†æˆ gobreaker
â”œâ”€â”€ å®ç° Bulkhead
â”œâ”€â”€ å®ç°é…ç½®çƒ­é‡è½½
â””â”€â”€ å®ç°ä¼˜é›…å…³é—­
```

**Week 5: æ‰“ç£¨**
```
â”œâ”€â”€ å‹æµ‹
â”œâ”€â”€ è°ƒå‚
â”œâ”€â”€ Docker é•œåƒ
â””â”€â”€ æ–‡æ¡£
```