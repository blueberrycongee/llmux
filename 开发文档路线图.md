# LLMux 开发路线图

## 项目定位
用 Go 语言重构 LiteLLM 核心功能，打造生产级、高性能的 LLM 网关。目标是解决 Python 版本的性能瓶颈（GIL、内存泄漏、冷启动慢），同时完美继承其模型兼容性和 OpenAI 协议统一的核心价值。

---

## Phase 1: 骨架搭建（Week 1-2）
**目标**：跑通 OpenAI 非流式请求，Prometheus 有数据输出

### 核心任务：

1. **初始化 Go 项目结构**
   ```
   llmux/
   ├── cmd/server/
   │   └── main.go
   ├── internal/
   │   ├── api/
   │   │   └── handler.go
   │   ├── provider/
   │   │   ├── interface.go
   │   │   ├── openai.go
   │   │   └── manager.go
   │   ├── router/
   │   │   ├── interface.go
   │   │   ├── simple.go
   │   │   └── manager.go
   │   ├── config/
   │   │   └── manager.go
   │   └── metrics/
   │       └── middleware.go
   ├── pkg/
   │   ├── types/
   │   └── errors/
   └── config/
       └── config.yaml
   ```

2. **定义核心接口**
   ```go
   // Provider interface
   type Provider interface {
       Name() string
       SupportedModels() []string
       MapParams(openaiParams map[string]any) map[string]any
       BuildRequest(ctx context.Context, req *ChatRequest) (*http.Request, error)
       ParseResponse(resp *http.Response) (*ChatResponse, error)
       ParseStreamChunk(chunk []byte) (*StreamChunk, error)
       MapError(statusCode int, body []byte) error
   }
   
   // Router interface
   type Router interface {
       Pick(ctx context.Context, model string) (*Deployment, error)
       ReportSuccess(deployment *Deployment, latency time.Duration)
       ReportFailure(deployment *Deployment, err error)
       IsCircuitOpen(deployment *Deployment) bool
   }
   
   // ConfigManager interface
   type ConfigManager interface {
       Get() *Config
       Watch(ctx context.Context) <-chan *Config
       Update(newConfig *Config) error
   }
   ```

3. **实现 HTTP Server**（使用 net/http）
4. **集成 Prometheus metrics 中间件**（request_total, latency_seconds, token_usage）
5. **集成 slog 结构化日志**
6. **实现配置加载 + fsnotify 热重载**（atomic.Pointer 原子替换）
7. **手写 OpenAI Provider 作为模板**
8. **跑通 `/v1/chat/completions`**（非流式）

### 验收标准：
- `curl` 能打通 OpenAI 请求
- Prometheus `/metrics` 有数据
- 配置热重载生效

---

## Phase 2: AI 批量生成 Provider（Week 3）
**目标**：覆盖 4-5 个主流 Provider

### 核心任务：

1. **从 litellm-main 提取类型定义，让 AI 翻译**
   - `types/utils.py` → Go struct（ModelResponse, Usage, Choices）
   - `types/llms/openai.py`, `anthropic.py` → Go struct
   - `exceptions.py` → Go error types

2. **让 AI 翻译 Provider Adapter**
   - `llms/anthropic/chat/transformation.py` → AnthropicProvider
   - `llms/azure/chat/gpt_transformation.py` → AzureProvider
   - `llms/gemini/chat/transformation.py` → GeminiProvider

3. **直接复制价格表**
   ```go
   //go:embed model_prices_and_context_window.json
   var modelPricesJSON []byte
   ```

4. **AI 生成参数映射表**（OpenAI → Anthropic/Azure/Gemini）

5. **人工 Review + 修正**，每个 Adapter 至少跑一个真实请求验证

### 验收标准：
- 支持 OpenAI/Claude/Gemini/Azure
- 可通过配置切换 Provider
- 参数映射正确

---

## Phase 3: SSE 流式支持（Week 4）
**目标**：流式转发稳定可靠，无 goroutine 泄漏

### 核心任务：

1. **实现 SSE 转发核心逻辑**
   ```go
   type SSEForwarder struct {
       upstream   io.ReadCloser
       downstream http.ResponseWriter
       clientCtx  context.Context
       bufferPool *sync.Pool
   }
   ```

2. **sync.Pool buffer 复用**，避免高频 GC
3. **client 断开检测 + 上游 cancel**（context.Done 监听）
4. **各 Provider 的流式响应格式适配**（Anthropic 的 `event:` 前缀处理）
5. **编写流式相关测试用例**（mock server + 异常注入）

### 验收标准：
- SSE 正常工作
- client 断开能立即 cancel 上游
- 无内存泄漏
- goroutine 数量稳定

---

## Phase 4: 高可用（Week 5-6）
**目标**：生产级稳定性，能扛住流量突增

### 核心任务：

1. **集成 gobreaker 熔断器**（per-provider 独立熔断）
   - 失败率 > 50% 且请求数 > 10 触发熔断
   - 半开状态最多放 3 个请求探测

2. **实现 Bulkhead 舱壁模式**（per-provider semaphore 并发控制）
3. **实现 Token Bucket 限流**（支持 per-key、per-team 维度）
4. **实现冷却判断逻辑**（429/401/408/404 触发冷却，其他 4xx 不冷却）
5. **实现路由策略**：
   - Simple Shuffle（随机）
   - Lowest Latency（最低延迟 + buffer 范围内随机）
   - Least Busy（最少并发）
6. **实现优雅关闭**（SIGTERM → drain mode → 等待 60s → 强杀）
7. **健康检查端点**（`/health/live`, `/health/ready`）
8. **压测 + 调参**（目标：1000 QPS 不崩，P99 < 100ms）

### 验收标准：
- 压测通过
- 熔断生效
- 上游挂了不会雪崩
- 优雅关闭不丢请求

---

## Phase 5: 可观测性增强（Week 7）
**目标**：成为 AI 流量的"显微镜"

### 核心任务：

1. **集成 OpenTelemetry tracing**（Span 注入 gen_ai.system, gen_ai.request.model）
2. **实现日志脱敏**（API Key、PII 自动 mask）
3. **完善 Prometheus metrics**：
   ```go
   // Token 用量
   llm_token_usage_total{team, model, type}  // type: input/output
   
   // 延迟分布
   llm_request_latency_seconds_bucket{provider, model}
   
   // 错误统计
   llm_upstream_error_total{error_type}
   ```
4. **集成 tiktoken-go 估算 Token**（流式场景用估算，非流式信任上游 usage）
5. **成本计算逻辑**（读取价格表，按 token 计费）

### 验收标准：
- Grafana 能看到完整监控面板
- Token 用量、延迟分布、错误率、成本归因清晰
- 日志中敏感信息被脱敏

---

## Phase 6: 云原生打包（Week 8）
**目标**：docker pull 就能跑，镜像 < 20MB

### 核心任务：

1. **多阶段 Dockerfile**（distroless/scratch 基础镜像）
2. **Helm Chart**（Deployment, Service, ConfigMap, HPA）
3. **K8s manifests 示例**
4. **CI/CD pipeline**（GitHub Actions：lint → test → build → push）
5. **支持 Unix Domain Socket**（Sidecar 部署场景）
6. **文档完善**（README、配置说明、迁移指南）

### 验收标准：
- 镜像 < 20MB
- 冷启动 < 1s
- Helm install 一键部署
- CI/CD 全自动

---

## 里程碑检查点

| 里程碑 | 时间 | 验收标准 |
|--------|------|----------|
| M1: 骨架完成 | Week 2 | curl 打通 OpenAI 非流式，Prometheus 有数据 |
| M2: 多 Provider | Week 3 | 支持 OpenAI/Claude/Gemini/Azure |
| M3: 流式可用 | Week 4 | SSE 正常，client 断开能 cancel 上游 |
| M4: 高可用 | Week 6 | 压测 1000 QPS 不崩，熔断生效 |
| M5: 可观测 | Week 7 | Grafana 完整监控面板 |
| M6: 可部署 | Week 8 | 镜像 < 20MB，Helm 一键部署 |

---

## 技术选型

| 模块 | 选型 | 理由 |
|------|------|------|
| HTTP Server | net/http 或 fasthttp | 标准库足够，fasthttp 可选 |
| 熔断器 | sony/gobreaker | 成熟稳定 |
| 配置热重载 | fsnotify + atomic.Pointer | 零停机更新 |
| Metrics | prometheus/client_golang | 行业标准 |
| 日志 | slog (Go 1.21+) | 官方支持，结构化 |
| Tracing | OpenTelemetry SDK | 云原生标准 |
| Token 计数 | pkoukk/tiktoken-go | 纯 Go 实现 |
| 限流器 | golang.org/x/time/rate | 官方库，支持 Token Bucket |
| 配置格式 | YAML | 可读性好，结构清晰 |
| 测试框架 | testify | 社区广泛使用 |

---

## 风险与应对

| 风险 | 应对策略 |
|------|----------|
| AI 生成代码有 bug | 每个 Adapter 必须跑真实请求验证 |
| Provider API 变更 | Adapter 设计易于修改，关注 changelog |
| 流式实现复杂 | 先做 OpenAI（最标准），其他参考它改 |
| 性能不达预期 | 早期加 benchmark，持续监控 |
| 与 Python 版数据兼容 | 复用 Redis key 结构和 Postgres schema |
| Provider 过多维护困难 | 插件化设计，支持外部 Adapter |

---

## 工作分配建议

### AI 批量生成（脏活累活）：
- 类型定义翻译（struct + json tag）
- 参数映射表
- 错误码映射
- 单元测试 cases
- 文档注释生成

### 人工把控（精细活）：
- SSE 流式转发（背压、连接检测、buffer 复用）
- 熔断器调参
- 配置热重载（原子性、一致性）
- Metrics label 设计（cardinality 控制）
- 超时层级设计
- 核心接口定义
- 性能调优
- 安全审计

---

## 核心接口定义（详细版）

```go
// 核心请求/响应类型
type ChatRequest struct {
    Model    string        `json:"model"`
    Messages []ChatMessage `json:"messages"`
    Stream   bool          `json:"stream,omitempty"`
    MaxTokens int          `json:"max_tokens,omitempty"`
    Temperature float64    `json:"temperature,omitempty"`
    // ... 其他 OpenAI 兼容参数
}

type ChatResponse struct {
    ID      string    `json:"id"`
    Object  string    `json:"object"`
    Created int64     `json:"created"`
    Model   string    `json:"model"`
    Choices []Choice  `json:"choices"`
    Usage   *Usage    `json:"usage,omitempty"`
}

// 部署配置
type Deployment struct {
    ID           string            `json:"id"`
    ModelName    string            `json:"model_name"`
    Provider     string            `json:"provider"`
    BaseURL      string            `json:"base_url"`
    APIKey       string            `json:"-"`
    MaxConcurrent int              `json:"max_concurrent"`
    Timeout      time.Duration     `json:"timeout"`
    MaxRetries   int               `json:"max_retries"`
    CoolDown     time.Duration     `json:"cool_down"`
    Metadata     map[string]string `json:"metadata"`
}
```

---

## 性能目标
- 延迟：P99 < 100ms
- 吞吐：1000 QPS
- 内存：< 100MB
- 连接数：支持 10k 并发
- 冷启动：< 1s
- 镜像大小：< 20MB