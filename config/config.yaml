# LLMux Gateway Configuration
# Environment variables can be used with ${VAR_NAME} syntax

server:
  port: 8080
  read_timeout: 30s
  write_timeout: 120s
  idle_timeout: 60s

providers:
  - name: openai
    type: openai
    api_key: ${OPENAI_API_KEY}
    base_url: https://api.openai.com/v1
    models:
      - gpt-4o
      - gpt-4o-mini
      - gpt-4-turbo
      - gpt-3.5-turbo
    max_concurrent: 100
    timeout: 60s

  # Anthropic Claude
  - name: anthropic
    type: anthropic
    api_key: ${ANTHROPIC_API_KEY}
    base_url: https://api.anthropic.com
    models:
      - claude-3-5-sonnet-20241022
      - claude-3-5-haiku-20241022
      - claude-3-opus-20240229
    max_concurrent: 50
    timeout: 120s

  # Google Gemini
  - name: gemini
    type: gemini
    api_key: ${GOOGLE_API_KEY}
    base_url: https://generativelanguage.googleapis.com
    models:
      - gemini-1.5-pro
      - gemini-1.5-flash
      - gemini-2.0-flash-exp
    max_concurrent: 50
    timeout: 60s

  # Azure OpenAI (example - uncomment and configure)
  # - name: azure-openai
  #   type: azure
  #   api_key: ${AZURE_OPENAI_API_KEY}
  #   base_url: https://your-resource.openai.azure.com
  #   models:
  #     - gpt-4
  #   max_concurrent: 50
  #   timeout: 60s
  #   headers:
  #     api-version: "2024-02-15-preview"

routing:
  default_provider: openai
  strategy: simple-shuffle  # simple-shuffle, lowest-latency, least-busy
  fallback_enabled: true
  retry_count: 3
  cooldown_period: 60s

rate_limit:
  enabled: false
  requests_per_minute: 60
  burst_size: 10

logging:
  level: info   # debug, info, warn, error
  format: json  # json, text

metrics:
  enabled: true
  path: /metrics

# OpenTelemetry Tracing
tracing:
  enabled: false
  endpoint: localhost:4317  # OTLP gRPC endpoint (Jaeger, Tempo, etc.)
  service_name: llmux
  sample_rate: 1.0          # 1.0 = 100% sampling, 0.1 = 10% sampling
  insecure: true            # Set to false for TLS connections
