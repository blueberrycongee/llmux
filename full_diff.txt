diff --git a/.agent/docs/reports/audit_report_final.md b/.agent/docs/reports/audit_report_final.md
index cb88545..8924816 100644
--- a/.agent/docs/reports/audit_report_final.md
+++ b/.agent/docs/reports/audit_report_final.md
@@ -1,116 +1,57 @@
-# ðŸ›¡ï¸ LLMux ç»ˆæžä»£ç å®¡è®¡æŠ¥å‘Š (Final Audit Report)
-
----
-
-## ðŸš€ ä¿®å¤è¿›åº¦ (Remediation Progress)
-
-| æ—¥æœŸ       | ä¿®å¤é¡¹         | çŠ¶æ€     | æäº¤                                                                                   |
-| ---------- | -------------- | -------- | -------------------------------------------------------------------------------------- |
-| 2026-01-10 | **åˆ†å¸ƒå¼é™æµ** | âœ… å·²å®Œæˆ | [3443482..8ab5cbc](https://github.com/blueberrycongee/llmux/compare/3443482...8ab5cbc) |
-
-### å·²å®Œæˆçš„åŽŸå­åŒ–å¼€å‘æ­¥éª¤ï¼š
-
-1. âœ… **Atom 1**: æ·»åŠ  `WithRateLimiter` å’Œ `WithRateLimiterConfig` é€‰é¡¹ (feat: add WithRateLimiter options)
-2. âœ… **Atom 2**: åœ¨ `Client` ç»“æž„ä½“ä¸­é›†æˆ `rateLimiter` å­—æ®µ (feat: integrate RateLimiter into Client)  
-3. âœ… **Atom 3**: å®žçŽ° `checkRateLimit()` æ–¹æ³• (feat: implement checkRateLimit method)
-4. âœ… **Atom 4**: åœ¨ `ChatCompletion`/`ChatCompletionStream`/`Embedding` ä¸­è°ƒç”¨é™æµæ£€æŸ¥ (feat: apply rate limiting to all request methods)
-5. âœ… **Atom 5**: æ›´æ–° Server `main.go` åˆå§‹åŒ– Redis Rate Limiter (feat: enable distributed rate limiting in server)
-
-### ä½¿ç”¨æ–¹æ³•ï¼š
-
-```yaml
-# config.yaml
-rate_limit:
-  enabled: true
-  distributed: true           # å¯ç”¨ Redis åˆ†å¸ƒå¼é™æµ
-  requests_per_minute: 100    # RPM é™åˆ¶
-  tokens_per_minute: 100000   # TPM é™åˆ¶
-  key_strategy: api_key       # é™æµé”®ç­–ç•¥
-
-cache:
-  redis:
-    addr: "localhost:6379"    # Redis åœ°å€
-```
-
----
-
-
-**å®¡è®¡æ—¥æœŸï¼š** 2026-01-10
-**å®¡è®¡å¯¹è±¡ï¼š** LLMux (Commit: HEAD)
-**æœ€ç»ˆå®šçº§ï¼š** **Single-Node Production Ready (å•æœºç”Ÿäº§çº§)**
-**æ ¸å¿ƒåˆ¤å†³ï¼š** ä»£ç è´¨é‡æžé«˜ï¼Œä½†åˆ†å¸ƒå¼èƒ½åŠ›å¤„äºŽâ€œæœªæ’ç”µâ€çŠ¶æ€ã€‚
-
-## 1. æ ¸å¿ƒäº‰è®®ç»ˆå®¡ (The Verdict on Controversies)
-
-é’ˆå¯¹ä¹‹å‰ä¸¤ä»½æŠ¥å‘Šæ‰“æž¶çš„ç„¦ç‚¹é—®é¢˜ï¼ŒåŸºäºŽä»£ç äº‹å®žçš„æœ€ç»ˆè£å†³å¦‚ä¸‹ï¼š
-
-| å®¡è®¡é¡¹         | çŠ¶æ€         | äº‹å®žä¾æ® (Code Truth)                                                                                                                                 | åˆ¤å†³                     |
-| :------------- | :----------- | :---------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------- |
-| **åˆ†å¸ƒå¼é™æµ** | âœ… **å·²å¯ç”¨** | `internal/resilience/redis_limiter.go` å·²åœ¨ `cmd/server/main.go` ä¸­æ­£ç¡®åˆå§‹åŒ–å¹¶æ³¨å…¥åˆ° Clientã€‚æ”¯æŒ RPM/TPM åˆ†å¸ƒå¼é™æµã€‚                               | **å·²ä¿®å¤ (Fixed)**       |
-| **åˆ†å¸ƒå¼è·¯ç”±** | âŒ **ä¸æ”¯æŒ** | `routers/leastbusy.go` è°ƒç”¨ `internal/router/base.go`ï¼Œå…¶çŠ¶æ€å­˜å‚¨åœ¨ `r.stats` (å†…å­˜ Map) ä¸­ã€‚å¤šå®žä¾‹éƒ¨ç½²æ—¶ï¼Œå„èŠ‚ç‚¹çŠ¶æ€éš”ç¦»ï¼Œè´Ÿè½½å‡è¡¡å°†é€€åŒ–ä¸ºéšæœºè½®è¯¢ã€‚ | **ä»…æ”¯æŒå•æœº**           |
-| **å¯è§‚æµ‹æ€§**   | âœ… **å·²é›†æˆ** | `cmd/server/main.go` (L108, L268) æ˜Žç¡®åˆå§‹åŒ–äº† OTel Tracing å’Œ Metrics ä¸­é—´ä»¶ã€‚ä¹‹å‰çš„æŠ¥å‘Šç§°å…¶ä¸ºâ€œåŠŸèƒ½å­¤å²›â€æ˜¯é”™è¯¯çš„ã€‚                                   | **å·²ä¸Šçº¿ (Operational)** |
-| **å¤šæ¨¡æ€**     | âŒ **ä¸æ”¯æŒ** | ä»£ç åº“ä¸­æ— ä»»ä½• Image/Audio å¤„ç†é€»è¾‘ã€‚                                                                                                                 | **å®Œå…¨ç¼ºå¤±**             |
-
----
-
-## 2. æ·±åº¦é£Žé™©è¯„ä¼° (Deep Risk Assessment)
-
-### ðŸ”´ è‡´å‘½é£Žé™©ï¼šåˆ†å¸ƒå¼å¹»è§‰ (The Distributed Illusion) [å·²éƒ¨åˆ†ä¿®å¤]
-*   **çŽ°è±¡ï¼š** å¼€å‘è€…ç¼–å†™äº†é«˜è´¨é‡çš„ Redis å®¢æˆ·ç«¯ (`caches/redis`) å’Œ Redis é™æµè„šæœ¬ (`redis_limiter.go`)ï¼Œè¿™ç»™ä»£ç å®¡æŸ¥è€…ä¸€ç§â€œæ”¯æŒåˆ†å¸ƒå¼â€çš„é”™è§‰ã€‚
-*   **çœŸç›¸ï¼š** è¿™äº›ç»„ä»¶ä¹‹å‰**æ²¡æœ‰è¢«ç»„è£…åœ¨ä¸€èµ·**ã€‚
-    *   `Router` ä»…æŽ¥å—æœ¬åœ°çŠ¶æ€ã€‚(ä»éœ€ä¿®å¤)
-    *   ~~`Client` åˆå§‹åŒ–æ—¶æ²¡æœ‰æä¾›æ³¨å…¥ `RedisLimiter` çš„é€‰é¡¹ã€‚~~ (**å·²ä¿®å¤ï¼š** Client çŽ°å·²é›†æˆ RedisLimiter)
-*   **åŽæžœï¼š** å¦‚æžœç”¨æˆ·å› ä¸ºçœ‹åˆ° `redis_limiter.go` å°±æ”¾å¿ƒåœ°å°† LLMux éƒ¨ç½²åˆ° Kubernetes é›†ç¾¤ä¸­ï¼Œ**é™æµå°†å®Œå…¨å¤±æ•ˆ**ã€‚(**æ›´æ–°ï¼š** åˆ†å¸ƒå¼é™æµçŽ°å·²ç”Ÿæ•ˆï¼Œä½†åˆ†å¸ƒå¼è·¯ç”±ä»æœªæ”¯æŒ)
-
-### ðŸŸ  é«˜å±é£Žé™©ï¼šé”™è¯¯å¤„ç†çš„â€œæ–¹è¨€â€é—®é¢˜
-*   **çŽ°è±¡ï¼š** `providers/openailike` å¼ºè¡Œå‡è®¾æ‰€æœ‰ä¸Šæ¸¸éƒ½è¿”å›ž OpenAI æ ¼å¼çš„ JSON é”™è¯¯ (`error.message`)ã€‚
-*   **çœŸç›¸ï¼š** Azure OpenAI è¿”å›ž XML æˆ–ä¸åŒçš„ JSON ç»“æž„ï¼›Anthropic æœ‰è‡ªå·±çš„é”™è¯¯ç ä½“ç³»ã€‚
-*   **åŽæžœï¼š** å½“é‡åˆ°éžæ ‡å‡†é”™è¯¯æ—¶ï¼ŒLLMux æ— æ³•æ­£ç¡®è§£æžé”™è¯¯ç±»åž‹ï¼ˆå¦‚åŒºåˆ†â€œå¯é‡è¯•çš„é™æµâ€ä¸Žâ€œä¸å¯é‡è¯•çš„åè¯·æ±‚â€ï¼‰ï¼Œå¯¼è‡´**é‡è¯•é£Žæš´**æˆ–**é”™è¯¯åžæ²¡**ã€‚
-
----
-
-## 3. æž¶æž„äº®ç‚¹ (Architectural Strengths)
-
-å°½ç®¡æœ‰ä¸Šè¿°é—®é¢˜ï¼ŒLLMux çš„åº•åº§è´¨é‡è¿œè¶…ä¸€èˆ¬çš„å¼€æºé¡¹ç›®ï¼Œå…·å¤‡æˆä¸ºé¡¶çº§ç½‘å…³çš„æ½œåŠ›ï¼š
-
-1.  **ðŸ”Œ ä¼ä¸šçº§æ’ä»¶ç³»ç»Ÿ (`internal/plugin`)**ï¼š
-    *   æ”¯æŒ `PreHook` / `PostHook` / `StreamHook`ã€‚
-    *   è®¾è®¡äº†å®Œå–„çš„ä¸Šä¸‹æ–‡ (`Context`) ä¼ é€’å’ŒçŸ­è·¯ (`ShortCircuit`) æœºåˆ¶ã€‚
-    *   è¿™æ˜¯å®žçŽ°è‡ªå®šä¹‰é‰´æƒã€è®¡è´¹ã€å®¡è®¡åŠŸèƒ½çš„å®Œç¾Žåˆ‡å…¥ç‚¹ã€‚
-2.  **ðŸ’¾ å¥å£®çš„ Redis å®¢æˆ·ç«¯ (`caches/redis`)**ï¼š
-    *   åŽŸç”Ÿæ”¯æŒ Clusterã€Sentinel å’Œ Failover æ¨¡å¼ã€‚
-    *   å°è£…äº† Pipeline å’ŒåŽŸå­æ“ä½œï¼Œä»£ç è´¨é‡å¾ˆé«˜ã€‚
-3.  **ðŸŒŠ ä¼˜ç§€çš„æµå¼å¤„ç† (`stream.go`)**ï¼š
-    *   åˆ©ç”¨ Go çš„å¹¶å‘ç‰¹æ€§ï¼Œå®žçŽ°äº†æ¯” Python (LiteLLM) æ›´ç¨³å®šçš„æµå¼è½¬å‘å’Œæ–­ç‚¹æ¢å¤é€»è¾‘ã€‚
-
----
-
-## 4. ä¿®å¤è·¯çº¿å›¾ (Remediation Roadmap)
-
-å¦‚æžœæ‚¨æ‰“ç®—å°† LLMux æŽ¨å‘ä¼ä¸šçº§ç”Ÿäº§çŽ¯å¢ƒï¼Œè¯·æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§è¿›è¡Œä¿®å¤ï¼š
-
-### Phase 1: æ¿€æ´»åˆ†å¸ƒå¼èƒ½åŠ› (å¿…é¡»åš)
-1.  âœ… **ä¿®æ”¹ `options.go`**ï¼šæ·»åŠ  `WithRateLimiter` é€‰é¡¹ã€‚(å·²å®Œæˆ)
-2.  âœ… **ä¿®æ”¹ `cmd/server/main.go`**ï¼š(å·²å®Œæˆ)
-    *   åˆå§‹åŒ– `redis.Client`ã€‚
-    *   åˆ›å»º `resilience.NewRedisLimiter(redisClient)`ã€‚
-    *   å°†å…¶æ³¨å…¥åˆ° `llmux.New(..., llmux.WithRateLimiter(limiter))`ã€‚
-3.  **æ”¹é€  `Router`**ï¼š(å¾…è¿›è¡Œ)
-    *   å®šä¹‰ `ClusterStats` æŽ¥å£ã€‚
-    *   ä½¿ç”¨ Redis å®žçŽ°è¯¥æŽ¥å£ï¼Œæ›¿æ¢ `internal/router/base.go` ä¸­çš„ `r.stats` Mapã€‚
-
-### Phase 2: å¢žå¼ºé²æ£’æ€§
-1.  **æŠ½è±¡é”™è¯¯è§£æžå™¨**ï¼šåœ¨ `Provider` æŽ¥å£ä¸­å¢žåŠ  `ErrorParser`ï¼Œå…è®¸ä¸åŒåŽ‚å•†æ³¨å…¥è‡ªå®šä¹‰çš„è§£æžé€»è¾‘ã€‚
-2.  **é…ç½®çƒ­åŠ è½½**ï¼šåˆ©ç”¨ `internal/config` ä¸­çš„ Watcher æœºåˆ¶ï¼Œå®žçŽ°è·¯ç”±ç­–ç•¥å’Œä»·æ ¼è¡¨çš„åŠ¨æ€æ›´æ–°ã€‚
-
----
-
-### 5. æœ€ç»ˆç»“è®º (Conclusion)
-
-**LLMux ä¸æ˜¯ä¸€ä¸ªçŽ©å…·ï¼Œå®ƒæ˜¯ä¸€è¾†ç»„è£…äº† 90% çš„æ³•æ‹‰åˆ©ã€‚**
-
-å¼•æ“Žï¼ˆGo å¹¶å‘æ¨¡åž‹ï¼‰ã€åº•ç›˜ï¼ˆæ’ä»¶ç³»ç»Ÿï¼‰å’Œè½®å­ï¼ˆRedis å®¢æˆ·ç«¯ï¼‰éƒ½æ˜¯é¡¶çº§çš„ã€‚åªæ˜¯å‡ºåŽ‚æ—¶ï¼Œå·¥ç¨‹å¸ˆå¿˜è®°æŠŠ**ä¼ åŠ¨è½´**ï¼ˆåˆ†å¸ƒå¼çŠ¶æ€åŒæ­¥ï¼‰æŽ¥ä¸Šã€‚
-
-**æœ€æ–°è¿›å±•ï¼š** å·¥ç¨‹å¸ˆåˆšåˆšæŽ¥ä¸Šäº†**å‰è½®ä¼ åŠ¨è½´**ï¼ˆåˆ†å¸ƒå¼é™æµï¼‰ï¼Œè½¦è¾†å·²ç»å…·å¤‡äº†åœ¨é›†ç¾¤ä¸­å®‰å…¨è¡Œé©¶çš„èƒ½åŠ›ã€‚ä¸‹ä¸€æ­¥æ˜¯æŽ¥ä¸Š**åŽè½®ä¼ åŠ¨è½´**ï¼ˆåˆ†å¸ƒå¼è·¯ç”±ï¼‰ï¼Œå®žçŽ°å®Œå…¨çš„å››é©±æ€§èƒ½ã€‚
-
-åªè¦å®Œæˆå‰©ä¸‹çš„å·¥ä½œï¼Œå®ƒå°†ç«‹å³è¶…è¶Š LiteLLMã€‚
+# å®¡è®¡è¿½è¸ªæ—¥å¿— (Audit Trace Log)
+
+## 1. æ–­é“¾ä¸Žæ­»ä»£ç  (Broken Links & Dead Code)
+
+### [ä¸¥é‡] æ¨¡å—å/åŠŸèƒ½ï¼šä¸‹æ¸¸é™æµ (Downstream Rate Limiting)
+*   **å®šä¹‰ä½ç½®ï¼š** `internal/auth/ratelimiter.go` (TenantRateLimiter)
+*   **è¿½è¸ªç»“æžœï¼š**
+    *   **å®šä¹‰ï¼š** `TenantRateLimiter` åŠå…¶æ–¹æ³• `RateLimitMiddleware` åœ¨ `internal/auth` åŒ…ä¸­è¢«å®Œæ•´å®šä¹‰ã€‚
+    *   **æ–­é“¾ï¼š** åœ¨ `cmd/server/main.go` çš„åˆå§‹åŒ–æµç¨‹ä¸­ï¼Œè™½ç„¶åˆå§‹åŒ–äº† `authMiddleware`ï¼Œä½†**ä»Žæœªè°ƒç”¨** `RateLimitMiddleware`ã€‚
+    *   **è¯æ®ï¼š** `main.go` ç¬¬ 248 è¡Œä»…è°ƒç”¨äº† `authMiddleware.Authenticate(httpHandler)`ã€‚è€Œåœ¨ `internal/auth/middleware.go` çš„ `Authenticate` æ–¹æ³•ä¸­ï¼ˆç¬¬ 50-134 è¡Œï¼‰ï¼Œä»…æ£€æŸ¥äº† `IsOverBudget`ï¼Œ**å®Œå…¨æ²¡æœ‰è°ƒç”¨** `RateLimiter.Check()` æˆ–ä»»ä½•é™æµé€»è¾‘ã€‚
+    *   **ç»“è®ºï¼š** æ‰€è°“çš„â€œç½‘å…³å±‚é™æµâ€æ˜¯**è™šå‡çš„**ã€‚ä»£ç å­˜åœ¨ä½†æœªå®žè£…ï¼Œæµé‡å¯ä»¥æ— é™åˆ¶åœ°ç©¿é€åˆ°åº”ç”¨å±‚ï¼ˆè™½ç„¶ `client.go` æœ‰åº”ç”¨å±‚é™æµï¼Œä½†ç½‘å…³å±‚é˜²å¾¡å¤±æ•ˆï¼‰ã€‚
+
+### [ä¸¥é‡] æ¨¡å—å/åŠŸèƒ½ï¼šè·¯ç”±å®žçŽ°åˆ†è£‚ (Zombie Router Implementation)
+*   **å®šä¹‰ä½ç½®ï¼š** `internal/router/` (simple_shuffle.go, least_busy.go ç­‰)
+*   **è¿½è¸ªç»“æžœï¼š**
+    *   **å®šä¹‰ï¼š** `internal/router` åŒ…ä¸­åŒ…å«äº†ä¸€æ•´å¥—è·¯ç”±ç­–ç•¥å®žçŽ°ã€‚
+    *   **æ–­é“¾ï¼š** `client.go` (ç¬¬ 895 è¡Œ) ä½¿ç”¨çš„æ˜¯ `routers` åŒ… (`github.com/blueberrycongee/llmux/routers`) æ¥åˆ›å»ºè·¯ç”±å™¨ã€‚
+    *   **è¯æ®ï¼š** `main.go` ä»…ä½¿ç”¨äº† `internal/router` ä¸­çš„ `NewRedisStatsStore` (ç¬¬ 418 è¡Œ)ã€‚`internal/router` ä¸­çš„æ‰€æœ‰å…·ä½“è·¯ç”±ç®—æ³•æ–‡ä»¶ï¼ˆå¦‚ `simple_shuffle.go`ï¼‰åœ¨ç”Ÿäº§è·¯å¾„ä¸­**ä»Žæœªè¢«å¼•ç”¨**ã€‚
+    *   **ç»“è®ºï¼š** `internal/router` æ˜¯**åƒµå°¸ä»£ç **åº“ï¼Œä¸ä»…å¢žåŠ äº†ç»´æŠ¤è´Ÿæ‹…ï¼Œè¿˜å¯èƒ½å¯¼è‡´æµ‹è¯•çŽ¯å¢ƒï¼ˆå¦‚æžœä½¿ç”¨äº†å®ƒï¼‰ä¸Žç”Ÿäº§çŽ¯å¢ƒè¡Œä¸ºä¸ä¸€è‡´ã€‚
+
+## 2. æ•°æ®æµå®Œæ•´æ€§åˆ†æž (Data Flow Integrity)
+
+### [æ¼æ´ž] å‚æ•°/åŠŸèƒ½ï¼šAnthropic Stream Options (æµå¼é€‰é¡¹)
+*   **è¯æ®ï¼š**
+    *   **è¾“å…¥ï¼š** `pkg/types/request.go` ä¸­ `ChatRequest` å®šä¹‰äº† `StreamOptions` å­—æ®µ (ç¬¬ 24 è¡Œ)ã€‚
+    *   **è½¬æ¢ï¼š** åœ¨ `providers/anthropic/anthropic.go` çš„ `transformRequest` å‡½æ•° (ç¬¬ 208 è¡Œ) ä¸­ã€‚
+    *   **ä¸¢å¤±ï¼š** `anthropicRequest` ç»“æž„ä½“ (ç¬¬ 104 è¡Œ) **æ²¡æœ‰å®šä¹‰** `stream_options` å­—æ®µï¼Œä¸”è½¬æ¢é€»è¾‘å®Œå…¨å¿½ç•¥äº† `req.StreamOptions`ã€‚
+    *   **åŽæžœï¼š** ç”¨æˆ·è¯·æ±‚ä¸­çš„ `include_usage: true` ç­‰æµå¼é€‰é¡¹åœ¨ä¼ é€’ç»™ Claude æ¨¡åž‹æ—¶ä¼šè¢«**é™é»˜ä¸¢å¼ƒ**ï¼Œå¯¼è‡´å®¢æˆ·ç«¯æ— æ³•èŽ·å– Token ä½¿ç”¨é‡ã€‚
+
+### [éšæ‚£] å‚æ•°/åŠŸèƒ½ï¼šå¤šæ¨¡æ€æ”¯æŒ (Multimodal Support)
+*   **è¯æ®ï¼š**
+    *   `pkg/types/request.go` ä¸­ `ChatMessage.Content` è¢«å®šä¹‰ä¸º `json.RawMessage`ã€‚
+    *   è™½ç„¶è¿™å…è®¸é€ä¼ ä»»æ„ JSONï¼ˆåŒ…æ‹¬å›¾ç‰‡æ•°ç»„ï¼‰ï¼Œä½†ä»£ç åº“ä¸­**ç¼ºä¹æ˜¾å¼çš„ Image/Audio ç±»åž‹å®šä¹‰å’Œæ ¡éªŒé€»è¾‘**ã€‚
+    *   è¿™æ„å‘³ç€ `llmux` å¯¹å¤šæ¨¡æ€çš„æ”¯æŒæ˜¯â€œç›²ç›®â€çš„â€”â€”å®ƒä¸ç†è§£ä¹Ÿä¸éªŒè¯å¤šæ¨¡æ€æ•°æ®ï¼Œåªæ˜¯ä½œä¸ºå­—èŠ‚æµé€ä¼ ã€‚è¿™åœ¨éœ€è¦å¯¹å›¾ç‰‡è¿›è¡Œè®¡è´¹æˆ–å¤„ç†ï¼ˆå¦‚åŽ‹ç¼©ã€å®¡æ ¸ï¼‰æ—¶å°†æˆä¸ºé‡å¤§é˜»ç¢ã€‚
+
+## 3. å…¨é“¾è·¯é€»è¾‘éªŒè¯æ€»ç»“
+
+*   **å·²æ‰“é€šçš„é“¾è·¯ (Solid Links)ï¼š**
+    *   **æ ¸å¿ƒå¯¹è¯æµç¨‹ (Chat Completion)ï¼š** `main` -> `handler` -> `client` -> `router` -> `openai provider` çš„é“¾è·¯æ˜¯ç•…é€šçš„ã€‚
+    *   **æµå¼å“åº” (Streaming)ï¼š** `stream.go` å®žçŽ°äº†å¥å£®çš„ `StreamReader`ï¼ŒåŒ…å« `tryRecover` é‡è¯•æœºåˆ¶ï¼Œèƒ½æ­£ç¡®å¤„ç†è¿žæŽ¥ä¸­æ–­å’Œ `[DONE]` ä¿¡å·ã€‚
+    *   **ä¸Šæ¸¸é™æµ (Upstream Rate Limit)ï¼š** `client.go` ä¸­çš„ `checkRateLimit` (ç¬¬ 679 è¡Œ) è¢«æ­£ç¡®è°ƒç”¨ï¼Œä¸”ä¸Ž `resilience` åŒ…æ‰“é€šã€‚
+
+*   **æœªæ‰“é€š/è™šå‡çš„é“¾è·¯ (Broken/Hollow Links)ï¼š**
+    *   **ç½‘å…³é˜²å¾¡ (Downstream Rate Limit)ï¼š** å®Œå…¨æ–­å¼€ï¼Œä»£ç å­˜åœ¨ä½†æœªæŽ¥å…¥ã€‚
+    *   **éž OpenAI æä¾›å•†çš„å®Œæ•´æ€§ï¼š** å¦‚ Anthropic çš„ `StreamOptions` ä¸¢å¤±ï¼Œè¡¨æ˜Žéž OpenAI æä¾›å•†çš„é€‚é…å±‚å­˜åœ¨æ•°æ®ä¸¢å¤±é£Žé™©ã€‚
+    *   **è·¯ç”±ä¸€è‡´æ€§ï¼š** ç”Ÿäº§ä»£ç ä¸Žæ—§çš„ `internal/router` å®žçŽ°å…±å­˜ï¼Œå­˜åœ¨æ··æ·†é£Žé™©ã€‚
+
+## 4. æœ€ç»ˆåˆ¤å†³
+
+åŸºäºŽ**æŽ§åˆ¶æµåˆ†æž**ï¼Œ`llmux` çš„å®Œæˆåº¦ç›®å‰å±žäºŽ **â€œç©ºå¿ƒåŒ– (Hollow)â€** çŠ¶æ€ã€‚
+
+è™½ç„¶å…¶æ ¸å¿ƒçš„ OpenAI è½¬å‘è·¯å¾„æ˜¯å®žå¿ƒçš„ï¼Œä½†åœ¨**ä¼ä¸šçº§ç½‘å…³**æ‰€å¿…é¡»çš„â€œé˜²å¾¡å±‚ï¼ˆé™æµï¼‰â€å’Œâ€œå¤šæ¨¡æ€é€‚é…å±‚â€ä¸Šå­˜åœ¨æ˜Žæ˜¾çš„**æ–­é“¾**å’Œ**æ•°æ®ä¸¢å¤±**ã€‚å®ƒæ›´åƒæ˜¯ä¸€ä¸ªâ€œå…·å¤‡é‡è¯•åŠŸèƒ½çš„ HTTP ä»£ç†â€ï¼Œè€Œéžä¸€ä¸ªä¸¥è°¨çš„â€œAI ç½‘å…³â€ã€‚
+
+**å»ºè®®ç«‹å³è¡ŒåŠ¨ï¼š**
+1.  **Wire Up Rate Limiter:** åœ¨ `main.go` ä¸­æ˜¾å¼è°ƒç”¨ `TenantRateLimiter.RateLimitMiddleware`ã€‚
+2.  **Purge Zombie Code:** åˆ é™¤ `internal/router` ä¸­é™¤ `StatsStore` ä»¥å¤–çš„æ‰€æœ‰ä»£ç ï¼Œæˆ–å°†å…¶åˆå¹¶åˆ° `routers` åŒ…ã€‚
+3.  **Fix Data Mutation:** ä¿®å¤ `providers/anthropic` (åŠå…¶ä»–æä¾›å•†) çš„å‚æ•°æ˜ å°„ï¼Œç¡®ä¿ `StreamOptions` ç­‰å­—æ®µæ­£ç¡®é€ä¼ ã€‚
diff --git a/cmd/server/main.go b/cmd/server/main.go
index c083232..15a800a 100644
--- a/cmd/server/main.go
+++ b/cmd/server/main.go
@@ -25,10 +25,10 @@ import (
 	"github.com/blueberrycongee/llmux/internal/metrics"
 	"github.com/blueberrycongee/llmux/internal/observability"
 	"github.com/blueberrycongee/llmux/internal/resilience"
-	"github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/internal/secret"
 	"github.com/blueberrycongee/llmux/internal/secret/env"
 	"github.com/blueberrycongee/llmux/internal/secret/vault"
+	"github.com/blueberrycongee/llmux/routers"
 )
 
 //go:embed all:ui_assets
@@ -415,7 +415,7 @@ func buildClientOptions(cfg *config.Config, logger *slog.Logger, secretManager *
 			if err := redisClient.Ping(ctx).Err(); err != nil {
 				logger.Error("failed to connect to Redis for distributed routing", "error", err)
 			} else {
-				statsStore := router.NewRedisStatsStore(redisClient)
+				statsStore := routers.NewRedisStatsStore(redisClient)
 				opts = append(opts, llmux.WithStatsStore(statsStore))
 				logger.Info("distributed routing enabled", "redis_addr", cfg.Cache.Redis.Addr)
 			}
diff --git a/internal/api/handler.go b/internal/api/handler.go
index c5d2245..414220a 100644
--- a/internal/api/handler.go
+++ b/internal/api/handler.go
@@ -14,10 +14,10 @@ import (
 	"github.com/blueberrycongee/llmux/internal/metrics"
 	"github.com/blueberrycongee/llmux/internal/pool"
 	"github.com/blueberrycongee/llmux/internal/provider"
-	llmrouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/internal/streaming"
 	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
 	pkgprovider "github.com/blueberrycongee/llmux/pkg/provider"
+	"github.com/blueberrycongee/llmux/pkg/router"
 )
 
 const (
@@ -34,14 +34,14 @@ type HandlerConfig struct {
 // Handler handles HTTP requests for the LLM gateway.
 type Handler struct {
 	registry    *provider.Registry
-	llmRouter   llmrouter.Router
+	llmRouter   router.Router
 	logger      *slog.Logger
 	httpClient  *http.Client
 	maxBodySize int64
 }
 
 // NewHandler creates a new API handler with a shared HTTP client.
-func NewHandler(registry *provider.Registry, r llmrouter.Router, logger *slog.Logger, cfg *HandlerConfig) *Handler {
+func NewHandler(registry *provider.Registry, r router.Router, logger *slog.Logger, cfg *HandlerConfig) *Handler {
 	maxBodySize := int64(DefaultMaxBodySize)
 	if cfg != nil && cfg.MaxBodySize > 0 {
 		maxBodySize = cfg.MaxBodySize
@@ -175,7 +175,7 @@ func (h *Handler) ChatCompletions(w http.ResponseWriter, r *http.Request) {
 	defer pool.PutChatResponse(chatResp)
 
 	// Record success metrics
-	h.llmRouter.ReportSuccess(deployment, &llmrouter.ResponseMetrics{Latency: latency})
+	h.llmRouter.ReportSuccess(deployment, &router.ResponseMetrics{Latency: latency})
 	metrics.RecordRequest(prov.Name(), req.Model, http.StatusOK, latency)
 	if chatResp.Usage != nil {
 		metrics.RecordTokens(prov.Name(), req.Model, chatResp.Usage.PromptTokens, chatResp.Usage.CompletionTokens)
@@ -186,7 +186,7 @@ func (h *Handler) ChatCompletions(w http.ResponseWriter, r *http.Request) {
 	_ = json.NewEncoder(w).Encode(chatResp)
 }
 
-func (h *Handler) handleStreamResponse(w http.ResponseWriter, r *http.Request, resp *http.Response, prov provider.Provider, deployment *provider.Deployment, model string, start time.Time) {
+func (h *Handler) handleStreamResponse(w http.ResponseWriter, r *http.Request, resp *http.Response, prov provider.Provider, deployment *pkgprovider.Deployment, model string, start time.Time) {
 	// Get provider-specific parser for chunk transformation
 	parser := streaming.GetParser(prov.Name())
 
@@ -215,7 +215,7 @@ func (h *Handler) handleStreamResponse(w http.ResponseWriter, r *http.Request, r
 
 	// Record metrics
 	latency := time.Since(start)
-	h.llmRouter.ReportSuccess(deployment, &llmrouter.ResponseMetrics{Latency: latency})
+	h.llmRouter.ReportSuccess(deployment, &router.ResponseMetrics{Latency: latency})
 	metrics.RecordRequest(prov.Name(), model, http.StatusOK, latency)
 }
 
diff --git a/internal/router/base.go b/internal/router/base.go
deleted file mode 100644
index eaad159..0000000
--- a/internal/router/base.go
+++ /dev/null
@@ -1,416 +0,0 @@
-package router
-
-import (
-	"context"
-	"errors"
-	"math/rand"
-	"sync"
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
-)
-
-// ErrNoAvailableDeployment is returned when no healthy deployment is available.
-var ErrNoAvailableDeployment = errors.New("no available deployment for model")
-
-// ErrNoDeploymentsWithTag is returned when no deployments match the requested tags.
-var ErrNoDeploymentsWithTag = errors.New("no deployments match the requested tags")
-
-// BaseRouter provides common functionality for all routing strategies.
-// Specific strategies embed this and override the selection logic.
-type BaseRouter struct {
-	mu          sync.RWMutex
-	rngMu       sync.Mutex                       // Separate mutex for rng (math/rand.Rand is not thread-safe)
-	deployments map[string][]*ExtendedDeployment // model -> deployments
-	stats       map[string]*DeploymentStats      // deploymentID -> stats
-	config      RouterConfig
-	rng         *rand.Rand
-	strategy    Strategy
-}
-
-// NewBaseRouter creates a new base router with the given configuration.
-func NewBaseRouter(config RouterConfig) *BaseRouter {
-	return &BaseRouter{
-		deployments: make(map[string][]*ExtendedDeployment),
-		stats:       make(map[string]*DeploymentStats),
-		config:      config,
-		rng:         rand.New(rand.NewSource(time.Now().UnixNano())),
-		strategy:    config.Strategy,
-	}
-}
-
-// GetStrategy returns the current routing strategy.
-func (r *BaseRouter) GetStrategy() Strategy {
-	return r.strategy
-}
-
-// randIntn returns a random int in [0, n) in a thread-safe manner.
-func (r *BaseRouter) randIntn(n int) int {
-	r.rngMu.Lock()
-	defer r.rngMu.Unlock()
-	return r.rng.Intn(n)
-}
-
-// randFloat64 returns a random float64 in [0.0, 1.0) in a thread-safe manner.
-func (r *BaseRouter) randFloat64() float64 {
-	r.rngMu.Lock()
-	defer r.rngMu.Unlock()
-	return r.rng.Float64()
-}
-
-// randShuffle shuffles a slice in a thread-safe manner.
-func (r *BaseRouter) randShuffle(n int, swap func(i, j int)) {
-	r.rngMu.Lock()
-	defer r.rngMu.Unlock()
-	r.rng.Shuffle(n, swap)
-}
-
-// AddDeployment registers a new deployment with default configuration.
-func (r *BaseRouter) AddDeployment(deployment *provider.Deployment) {
-	r.AddDeploymentWithConfig(deployment, DeploymentConfig{})
-}
-
-// AddDeploymentWithConfig registers a deployment with routing configuration.
-func (r *BaseRouter) AddDeploymentWithConfig(deployment *provider.Deployment, config DeploymentConfig) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	model := deployment.ModelName
-	if deployment.ModelAlias != "" {
-		model = deployment.ModelAlias
-	}
-
-	extended := &ExtendedDeployment{
-		Deployment: deployment,
-		Config:     config,
-	}
-
-	r.deployments[model] = append(r.deployments[model], extended)
-	r.stats[deployment.ID] = &DeploymentStats{
-		MaxLatencyListSize: r.config.MaxLatencyListSize,
-		LatencyHistory:     make([]float64, 0, r.config.MaxLatencyListSize),
-		TTFTHistory:        make([]float64, 0, r.config.MaxLatencyListSize),
-	}
-}
-
-// RemoveDeployment removes a deployment from the router.
-func (r *BaseRouter) RemoveDeployment(deploymentID string) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	for model, deps := range r.deployments {
-		for i, d := range deps {
-			if d.ID == deploymentID {
-				r.deployments[model] = append(deps[:i], deps[i+1:]...)
-				break
-			}
-		}
-	}
-	delete(r.stats, deploymentID)
-}
-
-// GetDeployments returns all deployments for a model.
-func (r *BaseRouter) GetDeployments(model string) []*provider.Deployment {
-	r.mu.RLock()
-	defer r.mu.RUnlock()
-
-	deps := r.deployments[model]
-	result := make([]*provider.Deployment, len(deps))
-	for i, d := range deps {
-		result[i] = d.Deployment
-	}
-	return result
-}
-
-// GetStats returns the current stats for a deployment.
-func (r *BaseRouter) GetStats(deploymentID string) *DeploymentStats {
-	r.mu.RLock()
-	defer r.mu.RUnlock()
-
-	if stats, ok := r.stats[deploymentID]; ok {
-		// Return a copy to prevent external modification
-		statsCopy := *stats
-		return &statsCopy
-	}
-	return nil
-}
-
-// IsCircuitOpen checks if the deployment is in cooldown.
-func (r *BaseRouter) IsCircuitOpen(deployment *provider.Deployment) bool {
-	r.mu.RLock()
-	defer r.mu.RUnlock()
-
-	stats, ok := r.stats[deployment.ID]
-	if !ok {
-		return false
-	}
-	return time.Now().Before(stats.CooldownUntil)
-}
-
-// ReportRequestStart increments the active request count.
-func (r *BaseRouter) ReportRequestStart(deployment *provider.Deployment) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	stats := r.getOrCreateStats(deployment.ID)
-	stats.ActiveRequests++
-}
-
-// ReportRequestEnd decrements the active request count.
-func (r *BaseRouter) ReportRequestEnd(deployment *provider.Deployment) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	stats := r.getOrCreateStats(deployment.ID)
-	if stats.ActiveRequests > 0 {
-		stats.ActiveRequests--
-	}
-}
-
-// ReportSuccess records a successful request with metrics.
-func (r *BaseRouter) ReportSuccess(deployment *provider.Deployment, metrics *ResponseMetrics) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	stats := r.getOrCreateStats(deployment.ID)
-	stats.TotalRequests++
-	stats.SuccessCount++
-	stats.LastRequestTime = time.Now()
-
-	// Update latency history
-	latencyMs := float64(metrics.Latency.Milliseconds())
-	r.appendToHistory(&stats.LatencyHistory, latencyMs, stats.MaxLatencyListSize)
-
-	// Update TTFT history for streaming requests
-	if metrics.TimeToFirstToken > 0 {
-		ttftMs := float64(metrics.TimeToFirstToken.Milliseconds())
-		r.appendToHistory(&stats.TTFTHistory, ttftMs, stats.MaxLatencyListSize)
-	}
-
-	// Update average latency (exponential moving average)
-	if stats.AvgLatencyMs == 0 {
-		stats.AvgLatencyMs = latencyMs
-	} else {
-		stats.AvgLatencyMs = stats.AvgLatencyMs*0.9 + latencyMs*0.1
-	}
-
-	// Update TPM/RPM for current minute
-	r.updateUsageStats(stats, metrics.TotalTokens)
-}
-
-// ReportFailure records a failed request and triggers cooldown if needed.
-func (r *BaseRouter) ReportFailure(deployment *provider.Deployment, err error) {
-	r.mu.Lock()
-	defer r.mu.Unlock()
-
-	stats := r.getOrCreateStats(deployment.ID)
-	stats.TotalRequests++
-	stats.FailureCount++
-	stats.LastRequestTime = time.Now()
-
-	// Add penalty latency for timeout errors (helps lowest-latency routing avoid slow deployments)
-	var llmErr *llmerrors.LLMError
-	if errors.As(err, &llmErr) {
-		if llmerrors.IsCooldownRequired(llmErr.StatusCode) {
-			stats.CooldownUntil = time.Now().Add(r.config.CooldownPeriod)
-		}
-		// Add high latency penalty for timeouts
-		if llmErr.StatusCode == 408 || llmErr.StatusCode == 504 {
-			r.appendToHistory(&stats.LatencyHistory, 1000000.0, stats.MaxLatencyListSize) // 1000s penalty
-		}
-	}
-}
-
-// getHealthyDeployments returns deployments that are not in cooldown.
-func (r *BaseRouter) getHealthyDeployments(model string) []*ExtendedDeployment {
-	deps, ok := r.deployments[model]
-	if !ok || len(deps) == 0 {
-		return nil
-	}
-
-	now := time.Now()
-	healthy := make([]*ExtendedDeployment, 0, len(deps))
-	for _, d := range deps {
-		stats := r.stats[d.ID]
-		if stats == nil || now.After(stats.CooldownUntil) {
-			healthy = append(healthy, d)
-		}
-	}
-	return healthy
-}
-
-// filterByTags filters deployments based on request tags.
-func (r *BaseRouter) filterByTags(deployments []*ExtendedDeployment, tags []string) []*ExtendedDeployment {
-	if len(tags) == 0 {
-		// For untagged requests, prefer deployments with "default" tag
-		defaults := make([]*ExtendedDeployment, 0)
-		for _, d := range deployments {
-			if containsTag(d.Config.Tags, "default") {
-				defaults = append(defaults, d)
-			}
-		}
-		if len(defaults) > 0 {
-			return defaults
-		}
-		return deployments
-	}
-
-	matched := make([]*ExtendedDeployment, 0)
-	defaults := make([]*ExtendedDeployment, 0)
-
-	for _, d := range deployments {
-		if len(d.Config.Tags) == 0 {
-			continue
-		}
-		if hasMatchingTag(d.Config.Tags, tags) {
-			matched = append(matched, d)
-		}
-		if containsTag(d.Config.Tags, "default") {
-			defaults = append(defaults, d)
-		}
-	}
-
-	if len(matched) > 0 {
-		return matched
-	}
-	if len(defaults) > 0 {
-		return defaults
-	}
-	return nil
-}
-
-// filterByTPMRPM filters out deployments that would exceed their TPM/RPM limits.
-func (r *BaseRouter) filterByTPMRPM(deployments []*ExtendedDeployment, inputTokens int) []*ExtendedDeployment {
-	filtered := make([]*ExtendedDeployment, 0, len(deployments))
-
-	for _, d := range deployments {
-		stats := r.stats[d.ID]
-		if stats == nil {
-			filtered = append(filtered, d)
-			continue
-		}
-
-		// Check TPM limit
-		if d.Config.TPMLimit > 0 && stats.CurrentMinuteTPM+int64(inputTokens) > d.Config.TPMLimit {
-			continue
-		}
-
-		// Check RPM limit
-		if d.Config.RPMLimit > 0 && stats.CurrentMinuteRPM+1 > d.Config.RPMLimit {
-			continue
-		}
-
-		filtered = append(filtered, d)
-	}
-
-	return filtered
-}
-
-// getOrCreateStats returns existing stats or creates new ones.
-func (r *BaseRouter) getOrCreateStats(deploymentID string) *DeploymentStats {
-	stats, ok := r.stats[deploymentID]
-	if !ok {
-		stats = &DeploymentStats{
-			MaxLatencyListSize: r.config.MaxLatencyListSize,
-			LatencyHistory:     make([]float64, 0, r.config.MaxLatencyListSize),
-			TTFTHistory:        make([]float64, 0, r.config.MaxLatencyListSize),
-		}
-		r.stats[deploymentID] = stats
-	}
-	return stats
-}
-
-// appendToHistory adds a value to a rolling history slice.
-func (r *BaseRouter) appendToHistory(history *[]float64, value float64, maxSize int) {
-	if maxSize <= 0 {
-		maxSize = 10 // Default size
-	}
-	if len(*history) < maxSize {
-		*history = append(*history, value)
-	} else {
-		// Shift left and append
-		copy((*history)[0:], (*history)[1:])
-		(*history)[len(*history)-1] = value
-	}
-}
-
-// updateUsageStats updates TPM/RPM counters for the current minute.
-func (r *BaseRouter) updateUsageStats(stats *DeploymentStats, tokens int) {
-	currentMinute := time.Now().Format("2006-01-02-15-04")
-
-	if stats.CurrentMinuteKey != currentMinute {
-		// New minute, reset counters
-		stats.CurrentMinuteKey = currentMinute
-		stats.CurrentMinuteTPM = 0
-		stats.CurrentMinuteRPM = 0
-	}
-
-	stats.CurrentMinuteTPM += int64(tokens)
-	stats.CurrentMinuteRPM++
-}
-
-// calculateAverageLatency calculates the average of a latency history slice.
-func calculateAverageLatency(history []float64) float64 {
-	if len(history) == 0 {
-		return 0
-	}
-	var sum float64
-	for _, v := range history {
-		sum += v
-	}
-	return sum / float64(len(history))
-}
-
-// containsTag checks if a tag list contains a specific tag.
-func containsTag(tags []string, tag string) bool {
-	for _, t := range tags {
-		if t == tag {
-			return true
-		}
-	}
-	return false
-}
-
-// hasMatchingTag checks if any deployment tag matches any request tag.
-func hasMatchingTag(deploymentTags, requestTags []string) bool {
-	for _, dt := range deploymentTags {
-		for _, rt := range requestTags {
-			if dt == rt {
-				return true
-			}
-		}
-	}
-	return false
-}
-
-// Pick implements basic random selection (used as fallback).
-func (r *BaseRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext implements basic random selection with context.
-func (r *BaseRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Copy deployment pointer before releasing lock
-	n := len(healthy)
-	r.mu.RUnlock()
-
-	// Random selection (thread-safe)
-	return healthy[r.randIntn(n)].Deployment, nil
-}
diff --git a/internal/router/factory.go b/internal/router/factory.go
deleted file mode 100644
index 91c9312..0000000
--- a/internal/router/factory.go
+++ /dev/null
@@ -1,57 +0,0 @@
-package router
-
-import "fmt"
-
-// New creates a new router based on the specified strategy.
-// Returns an error if the strategy is not recognized.
-func New(config RouterConfig) (Router, error) {
-	switch config.Strategy {
-	case StrategySimpleShuffle, "":
-		return NewSimpleShuffleRouter(config), nil
-	case StrategyLowestLatency:
-		return NewLowestLatencyRouter(config), nil
-	case StrategyLeastBusy:
-		return NewLeastBusyRouter(config), nil
-	case StrategyLowestTPMRPM:
-		return NewLowestTPMRPMRouter(config), nil
-	case StrategyLowestCost:
-		return NewLowestCostRouter(config), nil
-	case StrategyTagBased:
-		return NewTagBasedRouter(config), nil
-	default:
-		return nil, fmt.Errorf("unknown routing strategy: %s", config.Strategy)
-	}
-}
-
-// MustNew creates a new router and panics if the strategy is invalid.
-// Use this only when you're certain the strategy is valid.
-func MustNew(config RouterConfig) Router {
-	r, err := New(config)
-	if err != nil {
-		panic(err)
-	}
-	return r
-}
-
-// AvailableStrategies returns a list of all available routing strategies.
-func AvailableStrategies() []Strategy {
-	return []Strategy{
-		StrategySimpleShuffle,
-		StrategyLowestLatency,
-		StrategyLeastBusy,
-		StrategyLowestTPMRPM,
-		StrategyLowestCost,
-		StrategyTagBased,
-	}
-}
-
-// IsValidStrategy checks if a strategy string is valid.
-func IsValidStrategy(s string) bool {
-	strategy := Strategy(s)
-	for _, valid := range AvailableStrategies() {
-		if strategy == valid {
-			return true
-		}
-	}
-	return false
-}
diff --git a/internal/router/interface.go b/internal/router/interface.go
deleted file mode 100644
index ea6bdfa..0000000
--- a/internal/router/interface.go
+++ /dev/null
@@ -1,55 +0,0 @@
-// Package router provides request routing and load balancing for LLM deployments.
-// It supports multiple strategies including simple shuffle, lowest latency, least busy,
-// lowest TPM/RPM, lowest cost, and tag-based routing.
-package router
-
-import (
-	"context"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// Router selects the best deployment for a given request.
-// It tracks deployment health and performance metrics for intelligent routing.
-type Router interface {
-	// Pick selects the best available deployment for the given model.
-	// Returns ErrNoAvailableDeployment if all deployments are unavailable.
-	Pick(ctx context.Context, model string) (*provider.Deployment, error)
-
-	// PickWithContext selects the best deployment using request context.
-	// This enables advanced routing strategies like tag-based and streaming-aware routing.
-	PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error)
-
-	// ReportSuccess records a successful request to update routing metrics.
-	ReportSuccess(deployment *provider.Deployment, metrics *ResponseMetrics)
-
-	// ReportFailure records a failed request and potentially triggers cooldown.
-	ReportFailure(deployment *provider.Deployment, err error)
-
-	// ReportRequestStart records when a request starts (for least-busy tracking).
-	ReportRequestStart(deployment *provider.Deployment)
-
-	// ReportRequestEnd records when a request ends (for least-busy tracking).
-	ReportRequestEnd(deployment *provider.Deployment)
-
-	// IsCircuitOpen checks if the circuit breaker is open for a deployment.
-	IsCircuitOpen(deployment *provider.Deployment) bool
-
-	// AddDeployment registers a new deployment with the router.
-	AddDeployment(deployment *provider.Deployment)
-
-	// AddDeploymentWithConfig registers a deployment with routing configuration.
-	AddDeploymentWithConfig(deployment *provider.Deployment, config DeploymentConfig)
-
-	// RemoveDeployment removes a deployment from the router.
-	RemoveDeployment(deploymentID string)
-
-	// GetDeployments returns all deployments for a model.
-	GetDeployments(model string) []*provider.Deployment
-
-	// GetStats returns the current stats for a deployment.
-	GetStats(deploymentID string) *DeploymentStats
-
-	// GetStrategy returns the current routing strategy.
-	GetStrategy() Strategy
-}
diff --git a/internal/router/least_busy.go b/internal/router/least_busy.go
deleted file mode 100644
index 00bb68f..0000000
--- a/internal/router/least_busy.go
+++ /dev/null
@@ -1,96 +0,0 @@
-package router
-
-import (
-	"context"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// LeastBusyRouter selects the deployment with the fewest active requests.
-// This strategy helps distribute load evenly across deployments.
-//
-// Usage:
-//   - Call ReportRequestStart() when a request begins
-//   - Call ReportRequestEnd() when a request completes (success or failure)
-type LeastBusyRouter struct {
-	*BaseRouter
-}
-
-// NewLeastBusyRouter creates a new least busy router.
-func NewLeastBusyRouter(config RouterConfig) *LeastBusyRouter {
-	config.Strategy = StrategyLeastBusy
-	return &LeastBusyRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects the deployment with fewest active requests.
-func (r *LeastBusyRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext selects the deployment with fewest active requests.
-func (r *LeastBusyRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Apply TPM/RPM filtering
-	if reqCtx.EstimatedInputTokens > 0 {
-		healthy = r.filterByTPMRPM(healthy, reqCtx.EstimatedInputTokens)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoAvailableDeployment
-		}
-	}
-
-	// Copy data needed for selection
-	type deploymentInfo struct {
-		deployment     *ExtendedDeployment
-		activeRequests int64
-	}
-	candidates := make([]deploymentInfo, len(healthy))
-	for i, d := range healthy {
-		var activeRequests int64
-		if stats := r.stats[d.ID]; stats != nil {
-			activeRequests = stats.ActiveRequests
-		}
-		candidates[i] = deploymentInfo{deployment: d, activeRequests: activeRequests}
-	}
-	r.mu.RUnlock()
-
-	// Shuffle first to randomize selection among equal candidates (thread-safe)
-	r.randShuffle(len(candidates), func(i, j int) {
-		candidates[i], candidates[j] = candidates[j], candidates[i]
-	})
-
-	// Find deployment with minimum active requests
-	var minDeployment *ExtendedDeployment
-	minRequests := int64(-1)
-
-	for _, c := range candidates {
-		if minRequests < 0 || c.activeRequests < minRequests {
-			minRequests = c.activeRequests
-			minDeployment = c.deployment
-		}
-	}
-
-	if minDeployment == nil {
-		// Fallback to random selection (shouldn't happen, thread-safe)
-		return candidates[r.randIntn(len(candidates))].deployment.Deployment, nil
-	}
-
-	return minDeployment.Deployment, nil
-}
diff --git a/internal/router/lowest_cost.go b/internal/router/lowest_cost.go
deleted file mode 100644
index 51aeb0d..0000000
--- a/internal/router/lowest_cost.go
+++ /dev/null
@@ -1,104 +0,0 @@
-package router
-
-import (
-	"context"
-	"sort"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// DefaultCostPerToken is used when no cost is configured for a deployment.
-// Set high to deprioritize deployments without cost configuration.
-const DefaultCostPerToken = 5.0
-
-// LowestCostRouter selects the deployment with lowest cost per token.
-// Cost is calculated as: input_cost_per_token + output_cost_per_token
-//
-// This strategy is useful for cost optimization when you have multiple
-// deployments with different pricing (e.g., different regions, providers).
-type LowestCostRouter struct {
-	*BaseRouter
-}
-
-// NewLowestCostRouter creates a new lowest cost router.
-func NewLowestCostRouter(config RouterConfig) *LowestCostRouter {
-	config.Strategy = StrategyLowestCost
-	return &LowestCostRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects the deployment with lowest cost.
-func (r *LowestCostRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext selects the deployment with lowest cost per token.
-func (r *LowestCostRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Apply TPM/RPM filtering
-	if reqCtx.EstimatedInputTokens > 0 {
-		healthy = r.filterByTPMRPM(healthy, reqCtx.EstimatedInputTokens)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoAvailableDeployment
-		}
-	}
-
-	// Calculate cost for each deployment
-	type deploymentCost struct {
-		deployment *ExtendedDeployment
-		cost       float64
-	}
-
-	candidates := make([]deploymentCost, 0, len(healthy))
-
-	for _, d := range healthy {
-		inputCost := d.Config.InputCostPerToken
-		outputCost := d.Config.OutputCostPerToken
-
-		// Use default cost if not configured
-		if inputCost == 0 {
-			inputCost = DefaultCostPerToken
-		}
-		if outputCost == 0 {
-			outputCost = DefaultCostPerToken
-		}
-
-		totalCost := inputCost + outputCost
-
-		candidates = append(candidates, deploymentCost{
-			deployment: d,
-			cost:       totalCost,
-		})
-	}
-	r.mu.RUnlock()
-
-	// Shuffle first to randomize order for equal costs (thread-safe)
-	r.randShuffle(len(candidates), func(i, j int) {
-		candidates[i], candidates[j] = candidates[j], candidates[i]
-	})
-
-	// Sort by cost (stable sort preserves random order for equal values)
-	sort.SliceStable(candidates, func(i, j int) bool {
-		return candidates[i].cost < candidates[j].cost
-	})
-
-	// Return the lowest cost deployment
-	return candidates[0].deployment.Deployment, nil
-}
diff --git a/internal/router/lowest_latency.go b/internal/router/lowest_latency.go
deleted file mode 100644
index 2e5f206..0000000
--- a/internal/router/lowest_latency.go
+++ /dev/null
@@ -1,124 +0,0 @@
-package router
-
-import (
-	"context"
-	"sort"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// LowestLatencyRouter selects deployments based on response latency.
-// For streaming requests, it uses Time To First Token (TTFT) instead of total latency.
-// A configurable buffer allows random selection among deployments within X% of the lowest latency.
-type LowestLatencyRouter struct {
-	*BaseRouter
-}
-
-// NewLowestLatencyRouter creates a new lowest latency router.
-func NewLowestLatencyRouter(config RouterConfig) *LowestLatencyRouter {
-	config.Strategy = StrategyLowestLatency
-	return &LowestLatencyRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects the deployment with lowest latency.
-func (r *LowestLatencyRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext selects the deployment with lowest latency, considering streaming mode.
-func (r *LowestLatencyRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Apply TPM/RPM filtering
-	if reqCtx.EstimatedInputTokens > 0 {
-		healthy = r.filterByTPMRPM(healthy, reqCtx.EstimatedInputTokens)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoAvailableDeployment
-		}
-	}
-
-	// Calculate latency for each deployment
-	type deploymentLatency struct {
-		deployment *ExtendedDeployment
-		latency    float64
-	}
-
-	candidates := make([]deploymentLatency, 0, len(healthy))
-
-	for _, d := range healthy {
-		stats := r.stats[d.ID]
-		var latency float64
-
-		switch {
-		case stats == nil:
-			// No stats yet, use 0 latency (prioritize untested deployments)
-			latency = 0
-		case reqCtx.IsStreaming && len(stats.TTFTHistory) > 0:
-			// Use TTFT for streaming requests
-			latency = calculateAverageLatency(stats.TTFTHistory)
-		case len(stats.LatencyHistory) > 0:
-			// Use regular latency
-			latency = calculateAverageLatency(stats.LatencyHistory)
-		default:
-			latency = 0
-		}
-
-		candidates = append(candidates, deploymentLatency{
-			deployment: d,
-			latency:    latency,
-		})
-	}
-
-	latencyBuffer := r.config.LatencyBuffer
-	r.mu.RUnlock()
-
-	// Shuffle first to randomize order for equal latencies (thread-safe)
-	r.randShuffle(len(candidates), func(i, j int) {
-		candidates[i], candidates[j] = candidates[j], candidates[i]
-	})
-
-	// Sort by latency (stable sort preserves random order for equal values)
-	sort.SliceStable(candidates, func(i, j int) bool {
-		return candidates[i].latency < candidates[j].latency
-	})
-
-	// Find lowest latency
-	lowestLatency := candidates[0].latency
-
-	// If lowest latency is 0, just pick randomly from all candidates (thread-safe)
-	if lowestLatency == 0 {
-		return candidates[r.randIntn(len(candidates))].deployment.Deployment, nil
-	}
-
-	// Find all deployments within the buffer threshold
-	buffer := latencyBuffer * lowestLatency
-	threshold := lowestLatency + buffer
-
-	validCandidates := make([]deploymentLatency, 0)
-	for _, c := range candidates {
-		if c.latency <= threshold {
-			validCandidates = append(validCandidates, c)
-		}
-	}
-
-	// Random selection from valid candidates (thread-safe)
-	selected := validCandidates[r.randIntn(len(validCandidates))]
-	return selected.deployment.Deployment, nil
-}
diff --git a/internal/router/lowest_tpm_rpm.go b/internal/router/lowest_tpm_rpm.go
deleted file mode 100644
index 5b871c1..0000000
--- a/internal/router/lowest_tpm_rpm.go
+++ /dev/null
@@ -1,105 +0,0 @@
-package router
-
-import (
-	"context"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// LowestTPMRPMRouter selects the deployment with lowest TPM/RPM usage.
-// This strategy helps stay within rate limits by distributing requests
-// to deployments with the most available capacity.
-//
-// TPM (Tokens Per Minute) and RPM (Requests Per Minute) are tracked per deployment
-// and reset at the start of each minute.
-type LowestTPMRPMRouter struct {
-	*BaseRouter
-}
-
-// NewLowestTPMRPMRouter creates a new lowest TPM/RPM router.
-func NewLowestTPMRPMRouter(config RouterConfig) *LowestTPMRPMRouter {
-	config.Strategy = StrategyLowestTPMRPM
-	return &LowestTPMRPMRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects the deployment with lowest TPM usage.
-func (r *LowestTPMRPMRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext selects the deployment with lowest TPM/RPM usage.
-func (r *LowestTPMRPMRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Copy data needed for selection
-	type deploymentInfo struct {
-		deployment *ExtendedDeployment
-		currentTPM int64
-		currentRPM int64
-	}
-	candidates := make([]deploymentInfo, len(healthy))
-	for i, d := range healthy {
-		var currentTPM, currentRPM int64
-		if stats := r.stats[d.ID]; stats != nil {
-			currentTPM = stats.CurrentMinuteTPM
-			currentRPM = stats.CurrentMinuteRPM
-		}
-		candidates[i] = deploymentInfo{deployment: d, currentTPM: currentTPM, currentRPM: currentRPM}
-	}
-	r.mu.RUnlock()
-
-	// Shuffle first to randomize selection among equal candidates (thread-safe)
-	r.randShuffle(len(candidates), func(i, j int) {
-		candidates[i], candidates[j] = candidates[j], candidates[i]
-	})
-
-	// Filter by TPM/RPM limits and find lowest usage
-	var bestDeployment *ExtendedDeployment
-	lowestTPM := int64(-1)
-
-	for _, c := range candidates {
-		// Check if adding this request would exceed limits
-		estimatedTokens := int64(reqCtx.EstimatedInputTokens)
-		if estimatedTokens == 0 {
-			estimatedTokens = 100 // Default estimate
-		}
-
-		// Skip if would exceed TPM limit
-		if c.deployment.Config.TPMLimit > 0 && c.currentTPM+estimatedTokens > c.deployment.Config.TPMLimit {
-			continue
-		}
-
-		// Skip if would exceed RPM limit
-		if c.deployment.Config.RPMLimit > 0 && c.currentRPM+1 >= c.deployment.Config.RPMLimit {
-			continue
-		}
-
-		// Select deployment with lowest TPM
-		if lowestTPM < 0 || c.currentTPM < lowestTPM {
-			lowestTPM = c.currentTPM
-			bestDeployment = c.deployment
-		}
-	}
-
-	if bestDeployment == nil {
-		return nil, ErrNoAvailableDeployment
-	}
-
-	return bestDeployment.Deployment, nil
-}
diff --git a/internal/router/memory_stats_store.go b/internal/router/memory_stats_store.go
deleted file mode 100644
index 61ad2b7..0000000
--- a/internal/router/memory_stats_store.go
+++ /dev/null
@@ -1,254 +0,0 @@
-package router
-
-import (
-	"context"
-	"sync"
-	"time"
-
-	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
-)
-
-// MemoryStatsStore is an in-memory implementation of StatsStore.
-// It stores all statistics in local memory using maps protected by RWMutex.
-//
-// Characteristics:
-//   - Fast: No network calls, nanosecond latency
-//   - Local-only: Stats are not shared across multiple instances
-//   - No persistence: Stats are lost on restart
-//
-// Use Cases:
-//   - Single-instance deployments
-//   - Development and testing
-//   - Fallback when Redis is unavailable
-type MemoryStatsStore struct {
-	mu    sync.RWMutex
-	stats map[string]*DeploymentStats
-
-	maxLatencyListSize int
-}
-
-// NewMemoryStatsStore creates a new in-memory stats store.
-func NewMemoryStatsStore() *MemoryStatsStore {
-	return &MemoryStatsStore{
-		stats:              make(map[string]*DeploymentStats),
-		maxLatencyListSize: 10, // Default: keep last 10 latency samples
-	}
-}
-
-// NewMemoryStatsStoreWithConfig creates a new in-memory stats store with custom config.
-func NewMemoryStatsStoreWithConfig(maxLatencyListSize int) *MemoryStatsStore {
-	return &MemoryStatsStore{
-		stats:              make(map[string]*DeploymentStats),
-		maxLatencyListSize: maxLatencyListSize,
-	}
-}
-
-// GetStats retrieves statistics for a deployment.
-func (m *MemoryStatsStore) GetStats(ctx context.Context, deploymentID string) (*DeploymentStats, error) {
-	m.mu.RLock()
-	defer m.mu.RUnlock()
-
-	stats, ok := m.stats[deploymentID]
-	if !ok {
-		return nil, ErrStatsNotFound
-	}
-
-	// Return a deep copy to prevent external modification
-	statsCopy := *stats
-	statsCopy.LatencyHistory = append([]float64{}, stats.LatencyHistory...)
-	statsCopy.TTFTHistory = append([]float64{}, stats.TTFTHistory...)
-
-	return &statsCopy, nil
-}
-
-// IncrementActiveRequests atomically increments the active request count.
-func (m *MemoryStatsStore) IncrementActiveRequests(ctx context.Context, deploymentID string) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	stats := m.getOrCreateStatsLocked(deploymentID)
-	stats.ActiveRequests++
-
-	return nil
-}
-
-// DecrementActiveRequests atomically decrements the active request count.
-func (m *MemoryStatsStore) DecrementActiveRequests(ctx context.Context, deploymentID string) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	stats := m.getOrCreateStatsLocked(deploymentID)
-	if stats.ActiveRequests > 0 {
-		stats.ActiveRequests--
-	}
-
-	return nil
-}
-
-// RecordSuccess records a successful request with its metrics.
-func (m *MemoryStatsStore) RecordSuccess(ctx context.Context, deploymentID string, metrics *ResponseMetrics) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	stats := m.getOrCreateStatsLocked(deploymentID)
-	stats.TotalRequests++
-	stats.SuccessCount++
-	stats.LastRequestTime = time.Now()
-
-	// Update latency history
-	latencyMs := float64(metrics.Latency.Milliseconds())
-	m.appendToHistoryLocked(&stats.LatencyHistory, latencyMs)
-
-	// Update TTFT history for streaming requests
-	if metrics.TimeToFirstToken > 0 {
-		ttftMs := float64(metrics.TimeToFirstToken.Milliseconds())
-		m.appendToHistoryLocked(&stats.TTFTHistory, ttftMs)
-	}
-
-	// Update average latency (exponential moving average)
-	if stats.AvgLatencyMs == 0 {
-		stats.AvgLatencyMs = latencyMs
-	} else {
-		stats.AvgLatencyMs = stats.AvgLatencyMs*0.9 + latencyMs*0.1
-	}
-
-	// Update average TTFT
-	if metrics.TimeToFirstToken > 0 {
-		ttftMs := float64(metrics.TimeToFirstToken.Milliseconds())
-		if stats.AvgTTFTMs == 0 {
-			stats.AvgTTFTMs = ttftMs
-		} else {
-			stats.AvgTTFTMs = stats.AvgTTFTMs*0.9 + ttftMs*0.1
-		}
-	}
-
-	// Update TPM/RPM for current minute
-	m.updateUsageStatsLocked(stats, metrics.TotalTokens)
-
-	return nil
-}
-
-// RecordFailure records a failed request.
-func (m *MemoryStatsStore) RecordFailure(ctx context.Context, deploymentID string, err error) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	stats := m.getOrCreateStatsLocked(deploymentID)
-	stats.TotalRequests++
-	stats.FailureCount++
-	stats.LastRequestTime = time.Now()
-
-	// Check if error is cooldown-worthy (e.g., 500, 503, timeout)
-	if llmErr, ok := err.(*llmerrors.LLMError); ok {
-		// Add penalty latency for timeout errors (helps lowest-latency routing avoid slow deployments)
-		if llmErr.StatusCode == 408 || llmErr.StatusCode == 504 {
-			m.appendToHistoryLocked(&stats.LatencyHistory, 1000000.0) // 1000s penalty
-		}
-	}
-
-	return nil
-}
-
-// SetCooldown manually sets a cooldown period for a deployment.
-func (m *MemoryStatsStore) SetCooldown(ctx context.Context, deploymentID string, until time.Time) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	stats := m.getOrCreateStatsLocked(deploymentID)
-	stats.CooldownUntil = until
-
-	return nil
-}
-
-// GetCooldownUntil returns the cooldown expiration time for a deployment.
-func (m *MemoryStatsStore) GetCooldownUntil(ctx context.Context, deploymentID string) (time.Time, error) {
-	m.mu.RLock()
-	defer m.mu.RUnlock()
-
-	stats, ok := m.stats[deploymentID]
-	if !ok {
-		return time.Time{}, nil
-	}
-
-	return stats.CooldownUntil, nil
-}
-
-// ListDeployments returns all deployment IDs that have stats recorded.
-func (m *MemoryStatsStore) ListDeployments(ctx context.Context) ([]string, error) {
-	m.mu.RLock()
-	defer m.mu.RUnlock()
-
-	deploymentIDs := make([]string, 0, len(m.stats))
-	for id := range m.stats {
-		deploymentIDs = append(deploymentIDs, id)
-	}
-
-	return deploymentIDs, nil
-}
-
-// DeleteStats removes all stats for a deployment.
-func (m *MemoryStatsStore) DeleteStats(ctx context.Context, deploymentID string) error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	delete(m.stats, deploymentID)
-	return nil
-}
-
-// Close releases any resources held by the store.
-func (m *MemoryStatsStore) Close() error {
-	m.mu.Lock()
-	defer m.mu.Unlock()
-
-	// Clear all stats
-	m.stats = make(map[string]*DeploymentStats)
-	return nil
-}
-
-// getOrCreateStatsLocked returns existing stats or creates new ones.
-// MUST be called with m.mu locked.
-func (m *MemoryStatsStore) getOrCreateStatsLocked(deploymentID string) *DeploymentStats {
-	stats, ok := m.stats[deploymentID]
-	if !ok {
-		stats = &DeploymentStats{
-			MaxLatencyListSize: m.maxLatencyListSize,
-			LatencyHistory:     make([]float64, 0, m.maxLatencyListSize),
-			TTFTHistory:        make([]float64, 0, m.maxLatencyListSize),
-		}
-		m.stats[deploymentID] = stats
-	}
-	return stats
-}
-
-// appendToHistoryLocked adds a value to a rolling history slice.
-// MUST be called with m.mu locked.
-func (m *MemoryStatsStore) appendToHistoryLocked(history *[]float64, value float64) {
-	maxSize := m.maxLatencyListSize
-	if maxSize <= 0 {
-		maxSize = 10 // Default size
-	}
-
-	if len(*history) < maxSize {
-		*history = append(*history, value)
-	} else {
-		// Shift left and append
-		copy((*history)[0:], (*history)[1:])
-		(*history)[len(*history)-1] = value
-	}
-}
-
-// updateUsageStatsLocked updates TPM/RPM counters for the current minute.
-// MUST be called with m.mu locked.
-func (m *MemoryStatsStore) updateUsageStatsLocked(stats *DeploymentStats, tokens int) {
-	currentMinute := time.Now().Format("2006-01-02-15-04")
-
-	if stats.CurrentMinuteKey != currentMinute {
-		// New minute, reset counters
-		stats.CurrentMinuteKey = currentMinute
-		stats.CurrentMinuteTPM = 0
-		stats.CurrentMinuteRPM = 0
-	}
-
-	stats.CurrentMinuteTPM += int64(tokens)
-	stats.CurrentMinuteRPM++
-}
diff --git a/internal/router/redis_scripts.go b/internal/router/redis_scripts.go
deleted file mode 100644
index 0338d10..0000000
--- a/internal/router/redis_scripts.go
+++ /dev/null
@@ -1,233 +0,0 @@
-package router
-
-// This file contains Lua scripts for atomic Redis operations on router statistics.
-// The scripts ensure consistency in distributed deployments where multiple LLMux
-// instances share the same Redis backend.
-//
-// Design inspired by the atomic pattern used in internal/resilience/redis_limiter.go
-// for distributed rate limiting.
-
-const (
-	// recordSuccessScript atomically records a successful request with all metrics.
-	//
-	// Keys:
-	//   KEYS[1] - latency list key (e.g., "llmux:router:stats:deployment-1:latency")
-	//   KEYS[2] - ttft list key (e.g., "llmux:router:stats:deployment-1:ttft")
-	//   KEYS[3] - counters hash key (e.g., "llmux:router:stats:deployment-1:counters")
-	//   KEYS[4] - usage hash key (e.g., "llmux:router:stats:deployment-1:usage:2026-01-10-16-52")
-	//
-	// Args:
-	//   ARGV[1] - latency value in milliseconds (float)
-	//   ARGV[2] - ttft value in milliseconds (float, 0 if not streaming)
-	//   ARGV[3] - total tokens (integer)
-	//   ARGV[4] - max latency list size (integer, default 10)
-	//   ARGV[5] - usage TTL in seconds (integer, default 120)
-	//   ARGV[6] - current timestamp (for last_request_time)
-	//
-	// Returns:
-	//   "OK" on success
-	recordSuccessScript = `
-local latency_key = KEYS[1]
-local ttft_key = KEYS[2]
-local counters_key = KEYS[3]
-local usage_key = KEYS[4]
-
-local latency = tonumber(ARGV[1])
-local ttft = tonumber(ARGV[2])
-local tokens = tonumber(ARGV[3])
-local max_size = tonumber(ARGV[4])
-local usage_ttl = tonumber(ARGV[5])
-local now = tonumber(ARGV[6])
-
--- 1. Update latency history (rolling window)
-redis.call('LPUSH', latency_key, latency)
-redis.call('LTRIM', latency_key, 0, max_size - 1)
-redis.call('EXPIRE', latency_key, 3600)  -- 1 hour TTL
-
--- 2. Update TTFT history if provided (for streaming requests)
-if ttft and ttft > 0 then
-    redis.call('LPUSH', ttft_key, ttft)
-    redis.call('LTRIM', ttft_key, 0, max_size - 1)
-    redis.call('EXPIRE', ttft_key, 3600)
-end
-
--- 3. Update counters atomically
-redis.call('HINCRBY', counters_key, 'total_requests', 1)
-redis.call('HINCRBY', counters_key, 'success_count', 1)
-redis.call('HSET', counters_key, 'last_request_time', now)
-redis.call('EXPIRE', counters_key, 3600)
-
--- 4. Update TPM/RPM for current minute
-redis.call('HINCRBY', usage_key, 'tpm', tokens)
-redis.call('HINCRBY', usage_key, 'rpm', 1)
-redis.call('EXPIRE', usage_key, usage_ttl)
-
-return redis.status_reply("OK")
-`
-
-	// recordFailureScript atomically records a failed request.
-	//
-	// Keys:
-	//   KEYS[1] - counters hash key
-	//   KEYS[2] - latency list key (for penalty latency on timeout)
-	//
-	// Args:
-	//   ARGV[1] - current timestamp
-	//   ARGV[2] - is timeout error (1 or 0)
-	//   ARGV[3] - max latency list size
-	//
-	// Returns:
-	//   "OK" on success
-	recordFailureScript = `
-local counters_key = KEYS[1]
-local latency_key = KEYS[2]
-
-local now = tonumber(ARGV[1])
-local is_timeout = tonumber(ARGV[2])
-local max_size = tonumber(ARGV[3])
-
--- 1. Update failure counters
-redis.call('HINCRBY', counters_key, 'total_requests', 1)
-redis.call('HINCRBY', counters_key, 'failure_count', 1)
-redis.call('HSET', counters_key, 'last_request_time', now)
-redis.call('EXPIRE', counters_key, 3600)
-
--- 2. Add penalty latency for timeout errors (helps lowest-latency routing)
-if is_timeout == 1 then
-    redis.call('LPUSH', latency_key, 1000000)  -- 1000s penalty
-    redis.call('LTRIM', latency_key, 0, max_size - 1)
-    redis.call('EXPIRE', latency_key, 3600)
-end
-
-return redis.status_reply("OK")
-`
-
-	// incrementActiveRequestsScript atomically increments active request count.
-	//
-	// Keys:
-	//   KEYS[1] - counters hash key
-	//
-	// Returns:
-	//   The new active_requests count
-	incrementActiveRequestsScript = `
-local counters_key = KEYS[1]
-local result = redis.call('HINCRBY', counters_key, 'active_requests', 1)
-redis.call('EXPIRE', counters_key, 3600)
-return result
-`
-
-	// decrementActiveRequestsScript atomically decrements active request count.
-	//
-	// Keys:
-	//   KEYS[1] - counters hash key
-	//
-	// Returns:
-	//   The new active_requests count (minimum 0)
-	decrementActiveRequestsScript = `
-local counters_key = KEYS[1]
-local current = redis.call('HGET', counters_key, 'active_requests')
-
-if current and tonumber(current) > 0 then
-    local result = redis.call('HINCRBY', counters_key, 'active_requests', -1)
-    redis.call('EXPIRE', counters_key, 3600)
-    return result
-end
-
-return 0
-`
-
-	// getStatsScript retrieves all stats in a single round-trip.
-	//
-	// Keys:
-	//   KEYS[1] - latency list key
-	//   KEYS[2] - ttft list key
-	//   KEYS[3] - counters hash key
-	//   KEYS[4] - usage key prefix (without minute suffix)
-	//
-	// Args:
-	//   ARGV[1] - current minute key (e.g., "2026-01-10-16-52")
-	//
-	// Returns:
-	//   Array of 4 elements:
-	//     [1] - latency list (array of floats)
-	//     [2] - ttft list (array of floats)
-	//     [3] - counters hash (flat array: key1, val1, key2, val2, ...)
-	//     [4] - usage hash for current minute (flat array)
-	getStatsScript = `
-local latency_key = KEYS[1]
-local ttft_key = KEYS[2]
-local counters_key = KEYS[3]
-local usage_prefix = KEYS[4]
-
-local current_minute = ARGV[1]
-local usage_key = usage_prefix .. current_minute
-
-local result = {}
-result[1] = redis.call('LRANGE', latency_key, 0, -1)
-result[2] = redis.call('LRANGE', ttft_key, 0, -1)
-result[3] = redis.call('HGETALL', counters_key)
-result[4] = redis.call('HGETALL', usage_key)
-
-return result
-`
-
-	// setCooldownScript sets a cooldown expiration time.
-	//
-	// Keys:
-	//   KEYS[1] - cooldown key
-	//
-	// Args:
-	//   ARGV[1] - cooldown until timestamp (unix seconds)
-	//   ARGV[2] - TTL in seconds
-	//
-	// Returns:
-	//   "OK"
-	setCooldownScript = `
-local cooldown_key = KEYS[1]
-local until_timestamp = tonumber(ARGV[1])
-local ttl = tonumber(ARGV[2])
-
-redis.call('SET', cooldown_key, until_timestamp)
-redis.call('EXPIRE', cooldown_key, ttl)
-
-return redis.status_reply("OK")
-`
-
-	// deleteStatsScript removes all stats for a deployment.
-	//
-	// Keys:
-	//   KEYS[1] - latency key
-	//   KEYS[2] - ttft key
-	//   KEYS[3] - counters key
-	//   KEYS[4] - cooldown key
-	//   KEYS[5] - usage key pattern (for SCAN)
-	//
-	// Returns:
-	//   Number of keys deleted
-	deleteStatsScript = `
-local latency_key = KEYS[1]
-local ttft_key = KEYS[2]
-local counters_key = KEYS[3]
-local cooldown_key = KEYS[4]
-local usage_pattern = KEYS[5]
-
-local deleted = 0
-
--- Delete fixed keys
-deleted = deleted + redis.call('DEL', latency_key, ttft_key, counters_key, cooldown_key)
-
--- Delete usage keys (pattern match)
-local cursor = "0"
-repeat
-    local result = redis.call('SCAN', cursor, 'MATCH', usage_pattern, 'COUNT', 100)
-    cursor = result[1]
-    local keys = result[2]
-    
-    if #keys > 0 then
-        deleted = deleted + redis.call('DEL', unpack(keys))
-    end
-until cursor == "0"
-
-return deleted
-`
-)
diff --git a/internal/router/redis_stats_store.go b/internal/router/redis_stats_store.go
deleted file mode 100644
index e06637c..0000000
--- a/internal/router/redis_stats_store.go
+++ /dev/null
@@ -1,445 +0,0 @@
-package router
-
-import (
-	"context"
-	"fmt"
-	"strconv"
-	"time"
-
-	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
-	"github.com/redis/go-redis/v9"
-)
-
-// RedisStatsStore implements StatsStore using Redis for distributed statistics.
-// It uses Lua scripts to ensure atomic operations across multiple LLMux instances.
-type RedisStatsStore struct {
-	client             *redis.Client
-	keyPrefix          string
-	maxLatencyListSize int
-	usageTTL           time.Duration
-
-	// Precompiled Lua scripts
-	recordSuccessScript      *redis.Script
-	recordFailureScript      *redis.Script
-	incrementActiveReqScript *redis.Script
-	decrementActiveReqScript *redis.Script
-	getStatsScript           *redis.Script
-	setCooldownScript        *redis.Script
-	deleteStatsScript        *redis.Script
-}
-
-// RedisStatsOption configures RedisStatsStore.
-type RedisStatsOption func(*RedisStatsStore)
-
-// WithKeyPrefix sets the Redis key prefix (default: "llmux:router:stats").
-func WithKeyPrefix(prefix string) RedisStatsOption {
-	return func(r *RedisStatsStore) {
-		r.keyPrefix = prefix
-	}
-}
-
-// WithMaxLatencySamples sets the maximum number of latency samples to keep (default: 10).
-func WithMaxLatencySamples(size int) RedisStatsOption {
-	return func(r *RedisStatsStore) {
-		r.maxLatencyListSize = size
-	}
-}
-
-// WithUsageTTL sets the TTL for per-minute usage stats (default: 120s).
-func WithUsageTTL(ttl time.Duration) RedisStatsOption {
-	return func(r *RedisStatsStore) {
-		r.usageTTL = ttl
-	}
-}
-
-// NewRedisStatsStore creates a new Redis-backed stats store.
-func NewRedisStatsStore(client *redis.Client, opts ...RedisStatsOption) *RedisStatsStore {
-	store := &RedisStatsStore{
-		client:             client,
-		keyPrefix:          "llmux:router:stats",
-		maxLatencyListSize: 10,
-		usageTTL:           120 * time.Second,
-	}
-
-	// Apply options
-	for _, opt := range opts {
-		opt(store)
-	}
-
-	// Precompile Lua scripts
-	store.recordSuccessScript = redis.NewScript(recordSuccessScript)
-	store.recordFailureScript = redis.NewScript(recordFailureScript)
-	store.incrementActiveReqScript = redis.NewScript(incrementActiveRequestsScript)
-	store.decrementActiveReqScript = redis.NewScript(decrementActiveRequestsScript)
-	store.getStatsScript = redis.NewScript(getStatsScript)
-	store.setCooldownScript = redis.NewScript(setCooldownScript)
-	store.deleteStatsScript = redis.NewScript(deleteStatsScript)
-
-	return store
-}
-
-// GetStats retrieves statistics for a deployment.
-func (r *RedisStatsStore) GetStats(ctx context.Context, deploymentID string) (*DeploymentStats, error) {
-	keys := []string{
-		r.latencyKey(deploymentID),
-		r.ttftKey(deploymentID),
-		r.countersKey(deploymentID),
-		r.usageKeyPrefix(deploymentID),
-	}
-
-	currentMinute := time.Now().Format("2006-01-02-15-04")
-	args := []interface{}{currentMinute}
-
-	result, err := r.getStatsScript.Run(ctx, r.client, keys, args...).Result()
-	if err != nil {
-		return nil, err
-	}
-
-	resultSlice, ok := result.([]interface{})
-	if !ok || len(resultSlice) != 4 {
-		return nil, fmt.Errorf("unexpected result format from getStatsScript")
-	}
-
-	stats := &DeploymentStats{
-		MaxLatencyListSize: r.maxLatencyListSize,
-		CurrentMinuteKey:   currentMinute,
-	}
-
-	// Parse latency history (result[0])
-	if latencyList, ok := resultSlice[0].([]interface{}); ok {
-		stats.LatencyHistory = make([]float64, 0, len(latencyList))
-		for _, v := range latencyList {
-			if f, err := parseFloat(v); err == nil {
-				stats.LatencyHistory = append(stats.LatencyHistory, f)
-			}
-		}
-	}
-
-	// Parse TTFT history (result[1])
-	if ttftList, ok := resultSlice[1].([]interface{}); ok {
-		stats.TTFTHistory = make([]float64, 0, len(ttftList))
-		for _, v := range ttftList {
-			if f, err := parseFloat(v); err == nil {
-				stats.TTFTHistory = append(stats.TTFTHistory, f)
-			}
-		}
-	}
-
-	// Track whether the counters hash exists in Redis (has any fields)
-	// This is different from checking if the field VALUES are non-zero.
-	// A hash with [active_requests 0] still EXISTS, it just has zero value.
-	countersHashExists := false
-
-	// Parse counters hash (result[2])
-	// HGETALL returns an array of key-value pairs, or empty array if hash doesn't exist
-	if resultSlice[2] != nil {
-		if countersSlice, ok := resultSlice[2].([]interface{}); ok {
-			// If HGETALL returned any fields, the hash exists
-			countersHashExists = len(countersSlice) > 0
-
-			if countersHashExists {
-				countersMap := parseHashMap(countersSlice)
-
-				stats.TotalRequests = parseInt64(countersMap["total_requests"])
-				stats.SuccessCount = parseInt64(countersMap["success_count"])
-				stats.FailureCount = parseInt64(countersMap["failure_count"])
-				stats.ActiveRequests = parseInt64(countersMap["active_requests"])
-
-				if lastReqTime := parseInt64(countersMap["last_request_time"]); lastReqTime > 0 {
-					stats.LastRequestTime = time.Unix(lastReqTime, 0)
-				}
-			}
-		}
-	}
-
-	// Parse usage hash (result[3])
-	usageHashExists := false
-	if usageSlice, ok := resultSlice[3].([]interface{}); ok && len(usageSlice) > 0 {
-		usageHashExists = true
-		usageMap := parseHashMap(usageSlice)
-		stats.CurrentMinuteTPM = parseInt64(usageMap["tpm"])
-		stats.CurrentMinuteRPM = parseInt64(usageMap["rpm"])
-	}
-
-	// Calculate average latency
-	if len(stats.LatencyHistory) > 0 {
-		var sum float64
-		for _, lat := range stats.LatencyHistory {
-			sum += lat
-		}
-		stats.AvgLatencyMs = sum / float64(len(stats.LatencyHistory))
-	}
-
-	// Calculate average TTFT
-	if len(stats.TTFTHistory) > 0 {
-		var sum float64
-		for _, ttft := range stats.TTFTHistory {
-			sum += ttft
-		}
-		stats.AvgTTFTMs = sum / float64(len(stats.TTFTHistory))
-	}
-
-	// Get cooldown status
-	cooldownUntil, _ := r.GetCooldownUntil(ctx, deploymentID)
-	stats.CooldownUntil = cooldownUntil
-
-	// Determine if the deployment exists in Redis.
-	// A deployment exists if ANY of the following is true:
-	// - The counters hash has any fields (even if all values are 0)
-	// - The latency/TTFT lists have any entries
-	// - The usage hash has any fields
-	// - A cooldown is set
-	//
-	// This is the industry-standard approach: check if the Redis key/hash EXISTS,
-	// not whether the values are non-zero. A deployment with [active_requests=0]
-	// still exists, it just has no active requests.
-	hasLatencyData := len(stats.LatencyHistory) > 0
-	hasTTFTData := len(stats.TTFTHistory) > 0
-	hasCooldown := !stats.CooldownUntil.IsZero()
-
-	deploymentExists := countersHashExists || hasLatencyData || hasTTFTData || usageHashExists || hasCooldown
-
-	if !deploymentExists {
-		return nil, ErrStatsNotFound
-	}
-
-	return stats, nil
-}
-
-// IncrementActiveRequests atomically increments the active request count.
-func (r *RedisStatsStore) IncrementActiveRequests(ctx context.Context, deploymentID string) error {
-	keys := []string{r.countersKey(deploymentID)}
-	_, err := r.incrementActiveReqScript.Run(ctx, r.client, keys).Result()
-	return err
-}
-
-// DecrementActiveRequests atomically decrements the active request count.
-func (r *RedisStatsStore) DecrementActiveRequests(ctx context.Context, deploymentID string) error {
-	keys := []string{r.countersKey(deploymentID)}
-	_, err := r.decrementActiveReqScript.Run(ctx, r.client, keys).Result()
-	return err
-}
-
-// RecordSuccess records a successful request with its metrics.
-func (r *RedisStatsStore) RecordSuccess(ctx context.Context, deploymentID string, metrics *ResponseMetrics) error {
-	currentMinute := time.Now().Format("2006-01-02-15-04")
-
-	keys := []string{
-		r.latencyKey(deploymentID),
-		r.ttftKey(deploymentID),
-		r.countersKey(deploymentID),
-		r.usageKey(deploymentID, currentMinute),
-	}
-
-	latencyMs := float64(metrics.Latency.Milliseconds())
-	ttftMs := float64(0)
-	if metrics.TimeToFirstToken > 0 {
-		ttftMs = float64(metrics.TimeToFirstToken.Milliseconds())
-	}
-
-	args := []interface{}{
-		latencyMs,
-		ttftMs,
-		metrics.TotalTokens,
-		r.maxLatencyListSize,
-		int(r.usageTTL.Seconds()),
-		time.Now().Unix(),
-	}
-
-	_, err := r.recordSuccessScript.Run(ctx, r.client, keys, args...).Result()
-	return err
-}
-
-// RecordFailure records a failed request.
-func (r *RedisStatsStore) RecordFailure(ctx context.Context, deploymentID string, err error) error {
-	keys := []string{
-		r.countersKey(deploymentID),
-		r.latencyKey(deploymentID),
-	}
-
-	isTimeout := 0
-	if llmErr, ok := err.(*llmerrors.LLMError); ok {
-		// Timeout errors: 408 (Request Timeout) or 504 (Gateway Timeout)
-		if llmErr.StatusCode == 408 || llmErr.StatusCode == 504 {
-			isTimeout = 1
-		}
-	}
-
-	args := []interface{}{
-		time.Now().Unix(),
-		isTimeout,
-		r.maxLatencyListSize,
-	}
-
-	_, runErr := r.recordFailureScript.Run(ctx, r.client, keys, args...).Result()
-	return runErr
-}
-
-// SetCooldown manually sets a cooldown period for a deployment.
-func (r *RedisStatsStore) SetCooldown(ctx context.Context, deploymentID string, until time.Time) error {
-	keys := []string{r.cooldownKey(deploymentID)}
-
-	ttl := time.Until(until)
-	if ttl <= 0 {
-		// Already expired, delete the key
-		return r.client.Del(ctx, keys[0]).Err()
-	}
-
-	args := []interface{}{
-		until.Unix(),
-		int(ttl.Seconds()) + 10, // Add 10s buffer to TTL
-	}
-
-	_, err := r.setCooldownScript.Run(ctx, r.client, keys, args...).Result()
-	return err
-}
-
-// GetCooldownUntil returns the cooldown expiration time for a deployment.
-func (r *RedisStatsStore) GetCooldownUntil(ctx context.Context, deploymentID string) (time.Time, error) {
-	key := r.cooldownKey(deploymentID)
-	val, err := r.client.Get(ctx, key).Result()
-	if err == redis.Nil {
-		return time.Time{}, nil
-	}
-	if err != nil {
-		return time.Time{}, err
-	}
-
-	timestamp, err := strconv.ParseInt(val, 10, 64)
-	if err != nil {
-		return time.Time{}, err
-	}
-
-	return time.Unix(timestamp, 0), nil
-}
-
-// ListDeployments returns all deployment IDs that have stats recorded.
-func (r *RedisStatsStore) ListDeployments(ctx context.Context) ([]string, error) {
-	pattern := r.keyPrefix + ":*:counters"
-	var deploymentIDs []string
-
-	iter := r.client.Scan(ctx, 0, pattern, 100).Iterator()
-	for iter.Next(ctx) {
-		key := iter.Val()
-		// Extract deployment ID from key
-		// Format: "llmux:router:stats:{deploymentID}:counters"
-		deploymentID := r.extractDeploymentID(key)
-		if deploymentID != "" {
-			deploymentIDs = append(deploymentIDs, deploymentID)
-		}
-	}
-
-	if err := iter.Err(); err != nil {
-		return nil, err
-	}
-
-	return deploymentIDs, nil
-}
-
-// DeleteStats removes all stats for a deployment.
-func (r *RedisStatsStore) DeleteStats(ctx context.Context, deploymentID string) error {
-	keys := []string{
-		r.latencyKey(deploymentID),
-		r.ttftKey(deploymentID),
-		r.countersKey(deploymentID),
-		r.cooldownKey(deploymentID),
-		r.usageKeyPrefix(deploymentID) + "*", // Pattern for SCAN
-	}
-
-	_, err := r.deleteStatsScript.Run(ctx, r.client, keys).Result()
-	return err
-}
-
-// Close releases any resources held by the store.
-func (r *RedisStatsStore) Close() error {
-	// Redis client is shared, don't close it here
-	return nil
-}
-
-// Key generation helpers
-
-func (r *RedisStatsStore) latencyKey(deploymentID string) string {
-	return fmt.Sprintf("%s:%s:latency", r.keyPrefix, deploymentID)
-}
-
-func (r *RedisStatsStore) ttftKey(deploymentID string) string {
-	return fmt.Sprintf("%s:%s:ttft", r.keyPrefix, deploymentID)
-}
-
-func (r *RedisStatsStore) countersKey(deploymentID string) string {
-	return fmt.Sprintf("%s:%s:counters", r.keyPrefix, deploymentID)
-}
-
-func (r *RedisStatsStore) cooldownKey(deploymentID string) string {
-	return fmt.Sprintf("%s:%s:cooldown", r.keyPrefix, deploymentID)
-}
-
-func (r *RedisStatsStore) usageKeyPrefix(deploymentID string) string {
-	return fmt.Sprintf("%s:%s:usage:", r.keyPrefix, deploymentID)
-}
-
-func (r *RedisStatsStore) usageKey(deploymentID, minute string) string {
-	return fmt.Sprintf("%s:%s:usage:%s", r.keyPrefix, deploymentID, minute)
-}
-
-func (r *RedisStatsStore) extractDeploymentID(key string) string {
-	// Extract from "prefix:{deploymentID}:counters"
-	prefix := r.keyPrefix + ":"
-	suffix := ":counters"
-
-	if len(key) <= len(prefix)+len(suffix) {
-		return ""
-	}
-
-	start := len(prefix)
-	end := len(key) - len(suffix)
-
-	if end <= start {
-		return ""
-	}
-
-	return key[start:end]
-}
-
-// Parsing helpers
-
-func parseFloat(v interface{}) (float64, error) {
-	switch val := v.(type) {
-	case string:
-		return strconv.ParseFloat(val, 64)
-	case float64:
-		return val, nil
-	case int64:
-		return float64(val), nil
-	default:
-		return 0, fmt.Errorf("cannot parse float from %T", v)
-	}
-}
-
-func parseInt64(v interface{}) int64 {
-	switch val := v.(type) {
-	case string:
-		i, _ := strconv.ParseInt(val, 10, 64)
-		return i
-	case int64:
-		return val
-	case float64:
-		return int64(val)
-	default:
-		return 0
-	}
-}
-
-func parseHashMap(slice []interface{}) map[string]interface{} {
-	m := make(map[string]interface{})
-	for i := 0; i < len(slice); i += 2 {
-		if i+1 < len(slice) {
-			key, ok1 := slice[i].(string)
-			val := slice[i+1]
-			if ok1 {
-				m[key] = val
-			}
-		}
-	}
-	return m
-}
diff --git a/internal/router/router_test.go b/internal/router/router_test.go
deleted file mode 100644
index 624191c..0000000
--- a/internal/router/router_test.go
+++ /dev/null
@@ -1,367 +0,0 @@
-package router
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
-)
-
-// Helper to create test deployments
-func createTestDeployments(count int) []*provider.Deployment {
-	deployments := make([]*provider.Deployment, count)
-	for i := 0; i < count; i++ {
-		deployments[i] = &provider.Deployment{
-			ID:           string(rune('a' + i)),
-			ProviderName: "test",
-			ModelName:    "gpt-4",
-		}
-	}
-	return deployments
-}
-
-func TestSimpleShuffleRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	config.Strategy = StrategySimpleShuffle
-	router := NewSimpleShuffleRouter(config)
-
-	deployments := createTestDeployments(3)
-	for _, d := range deployments {
-		router.AddDeployment(d)
-	}
-
-	ctx := context.Background()
-
-	// Should pick a deployment
-	picked, err := router.Pick(ctx, "gpt-4")
-	if err != nil {
-		t.Fatalf("unexpected error: %v", err)
-	}
-	if picked == nil {
-		t.Fatal("expected deployment, got nil")
-	}
-
-	// Should return error for unknown model
-	_, err = router.Pick(ctx, "unknown-model")
-	if err != ErrNoAvailableDeployment {
-		t.Errorf("expected ErrNoAvailableDeployment, got %v", err)
-	}
-}
-
-func TestSimpleShuffleRouter_WeightedPick(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewSimpleShuffleRouter(config)
-
-	// Add deployments with weights
-	d1 := &provider.Deployment{ID: "a", ModelName: "gpt-4"}
-	d2 := &provider.Deployment{ID: "b", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{Weight: 0.9})
-	router.AddDeploymentWithConfig(d2, DeploymentConfig{Weight: 0.1})
-
-	ctx := context.Background()
-	counts := make(map[string]int)
-
-	// Pick many times and count distribution
-	for i := 0; i < 1000; i++ {
-		picked, _ := router.Pick(ctx, "gpt-4")
-		counts[picked.ID]++
-	}
-
-	// d1 should be picked significantly more often
-	if counts["a"] < counts["b"]*2 {
-		t.Errorf("weighted selection not working: a=%d, b=%d", counts["a"], counts["b"])
-	}
-}
-
-func TestLowestLatencyRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	config.Strategy = StrategyLowestLatency
-	config.LatencyBuffer = 0.1
-	router := NewLowestLatencyRouter(config)
-
-	deployments := createTestDeployments(3)
-	for _, d := range deployments {
-		router.AddDeployment(d)
-	}
-
-	ctx := context.Background()
-
-	// Report different latencies
-	router.ReportSuccess(deployments[0], &ResponseMetrics{Latency: 100 * time.Millisecond})
-	router.ReportSuccess(deployments[1], &ResponseMetrics{Latency: 50 * time.Millisecond})
-	router.ReportSuccess(deployments[2], &ResponseMetrics{Latency: 200 * time.Millisecond})
-
-	// Should prefer deployment with lowest latency
-	counts := make(map[string]int)
-	for i := 0; i < 100; i++ {
-		picked, _ := router.Pick(ctx, "gpt-4")
-		counts[picked.ID]++
-	}
-
-	// Deployment "b" (50ms) should be picked most often
-	if counts["b"] < counts["a"] || counts["b"] < counts["c"] {
-		t.Errorf("lowest latency not preferred: a=%d, b=%d, c=%d", counts["a"], counts["b"], counts["c"])
-	}
-}
-
-func TestLowestLatencyRouter_StreamingTTFT(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewLowestLatencyRouter(config)
-
-	deployments := createTestDeployments(2)
-	for _, d := range deployments {
-		router.AddDeployment(d)
-	}
-
-	// d1: high latency but low TTFT
-	// d2: low latency but high TTFT
-	router.ReportSuccess(deployments[0], &ResponseMetrics{
-		Latency:          500 * time.Millisecond,
-		TimeToFirstToken: 50 * time.Millisecond,
-	})
-	router.ReportSuccess(deployments[1], &ResponseMetrics{
-		Latency:          100 * time.Millisecond,
-		TimeToFirstToken: 200 * time.Millisecond,
-	})
-
-	ctx := context.Background()
-
-	// For streaming requests, should prefer d1 (lower TTFT)
-	counts := make(map[string]int)
-	for i := 0; i < 100; i++ {
-		picked, _ := router.PickWithContext(ctx, &RequestContext{
-			Model:       "gpt-4",
-			IsStreaming: true,
-		})
-		counts[picked.ID]++
-	}
-
-	if counts["a"] < counts["b"] {
-		t.Errorf("TTFT not preferred for streaming: a=%d, b=%d", counts["a"], counts["b"])
-	}
-}
-
-func TestLeastBusyRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewLeastBusyRouter(config)
-
-	deployments := createTestDeployments(3)
-	for _, d := range deployments {
-		router.AddDeployment(d)
-	}
-
-	// Simulate active requests
-	router.ReportRequestStart(deployments[0])
-	router.ReportRequestStart(deployments[0])
-	router.ReportRequestStart(deployments[1])
-	// deployments[2] has 0 active requests
-
-	ctx := context.Background()
-
-	// Should always pick deployment with fewest active requests
-	for i := 0; i < 10; i++ {
-		picked, _ := router.Pick(ctx, "gpt-4")
-		if picked.ID != "c" {
-			t.Errorf("expected deployment c (0 active), got %s", picked.ID)
-		}
-	}
-}
-
-func TestLowestTPMRPMRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewLowestTPMRPMRouter(config)
-
-	d1 := &provider.Deployment{ID: "a", ModelName: "gpt-4"}
-	d2 := &provider.Deployment{ID: "b", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{TPMLimit: 1000, RPMLimit: 10})
-	router.AddDeploymentWithConfig(d2, DeploymentConfig{TPMLimit: 1000, RPMLimit: 10})
-
-	// Simulate usage on d1
-	router.ReportSuccess(d1, &ResponseMetrics{TotalTokens: 500})
-	router.ReportSuccess(d1, &ResponseMetrics{TotalTokens: 300})
-
-	ctx := context.Background()
-
-	// Should prefer d2 (lower usage)
-	picked, _ := router.Pick(ctx, "gpt-4")
-	if picked.ID != "b" {
-		t.Errorf("expected deployment b (lower usage), got %s", picked.ID)
-	}
-}
-
-func TestLowestTPMRPMRouter_RespectLimits(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewLowestTPMRPMRouter(config)
-
-	d1 := &provider.Deployment{ID: "a", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{TPMLimit: 100, RPMLimit: 5})
-
-	// Exhaust the TPM limit
-	router.ReportSuccess(d1, &ResponseMetrics{TotalTokens: 100})
-
-	ctx := context.Background()
-	reqCtx := &RequestContext{
-		Model:                "gpt-4",
-		EstimatedInputTokens: 50,
-	}
-
-	// Should return error when limit exceeded
-	_, err := router.PickWithContext(ctx, reqCtx)
-	if err != ErrNoAvailableDeployment {
-		t.Errorf("expected ErrNoAvailableDeployment, got %v", err)
-	}
-}
-
-func TestLowestCostRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewLowestCostRouter(config)
-
-	d1 := &provider.Deployment{ID: "expensive", ModelName: "gpt-4"}
-	d2 := &provider.Deployment{ID: "cheap", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{
-		InputCostPerToken:  0.01,
-		OutputCostPerToken: 0.03,
-	})
-	router.AddDeploymentWithConfig(d2, DeploymentConfig{
-		InputCostPerToken:  0.001,
-		OutputCostPerToken: 0.002,
-	})
-
-	ctx := context.Background()
-
-	// Should always pick cheaper deployment
-	for i := 0; i < 10; i++ {
-		picked, _ := router.Pick(ctx, "gpt-4")
-		if picked.ID != "cheap" {
-			t.Errorf("expected cheap deployment, got %s", picked.ID)
-		}
-	}
-}
-
-func TestTagBasedRouter_Pick(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewTagBasedRouter(config)
-
-	d1 := &provider.Deployment{ID: "premium", ModelName: "gpt-4"}
-	d2 := &provider.Deployment{ID: "standard", ModelName: "gpt-4"}
-	d3 := &provider.Deployment{ID: "default", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{Tags: []string{"premium", "fast"}})
-	router.AddDeploymentWithConfig(d2, DeploymentConfig{Tags: []string{"standard"}})
-	router.AddDeploymentWithConfig(d3, DeploymentConfig{Tags: []string{"default"}})
-
-	ctx := context.Background()
-
-	// Request with "premium" tag should only get premium deployment
-	picked, _ := router.PickWithContext(ctx, &RequestContext{
-		Model: "gpt-4",
-		Tags:  []string{"premium"},
-	})
-	if picked.ID != "premium" {
-		t.Errorf("expected premium deployment, got %s", picked.ID)
-	}
-
-	// Request with unknown tag should fall back to default
-	picked, _ = router.PickWithContext(ctx, &RequestContext{
-		Model: "gpt-4",
-		Tags:  []string{"unknown"},
-	})
-	if picked.ID != "default" {
-		t.Errorf("expected default deployment, got %s", picked.ID)
-	}
-}
-
-func TestTagBasedRouter_NoMatchingTags(t *testing.T) {
-	config := DefaultRouterConfig()
-	router := NewTagBasedRouter(config)
-
-	d1 := &provider.Deployment{ID: "premium", ModelName: "gpt-4"}
-	router.AddDeploymentWithConfig(d1, DeploymentConfig{Tags: []string{"premium"}})
-
-	ctx := context.Background()
-
-	// Request with non-matching tag and no default should fail
-	_, err := router.PickWithContext(ctx, &RequestContext{
-		Model: "gpt-4",
-		Tags:  []string{"standard"},
-	})
-	if err != ErrNoDeploymentsWithTag {
-		t.Errorf("expected ErrNoDeploymentsWithTag, got %v", err)
-	}
-}
-
-func TestRouter_Cooldown(t *testing.T) {
-	config := DefaultRouterConfig()
-	config.CooldownPeriod = 100 * time.Millisecond
-	router := NewSimpleShuffleRouter(config)
-
-	d := &provider.Deployment{ID: "a", ModelName: "gpt-4"}
-	router.AddDeployment(d)
-
-	// Trigger cooldown with a rate limit error
-	rateLimitErr := llmerrors.NewRateLimitError("test", "gpt-4", "rate limited")
-	router.ReportFailure(d, rateLimitErr)
-
-	// Should be in cooldown
-	if !router.IsCircuitOpen(d) {
-		t.Error("expected circuit to be open")
-	}
-
-	ctx := context.Background()
-	_, err := router.Pick(ctx, "gpt-4")
-	if err != ErrNoAvailableDeployment {
-		t.Errorf("expected ErrNoAvailableDeployment during cooldown, got %v", err)
-	}
-
-	// Wait for cooldown to expire
-	time.Sleep(150 * time.Millisecond)
-
-	// Should be available again
-	if router.IsCircuitOpen(d) {
-		t.Error("expected circuit to be closed after cooldown")
-	}
-
-	picked, err := router.Pick(ctx, "gpt-4")
-	if err != nil {
-		t.Errorf("unexpected error after cooldown: %v", err)
-	}
-	if picked.ID != "a" {
-		t.Errorf("expected deployment a, got %s", picked.ID)
-	}
-}
-
-func TestFactory_New(t *testing.T) {
-	strategies := AvailableStrategies()
-
-	for _, strategy := range strategies {
-		config := RouterConfig{Strategy: strategy}
-		router, err := New(config)
-		if err != nil {
-			t.Errorf("failed to create router for strategy %s: %v", strategy, err)
-		}
-		if router.GetStrategy() != strategy {
-			t.Errorf("expected strategy %s, got %s", strategy, router.GetStrategy())
-		}
-	}
-}
-
-func TestFactory_InvalidStrategy(t *testing.T) {
-	config := RouterConfig{Strategy: "invalid-strategy"}
-	_, err := New(config)
-	if err == nil {
-		t.Error("expected error for invalid strategy")
-	}
-}
-
-func TestIsValidStrategy(t *testing.T) {
-	if !IsValidStrategy("simple-shuffle") {
-		t.Error("simple-shuffle should be valid")
-	}
-	if !IsValidStrategy("lowest-latency") {
-		t.Error("lowest-latency should be valid")
-	}
-	if IsValidStrategy("invalid") {
-		t.Error("invalid should not be valid")
-	}
-}
diff --git a/internal/router/simple.go b/internal/router/simple.go
deleted file mode 100644
index 9537eb2..0000000
--- a/internal/router/simple.go
+++ /dev/null
@@ -1,94 +0,0 @@
-package router
-
-import (
-	"context"
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// SimpleRouter is a backward-compatible wrapper around SimpleShuffleRouter.
-//
-// Deprecated: Use NewSimpleShuffleRouter or New(config) instead.
-type SimpleRouter struct {
-	*SimpleShuffleRouter
-}
-
-// NewSimpleRouter creates a new simple router with the given cooldown period.
-//
-// Deprecated: Use NewSimpleShuffleRouter or New(config) instead.
-func NewSimpleRouter(cooldownPeriod time.Duration) *SimpleRouter {
-	config := RouterConfig{
-		Strategy:           StrategySimpleShuffle,
-		CooldownPeriod:     cooldownPeriod,
-		MaxLatencyListSize: 10,
-	}
-	return &SimpleRouter{
-		SimpleShuffleRouter: NewSimpleShuffleRouter(config),
-	}
-}
-
-// Pick selects a random healthy deployment for the given model.
-func (r *SimpleRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.SimpleShuffleRouter.Pick(ctx, model)
-}
-
-// PickWithContext selects a deployment using request context.
-func (r *SimpleRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	return r.SimpleShuffleRouter.PickWithContext(ctx, reqCtx)
-}
-
-// ReportSuccess records a successful request with metrics.
-func (r *SimpleRouter) ReportSuccess(deployment *provider.Deployment, metrics *ResponseMetrics) {
-	r.SimpleShuffleRouter.ReportSuccess(deployment, metrics)
-}
-
-// ReportFailure records a failed request.
-func (r *SimpleRouter) ReportFailure(deployment *provider.Deployment, err error) {
-	r.SimpleShuffleRouter.ReportFailure(deployment, err)
-}
-
-// ReportRequestStart records when a request starts.
-func (r *SimpleRouter) ReportRequestStart(deployment *provider.Deployment) {
-	r.SimpleShuffleRouter.ReportRequestStart(deployment)
-}
-
-// ReportRequestEnd records when a request ends.
-func (r *SimpleRouter) ReportRequestEnd(deployment *provider.Deployment) {
-	r.SimpleShuffleRouter.ReportRequestEnd(deployment)
-}
-
-// IsCircuitOpen checks if the deployment is in cooldown.
-func (r *SimpleRouter) IsCircuitOpen(deployment *provider.Deployment) bool {
-	return r.SimpleShuffleRouter.IsCircuitOpen(deployment)
-}
-
-// AddDeployment registers a new deployment.
-func (r *SimpleRouter) AddDeployment(deployment *provider.Deployment) {
-	r.SimpleShuffleRouter.AddDeployment(deployment)
-}
-
-// AddDeploymentWithConfig registers a deployment with routing configuration.
-func (r *SimpleRouter) AddDeploymentWithConfig(deployment *provider.Deployment, config DeploymentConfig) {
-	r.SimpleShuffleRouter.AddDeploymentWithConfig(deployment, config)
-}
-
-// RemoveDeployment removes a deployment from the router.
-func (r *SimpleRouter) RemoveDeployment(deploymentID string) {
-	r.SimpleShuffleRouter.RemoveDeployment(deploymentID)
-}
-
-// GetDeployments returns all deployments for a model.
-func (r *SimpleRouter) GetDeployments(model string) []*provider.Deployment {
-	return r.SimpleShuffleRouter.GetDeployments(model)
-}
-
-// GetStats returns the current stats for a deployment.
-func (r *SimpleRouter) GetStats(deploymentID string) *DeploymentStats {
-	return r.SimpleShuffleRouter.GetStats(deploymentID)
-}
-
-// GetStrategy returns the current routing strategy.
-func (r *SimpleRouter) GetStrategy() Strategy {
-	return r.SimpleShuffleRouter.GetStrategy()
-}
diff --git a/internal/router/simple_shuffle.go b/internal/router/simple_shuffle.go
deleted file mode 100644
index 3358e62..0000000
--- a/internal/router/simple_shuffle.go
+++ /dev/null
@@ -1,129 +0,0 @@
-package router
-
-import (
-	"context"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// SimpleShuffleRouter implements random selection with optional weighted picking.
-// Weights can be specified via weight, rpm, or tpm parameters in deployment config.
-type SimpleShuffleRouter struct {
-	*BaseRouter
-}
-
-// NewSimpleShuffleRouter creates a new simple shuffle router.
-func NewSimpleShuffleRouter(config RouterConfig) *SimpleShuffleRouter {
-	config.Strategy = StrategySimpleShuffle
-	return &SimpleShuffleRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects a random deployment, optionally weighted.
-func (r *SimpleShuffleRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext selects a deployment using weighted random selection if weights are configured.
-func (r *SimpleShuffleRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering if enabled
-	if r.config.EnableTagFiltering && len(reqCtx.Tags) > 0 {
-		healthy = r.filterByTags(healthy, reqCtx.Tags)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoDeploymentsWithTag
-		}
-	}
-
-	// Apply TPM/RPM filtering
-	if reqCtx.EstimatedInputTokens > 0 {
-		healthy = r.filterByTPMRPM(healthy, reqCtx.EstimatedInputTokens)
-		if len(healthy) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoAvailableDeployment
-		}
-	}
-
-	// Make a copy of healthy slice for use after releasing lock
-	healthyCopy := make([]*ExtendedDeployment, len(healthy))
-	copy(healthyCopy, healthy)
-	r.mu.RUnlock()
-
-	// Try weighted selection by weight, rpm, or tpm (in that order)
-	if deployment := r.weightedPick(healthyCopy, "weight"); deployment != nil {
-		return deployment, nil
-	}
-	if deployment := r.weightedPick(healthyCopy, "rpm"); deployment != nil {
-		return deployment, nil
-	}
-	if deployment := r.weightedPick(healthyCopy, "tpm"); deployment != nil {
-		return deployment, nil
-	}
-
-	// Fall back to uniform random selection (thread-safe)
-	return healthyCopy[r.randIntn(len(healthyCopy))].Deployment, nil
-}
-
-// weightedPick performs weighted random selection based on the specified weight type.
-// Returns nil if no weights are configured for the given type.
-// Note: This method uses thread-safe random functions.
-func (r *SimpleShuffleRouter) weightedPick(deployments []*ExtendedDeployment, weightType string) *provider.Deployment {
-	weights := make([]float64, len(deployments))
-	hasWeights := false
-
-	for i, d := range deployments {
-		var weight float64
-		switch weightType {
-		case "weight":
-			weight = d.Config.Weight
-		case "rpm":
-			weight = float64(d.Config.RPMLimit)
-		case "tpm":
-			weight = float64(d.Config.TPMLimit)
-		}
-		weights[i] = weight
-		if weight > 0 {
-			hasWeights = true
-		}
-	}
-
-	if !hasWeights {
-		return nil
-	}
-
-	// Calculate total weight
-	var totalWeight float64
-	for _, w := range weights {
-		totalWeight += w
-	}
-
-	if totalWeight == 0 {
-		return nil
-	}
-
-	// Normalize weights
-	for i := range weights {
-		weights[i] /= totalWeight
-	}
-
-	// Weighted random selection (thread-safe)
-	randVal := r.randFloat64()
-	var cumulative float64
-	for i, w := range weights {
-		cumulative += w
-		if randVal <= cumulative {
-			return deployments[i].Deployment
-		}
-	}
-
-	// Fallback to last deployment (shouldn't happen due to floating point)
-	return deployments[len(deployments)-1].Deployment
-}
diff --git a/internal/router/simple_test.go b/internal/router/simple_test.go
deleted file mode 100644
index 89efa9c..0000000
--- a/internal/router/simple_test.go
+++ /dev/null
@@ -1,227 +0,0 @@
-package router
-
-import (
-	"context"
-	"net/http"
-	"testing"
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
-)
-
-func TestSimpleRouter_Pick(t *testing.T) {
-	t.Run("returns deployment for registered model", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-		dep := &provider.Deployment{
-			ID:           "dep-1",
-			ProviderName: "openai",
-			ModelName:    "gpt-4",
-		}
-		r.AddDeployment(dep)
-
-		got, err := r.Pick(context.Background(), "gpt-4")
-		if err != nil {
-			t.Fatalf("Pick() error = %v", err)
-		}
-		if got.ID != "dep-1" {
-			t.Errorf("Pick() = %s, want dep-1", got.ID)
-		}
-	})
-
-	t.Run("returns error for unknown model", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-
-		_, err := r.Pick(context.Background(), "unknown-model")
-		if err != ErrNoAvailableDeployment {
-			t.Errorf("Pick() error = %v, want ErrNoAvailableDeployment", err)
-		}
-	})
-
-	t.Run("skips cooled down deployments", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-
-		dep1 := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-		dep2 := &provider.Deployment{ID: "dep-2", ModelName: "gpt-4", ProviderName: "openai"}
-		r.AddDeployment(dep1)
-		r.AddDeployment(dep2)
-
-		// Cool down dep-1
-		rateLimitErr := llmerrors.NewRateLimitError("openai", "gpt-4", "rate limited")
-		r.ReportFailure(dep1, rateLimitErr)
-
-		// Should only return dep-2
-		for i := 0; i < 10; i++ {
-			got, err := r.Pick(context.Background(), "gpt-4")
-			if err != nil {
-				t.Fatalf("Pick() error = %v", err)
-			}
-			if got.ID != "dep-2" {
-				t.Errorf("Pick() = %s, want dep-2 (dep-1 should be cooled down)", got.ID)
-			}
-		}
-	})
-
-	t.Run("returns error when all deployments cooled down", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-
-		dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-		r.AddDeployment(dep)
-
-		// Cool down the only deployment
-		rateLimitErr := llmerrors.NewRateLimitError("openai", "gpt-4", "rate limited")
-		r.ReportFailure(dep, rateLimitErr)
-
-		_, err := r.Pick(context.Background(), "gpt-4")
-		if err != ErrNoAvailableDeployment {
-			t.Errorf("Pick() error = %v, want ErrNoAvailableDeployment", err)
-		}
-	})
-}
-
-func TestSimpleRouter_ReportSuccess(t *testing.T) {
-	r := NewSimpleRouter(60 * time.Second)
-	dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-	r.AddDeployment(dep)
-
-	// Report multiple successes
-	r.ReportSuccess(dep, &ResponseMetrics{Latency: 100 * time.Millisecond})
-	r.ReportSuccess(dep, &ResponseMetrics{Latency: 200 * time.Millisecond})
-	r.ReportSuccess(dep, &ResponseMetrics{Latency: 150 * time.Millisecond})
-
-	// Verify stats updated
-	stats := r.GetStats(dep.ID)
-
-	if stats.TotalRequests != 3 {
-		t.Errorf("TotalRequests = %d, want 3", stats.TotalRequests)
-	}
-	if stats.SuccessCount != 3 {
-		t.Errorf("SuccessCount = %d, want 3", stats.SuccessCount)
-	}
-	if stats.AvgLatencyMs <= 0 {
-		t.Error("AvgLatencyMs should be positive")
-	}
-}
-
-func TestSimpleRouter_ReportFailure(t *testing.T) {
-	t.Run("cooldown on rate limit", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-		dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-		r.AddDeployment(dep)
-
-		err := llmerrors.NewRateLimitError("openai", "gpt-4", "rate limited")
-		r.ReportFailure(dep, err)
-
-		if !r.IsCircuitOpen(dep) {
-			t.Error("circuit should be open after rate limit error")
-		}
-	})
-
-	t.Run("no cooldown on bad request", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-		dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-		r.AddDeployment(dep)
-
-		err := llmerrors.NewInvalidRequestError("openai", "gpt-4", "bad request")
-		r.ReportFailure(dep, err)
-
-		if r.IsCircuitOpen(dep) {
-			t.Error("circuit should NOT be open after bad request error")
-		}
-	})
-
-	t.Run("cooldown on 5xx errors", func(t *testing.T) {
-		r := NewSimpleRouter(60 * time.Second)
-		dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-		r.AddDeployment(dep)
-
-		err := llmerrors.NewServiceUnavailableError("openai", "gpt-4", "service down")
-		r.ReportFailure(dep, err)
-
-		if !r.IsCircuitOpen(dep) {
-			t.Error("circuit should be open after 503 error")
-		}
-	})
-}
-
-func TestSimpleRouter_ModelAlias(t *testing.T) {
-	r := NewSimpleRouter(60 * time.Second)
-	dep := &provider.Deployment{
-		ID:           "dep-1",
-		ProviderName: "openai",
-		ModelName:    "gpt-4-0613",
-		ModelAlias:   "gpt-4", // User requests "gpt-4", routes to "gpt-4-0613"
-	}
-	r.AddDeployment(dep)
-
-	// Should be accessible via alias
-	got, err := r.Pick(context.Background(), "gpt-4")
-	if err != nil {
-		t.Fatalf("Pick() error = %v", err)
-	}
-	if got.ModelName != "gpt-4-0613" {
-		t.Errorf("ModelName = %s, want gpt-4-0613", got.ModelName)
-	}
-}
-
-func TestSimpleRouter_RemoveDeployment(t *testing.T) {
-	r := NewSimpleRouter(60 * time.Second)
-	dep := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-	r.AddDeployment(dep)
-
-	// Verify it exists
-	_, err := r.Pick(context.Background(), "gpt-4")
-	if err != nil {
-		t.Fatalf("deployment should exist: %v", err)
-	}
-
-	// Remove it
-	r.RemoveDeployment("dep-1")
-
-	// Should no longer be available
-	_, err = r.Pick(context.Background(), "gpt-4")
-	if err != ErrNoAvailableDeployment {
-		t.Errorf("Pick() error = %v, want ErrNoAvailableDeployment", err)
-	}
-}
-
-func TestSimpleRouter_GetDeployments(t *testing.T) {
-	r := NewSimpleRouter(60 * time.Second)
-
-	dep1 := &provider.Deployment{ID: "dep-1", ModelName: "gpt-4", ProviderName: "openai"}
-	dep2 := &provider.Deployment{ID: "dep-2", ModelName: "gpt-4", ProviderName: "azure"}
-	r.AddDeployment(dep1)
-	r.AddDeployment(dep2)
-
-	deps := r.GetDeployments("gpt-4")
-	if len(deps) != 2 {
-		t.Errorf("GetDeployments() count = %d, want 2", len(deps))
-	}
-}
-
-func TestIsCooldownRequired_Integration(t *testing.T) {
-	// Verify our error types trigger correct cooldown behavior
-	tests := []struct {
-		name     string
-		err      *llmerrors.LLMError
-		wantCool bool
-	}{
-		{"rate limit", llmerrors.NewRateLimitError("p", "m", "msg"), true},
-		{"auth error", llmerrors.NewAuthenticationError("p", "m", "msg"), true},
-		{"timeout", llmerrors.NewTimeoutError("p", "m", "msg"), true},
-		{"not found", llmerrors.NewNotFoundError("p", "m", "msg"), true},
-		{"service unavailable", llmerrors.NewServiceUnavailableError("p", "m", "msg"), true},
-		{"internal error", llmerrors.NewInternalError("p", "m", "msg"), true},
-		{"bad request", llmerrors.NewInvalidRequestError("p", "m", "msg"), false},
-	}
-
-	for _, tt := range tests {
-		t.Run(tt.name, func(t *testing.T) {
-			got := llmerrors.IsCooldownRequired(tt.err.StatusCode)
-			if got != tt.wantCool {
-				t.Errorf("IsCooldownRequired(%d) = %v, want %v (status: %s)",
-					tt.err.StatusCode, got, tt.wantCool, http.StatusText(tt.err.StatusCode))
-			}
-		})
-	}
-}
diff --git a/internal/router/stats_store.go b/internal/router/stats_store.go
deleted file mode 100644
index 3d36e17..0000000
--- a/internal/router/stats_store.go
+++ /dev/null
@@ -1,73 +0,0 @@
-// Package router provides request routing and load balancing for LLM deployments.
-package router
-
-import (
-	"context"
-	"errors"
-	"time"
-)
-
-var (
-	// ErrStatsNotFound is returned when stats for a deployment are not found.
-	ErrStatsNotFound = errors.New("stats not found for deployment")
-
-	// ErrStoreNotAvailable is returned when the stats store is not available.
-	ErrStoreNotAvailable = errors.New("stats store not available")
-)
-
-// StatsStore defines the interface for storing and retrieving deployment statistics.
-// Implementations can be in-memory (MemoryStatsStore) or distributed (RedisStatsStore).
-//
-// Design Principles:
-//   - Thread-safe: All methods must be safe for concurrent use
-//   - Fail-safe: Errors should not cause request failures, only logging
-//   - Context-aware: All methods accept context for cancellation and tracing
-type StatsStore interface {
-	// GetStats retrieves statistics for a deployment.
-	// Returns ErrStatsNotFound if the deployment has no recorded stats.
-	GetStats(ctx context.Context, deploymentID string) (*DeploymentStats, error)
-
-	// IncrementActiveRequests atomically increments the active request count.
-	// This is called when a request starts routing to a deployment.
-	IncrementActiveRequests(ctx context.Context, deploymentID string) error
-
-	// DecrementActiveRequests atomically decrements the active request count.
-	// This is called when a request completes (success or failure).
-	DecrementActiveRequests(ctx context.Context, deploymentID string) error
-
-	// RecordSuccess records a successful request with its metrics.
-	// This updates:
-	//   - Total request count
-	//   - Success count
-	//   - Latency history
-	//   - TTFT history (for streaming requests)
-	//   - TPM/RPM for current minute
-	RecordSuccess(ctx context.Context, deploymentID string, metrics *ResponseMetrics) error
-
-	// RecordFailure records a failed request.
-	// This updates:
-	//   - Total request count
-	//   - Failure count
-	//   - Cooldown status (if error is cooldown-worthy)
-	RecordFailure(ctx context.Context, deploymentID string, err error) error
-
-	// SetCooldown manually sets a cooldown period for a deployment.
-	// The deployment will be excluded from routing until the specified time.
-	SetCooldown(ctx context.Context, deploymentID string, until time.Time) error
-
-	// GetCooldownUntil returns the cooldown expiration time for a deployment.
-	// Returns zero time if no cooldown is active.
-	GetCooldownUntil(ctx context.Context, deploymentID string) (time.Time, error)
-
-	// ListDeployments returns all deployment IDs that have stats recorded.
-	// This is useful for monitoring and cleanup operations.
-	ListDeployments(ctx context.Context) ([]string, error)
-
-	// DeleteStats removes all stats for a deployment.
-	// This is called when a deployment is removed from the router.
-	DeleteStats(ctx context.Context, deploymentID string) error
-
-	// Close releases any resources held by the store.
-	// After Close is called, the store should not be used.
-	Close() error
-}
diff --git a/internal/router/stats_store_test.go b/internal/router/stats_store_test.go
deleted file mode 100644
index 156aa4f..0000000
--- a/internal/router/stats_store_test.go
+++ /dev/null
@@ -1,452 +0,0 @@
-package router_test
-
-import (
-	"context"
-	"fmt"
-	"sync"
-	"testing"
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/router"
-	"github.com/blueberrycongee/llmux/pkg/errors"
-	"github.com/redis/go-redis/v9"
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
-	"github.com/testcontainers/testcontainers-go"
-	"github.com/testcontainers/testcontainers-go/wait"
-)
-
-// setupRedisStoreIfAvailable attempts to start a Redis container for testing.
-// Returns nil if Docker is not available or container fails to start.
-// This allows tests to gracefully degrade to memory-only mode.
-func setupRedisStoreIfAvailable(t *testing.T) router.StatsStore {
-	t.Helper()
-
-	// Recover from panics (e.g., "rootless Docker is not supported on Windows")
-	defer func() {
-		if r := recover(); r != nil {
-			t.Logf("âš ï¸ Docker setup failed (panic recovered): %v", r)
-		}
-	}()
-
-	ctx := context.Background()
-
-	// Try to start Redis container
-	req := testcontainers.ContainerRequest{
-		Image:        "redis:7-alpine",
-		ExposedPorts: []string{"6379/tcp"},
-		WaitingFor:   wait.ForLog("Ready to accept connections").WithStartupTimeout(30 * time.Second),
-	}
-
-	redisContainer, err := testcontainers.GenericContainer(ctx, testcontainers.GenericContainerRequest{
-		ContainerRequest: req,
-		Started:          true,
-	})
-	if err != nil {
-		// Docker not available or container failed to start
-		t.Logf("âš ï¸ Failed to start Redis container: %v", err)
-		return nil
-	}
-
-	// Register cleanup
-	t.Cleanup(func() {
-		if terminateErr := redisContainer.Terminate(ctx); terminateErr != nil {
-			t.Logf("Failed to terminate Redis container: %v", terminateErr)
-		}
-	})
-
-	// Get container host and port
-	host, err := redisContainer.Host(ctx)
-	if err != nil {
-		t.Logf("Failed to get container host: %v", err)
-		return nil
-	}
-
-	port, err := redisContainer.MappedPort(ctx, "6379")
-	if err != nil {
-		t.Logf("Failed to get container port: %v", err)
-		return nil
-	}
-
-	// Create Redis client
-	addr := fmt.Sprintf("%s:%s", host, port.Port())
-	client := redis.NewClient(&redis.Options{
-		Addr: addr,
-	})
-
-	// Verify connection
-	pingCtx, cancel := context.WithTimeout(ctx, 5*time.Second)
-	defer cancel()
-	if err := client.Ping(pingCtx).Err(); err != nil {
-		t.Logf("Failed to ping Redis: %v", err)
-		return nil
-	}
-
-	t.Logf("âœ… Redis container started successfully at %s", addr)
-
-	// Create and return RedisStatsStore
-	return router.NewRedisStatsStore(client)
-}
-
-// TestStatsStore_Interface defines the contract that all StatsStore implementations must satisfy.
-// This is a comprehensive test suite that should be run against both MemoryStatsStore and RedisStatsStore.
-func TestStatsStore_Interface(t *testing.T) {
-	stores := map[string]router.StatsStore{
-		"Memory": router.NewMemoryStatsStore(),
-	}
-
-	// Try to add Redis store (requires Docker)
-	if redisStore := setupRedisStoreIfAvailable(t); redisStore != nil {
-		stores["Redis"] = redisStore
-		t.Log("âœ… Redis container started, testing distributed stats")
-	} else {
-		t.Log("âš ï¸ Docker not available, skipping Redis tests (Memory tests only)")
-	}
-
-	for storeName, store := range stores {
-		t.Run(storeName, func(t *testing.T) {
-			// Ensure cleanup after all subtests
-			defer func() {
-				if err := store.Close(); err != nil {
-					t.Logf("Store cleanup error: %v", err)
-				}
-			}()
-
-			// Run all contract tests
-			t.Run("RecordSuccess", func(t *testing.T) {
-				testRecordSuccess(t, store)
-			})
-
-			t.Run("RecordFailure", func(t *testing.T) {
-				testRecordFailure(t, store)
-			})
-
-			t.Run("ActiveRequestsIncDec", func(t *testing.T) {
-				testActiveRequestsIncDec(t, store)
-			})
-
-			t.Run("CooldownManagement", func(t *testing.T) {
-				testCooldownManagement(t, store)
-			})
-
-			t.Run("LatencyHistoryRolling", func(t *testing.T) {
-				testLatencyHistoryRolling(t, store)
-			})
-
-			t.Run("TPMRPMTracking", func(t *testing.T) {
-				testTPMRPMTracking(t, store)
-			})
-
-			t.Run("ListAndDeleteDeployments", func(t *testing.T) {
-				testListAndDeleteDeployments(t, store)
-			})
-
-			t.Run("ConcurrentAccess", func(t *testing.T) {
-				testConcurrentAccess(t, store)
-			})
-		})
-	}
-}
-
-// testRecordSuccess verifies basic success recording functionality.
-func testRecordSuccess(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-success"
-
-	// Test 1: Record a successful request
-	metrics := &router.ResponseMetrics{
-		Latency:          100 * time.Millisecond,
-		TimeToFirstToken: 50 * time.Millisecond,
-		TotalTokens:      50,
-		InputTokens:      10,
-		OutputTokens:     40,
-	}
-
-	err := store.RecordSuccess(ctx, deploymentID, metrics)
-	require.NoError(t, err)
-
-	// Test 2: Verify stats were recorded
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(1), stats.TotalRequests)
-	assert.Equal(t, int64(1), stats.SuccessCount)
-	assert.Equal(t, int64(0), stats.FailureCount)
-
-	// Test 3: Verify latency was recorded
-	require.Len(t, stats.LatencyHistory, 1)
-	assert.Equal(t, 100.0, stats.LatencyHistory[0])
-
-	// Test 4: Verify TTFT was recorded
-	require.Len(t, stats.TTFTHistory, 1)
-	assert.Equal(t, 50.0, stats.TTFTHistory[0])
-
-	// Test 5: Verify average latency
-	assert.Equal(t, 100.0, stats.AvgLatencyMs)
-}
-
-// testRecordFailure verifies failure recording functionality.
-func testRecordFailure(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-failure"
-
-	// Test 1: Record a failed request
-	err := store.RecordFailure(ctx, deploymentID, errors.NewServiceUnavailableError(
-		"openai",
-		"gpt-4",
-		"Internal Server Error",
-	))
-	require.NoError(t, err)
-
-	// Test 2: Verify stats were recorded
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(1), stats.TotalRequests)
-	assert.Equal(t, int64(0), stats.SuccessCount)
-	assert.Equal(t, int64(1), stats.FailureCount)
-}
-
-// testActiveRequestsIncDec verifies active request counting.
-func testActiveRequestsIncDec(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-active"
-
-	// Test 1: Increment active requests
-	err := store.IncrementActiveRequests(ctx, deploymentID)
-	require.NoError(t, err)
-
-	err = store.IncrementActiveRequests(ctx, deploymentID)
-	require.NoError(t, err)
-
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(2), stats.ActiveRequests)
-
-	// Test 2: Decrement active requests
-	err = store.DecrementActiveRequests(ctx, deploymentID)
-	require.NoError(t, err)
-
-	stats, err = store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(1), stats.ActiveRequests)
-
-	// Test 3: Decrement to zero
-	err = store.DecrementActiveRequests(ctx, deploymentID)
-	require.NoError(t, err)
-
-	stats, err = store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(0), stats.ActiveRequests)
-
-	// Test 4: Decrementing below zero should not panic or error
-	err = store.DecrementActiveRequests(ctx, deploymentID)
-	require.NoError(t, err)
-
-	stats, err = store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(0), stats.ActiveRequests, "Active requests should not go below zero")
-}
-
-// testCooldownManagement verifies cooldown functionality.
-func testCooldownManagement(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-cooldown"
-
-	// Test 1: Initially no cooldown
-	cooldownUntil, err := store.GetCooldownUntil(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.True(t, cooldownUntil.IsZero())
-
-	// Test 2: Set cooldown
-	future := time.Now().Add(5 * time.Minute)
-	err = store.SetCooldown(ctx, deploymentID, future)
-	require.NoError(t, err)
-
-	// Test 3: Verify cooldown was set
-	cooldownUntil, err = store.GetCooldownUntil(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.True(t, cooldownUntil.After(time.Now()))
-	assert.WithinDuration(t, future, cooldownUntil, time.Second)
-
-	// Test 4: Stats should reflect cooldown
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.True(t, stats.CooldownUntil.After(time.Now()))
-}
-
-// testLatencyHistoryRolling verifies rolling window behavior.
-func testLatencyHistoryRolling(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-rolling"
-
-	// Create store with small max size for testing
-	_, ok := store.(*router.MemoryStatsStore)
-	if !ok {
-		t.Skip("Rolling window test only for MemoryStatsStore")
-	}
-
-	// Re-create with max size = 3
-	memStore := router.NewMemoryStatsStoreWithConfig(3)
-	defer memStore.Close()
-
-	// Test 1: Add latencies up to max size
-	for i := 0; i < 3; i++ {
-		metrics := &router.ResponseMetrics{
-			Latency: time.Duration(i*100) * time.Millisecond,
-		}
-		err := memStore.RecordSuccess(ctx, deploymentID, metrics)
-		require.NoError(t, err)
-	}
-
-	stats, _ := memStore.GetStats(ctx, deploymentID)
-	require.Len(t, stats.LatencyHistory, 3)
-	assert.Equal(t, []float64{0.0, 100.0, 200.0}, stats.LatencyHistory)
-
-	// Test 2: Add one more to trigger rolling
-	metrics := &router.ResponseMetrics{
-		Latency: 300 * time.Millisecond,
-	}
-	err := memStore.RecordSuccess(ctx, deploymentID, metrics)
-	require.NoError(t, err)
-
-	stats, _ = memStore.GetStats(ctx, deploymentID)
-	require.Len(t, stats.LatencyHistory, 3, "History should maintain max size")
-	assert.Equal(t, []float64{100.0, 200.0, 300.0}, stats.LatencyHistory, "Oldest value should be dropped")
-}
-
-// testTPMRPMTracking verifies per-minute token and request tracking.
-func testTPMRPMTracking(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-tpm-rpm"
-
-	// Test 1: Record multiple requests
-	for i := 0; i < 5; i++ {
-		metrics := &router.ResponseMetrics{
-			Latency:     100 * time.Millisecond,
-			TotalTokens: 100,
-		}
-		err := store.RecordSuccess(ctx, deploymentID, metrics)
-		require.NoError(t, err)
-	}
-
-	// Test 2: Verify TPM/RPM
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-	assert.Equal(t, int64(500), stats.CurrentMinuteTPM, "TPM = 5 requests * 100 tokens")
-	assert.Equal(t, int64(5), stats.CurrentMinuteRPM, "RPM = 5 requests")
-
-	// Test 3: Verify minute key format
-	expectedMinute := time.Now().Format("2006-01-02-15-04")
-	assert.Equal(t, expectedMinute, stats.CurrentMinuteKey)
-}
-
-// testListAndDeleteDeployments verifies deployment listing and deletion.
-func testListAndDeleteDeployments(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-
-	// Test 1: Initially empty
-	deployments, err := store.ListDeployments(ctx)
-	require.NoError(t, err)
-	initialCount := len(deployments)
-
-	// Test 2: Add some deployments
-	for i := 0; i < 3; i++ {
-		deploymentID := "test-deployment-" + string(rune('A'+i))
-		metrics := &router.ResponseMetrics{Latency: 100 * time.Millisecond}
-		recordErr := store.RecordSuccess(ctx, deploymentID, metrics)
-		require.NoError(t, recordErr)
-	}
-
-	deployments, err = store.ListDeployments(ctx)
-	require.NoError(t, err)
-	assert.Len(t, deployments, initialCount+3)
-
-	// Test 3: Delete one deployment
-	err = store.DeleteStats(ctx, "test-deployment-A")
-	require.NoError(t, err)
-
-	deployments, err = store.ListDeployments(ctx)
-	require.NoError(t, err)
-	assert.Len(t, deployments, initialCount+2)
-
-	// Test 4: Deleted deployment should return error
-	_, err = store.GetStats(ctx, "test-deployment-A")
-	assert.ErrorIs(t, err, router.ErrStatsNotFound)
-}
-
-// testConcurrentAccess verifies thread-safety.
-func testConcurrentAccess(t *testing.T, store router.StatsStore) {
-	ctx := context.Background()
-	deploymentID := "test-deployment-concurrent"
-
-	const (
-		goroutines = 100
-		operations = 10
-	)
-
-	var wg sync.WaitGroup
-	wg.Add(goroutines)
-
-	// Test: Concurrent increments
-	for i := 0; i < goroutines; i++ {
-		go func() {
-			defer wg.Done()
-
-			for j := 0; j < operations; j++ {
-				// Mix of operations
-				_ = store.IncrementActiveRequests(ctx, deploymentID)
-
-				metrics := &router.ResponseMetrics{
-					Latency:     100 * time.Millisecond,
-					TotalTokens: 50,
-				}
-				_ = store.RecordSuccess(ctx, deploymentID, metrics)
-
-				_ = store.DecrementActiveRequests(ctx, deploymentID)
-			}
-		}()
-	}
-
-	wg.Wait()
-
-	// Verify: Stats should be consistent
-	stats, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-
-	// Total requests should equal goroutines * operations
-	assert.Equal(t, int64(goroutines*operations), stats.TotalRequests)
-	assert.Equal(t, int64(goroutines*operations), stats.SuccessCount)
-
-	// Active requests should be zero (all increments matched with decrements)
-	assert.Equal(t, int64(0), stats.ActiveRequests)
-}
-
-// TestMemoryStatsStore_DeepCopy verifies that GetStats returns a copy.
-func TestMemoryStatsStore_DeepCopy(t *testing.T) {
-	store := router.NewMemoryStatsStore()
-	defer store.Close()
-
-	ctx := context.Background()
-	deploymentID := "test-deployment-copy"
-
-	// Record some stats
-	metrics := &router.ResponseMetrics{
-		Latency:     100 * time.Millisecond,
-		TotalTokens: 50,
-	}
-	err := store.RecordSuccess(ctx, deploymentID, metrics)
-	require.NoError(t, err)
-
-	// Get stats twice
-	stats1, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-
-	stats2, err := store.GetStats(ctx, deploymentID)
-	require.NoError(t, err)
-
-	// Modify stats1.LatencyHistory
-	stats1.LatencyHistory[0] = 999.0
-
-	// Verify stats2 was not affected (deep copy)
-	assert.NotEqual(t, stats1.LatencyHistory[0], stats2.LatencyHistory[0])
-	assert.Equal(t, 100.0, stats2.LatencyHistory[0], "Stats should be independent copies")
-}
diff --git a/internal/router/tag_based.go b/internal/router/tag_based.go
deleted file mode 100644
index cd39ce1..0000000
--- a/internal/router/tag_based.go
+++ /dev/null
@@ -1,73 +0,0 @@
-package router
-
-import (
-	"context"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// TagBasedRouter filters deployments based on request tags before applying
-// another routing strategy (defaults to random selection).
-//
-// Tag matching rules:
-//   - If request has tags, only deployments with at least one matching tag are considered
-//   - If no deployments match, deployments with "default" tag are used as fallback
-//   - If request has no tags, deployments with "default" tag are preferred
-//   - If no "default" deployments exist, all deployments are considered
-//
-// Example usage:
-//
-//	// Deployment config
-//	config := DeploymentConfig{Tags: []string{"premium", "us-east"}}
-//
-//	// Request with tags
-//	reqCtx := &RequestContext{Tags: []string{"premium"}}
-type TagBasedRouter struct {
-	*BaseRouter
-}
-
-// NewTagBasedRouter creates a new tag-based router.
-func NewTagBasedRouter(config RouterConfig) *TagBasedRouter {
-	config.Strategy = StrategyTagBased
-	config.EnableTagFiltering = true // Always enable for this router
-	return &TagBasedRouter{
-		BaseRouter: NewBaseRouter(config),
-	}
-}
-
-// Pick selects a random deployment (tag filtering requires context).
-func (r *TagBasedRouter) Pick(ctx context.Context, model string) (*provider.Deployment, error) {
-	return r.PickWithContext(ctx, &RequestContext{Model: model})
-}
-
-// PickWithContext filters deployments by tags and selects randomly.
-func (r *TagBasedRouter) PickWithContext(ctx context.Context, reqCtx *RequestContext) (*provider.Deployment, error) {
-	r.mu.RLock()
-	healthy := r.getHealthyDeployments(reqCtx.Model)
-	if len(healthy) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoAvailableDeployment
-	}
-
-	// Apply tag filtering
-	filtered := r.filterByTags(healthy, reqCtx.Tags)
-	if len(filtered) == 0 {
-		r.mu.RUnlock()
-		return nil, ErrNoDeploymentsWithTag
-	}
-
-	// Apply TPM/RPM filtering
-	if reqCtx.EstimatedInputTokens > 0 {
-		filtered = r.filterByTPMRPM(filtered, reqCtx.EstimatedInputTokens)
-		if len(filtered) == 0 {
-			r.mu.RUnlock()
-			return nil, ErrNoAvailableDeployment
-		}
-	}
-
-	n := len(filtered)
-	r.mu.RUnlock()
-
-	// Random selection from filtered deployments (thread-safe)
-	return filtered[r.randIntn(n)].Deployment, nil
-}
diff --git a/internal/router/types.go b/internal/router/types.go
deleted file mode 100644
index 2181580..0000000
--- a/internal/router/types.go
+++ /dev/null
@@ -1,161 +0,0 @@
-// Package router provides request routing and load balancing for LLM deployments.
-package router
-
-import (
-	"time"
-
-	"github.com/blueberrycongee/llmux/internal/provider"
-)
-
-// Strategy defines the routing strategy type.
-type Strategy string
-
-const (
-	// StrategySimpleShuffle randomly selects from available deployments.
-	// Supports weighted selection based on weight/rpm/tpm parameters.
-	StrategySimpleShuffle Strategy = "simple-shuffle"
-
-	// StrategyLowestLatency selects the deployment with lowest average latency.
-	// For streaming requests, uses Time To First Token (TTFT) instead.
-	StrategyLowestLatency Strategy = "lowest-latency"
-
-	// StrategyLeastBusy selects the deployment with fewest active requests.
-	StrategyLeastBusy Strategy = "least-busy"
-
-	// StrategyLowestTPMRPM selects the deployment with lowest TPM/RPM usage.
-	// Useful for staying within rate limits across multiple deployments.
-	StrategyLowestTPMRPM Strategy = "lowest-tpm-rpm"
-
-	// StrategyLowestCost selects the deployment with lowest cost per token.
-	// Considers both input and output token costs.
-	StrategyLowestCost Strategy = "lowest-cost"
-
-	// StrategyTagBased filters deployments based on request tags.
-	// Only deployments with matching tags are considered.
-	StrategyTagBased Strategy = "tag-based"
-)
-
-// DeploymentStats tracks performance metrics for a deployment.
-type DeploymentStats struct {
-	// Request counts
-	TotalRequests  int64
-	SuccessCount   int64
-	FailureCount   int64
-	ActiveRequests int64
-
-	// Latency tracking (rolling window)
-	LatencyHistory     []float64 // in milliseconds
-	TTFTHistory        []float64 // Time To First Token for streaming
-	AvgLatencyMs       float64
-	AvgTTFTMs          float64
-	MaxLatencyListSize int
-
-	// Usage tracking (per minute)
-	CurrentMinuteTPM int64  // Tokens Per Minute
-	CurrentMinuteRPM int64  // Requests Per Minute
-	CurrentMinuteKey string // Format: "YYYY-MM-DD-HH-MM"
-
-	// Timing
-	LastRequestTime time.Time
-	CooldownUntil   time.Time
-}
-
-// RouterConfig contains router configuration options.
-type RouterConfig struct {
-	// Strategy determines how deployments are selected
-	Strategy Strategy
-
-	// CooldownPeriod is how long to wait before retrying a failed deployment
-	CooldownPeriod time.Duration
-
-	// LatencyBuffer for lowest-latency: select randomly within this % of lowest
-	// e.g., 0.1 means select from deployments within 10% of the lowest latency
-	LatencyBuffer float64
-
-	// MaxLatencyListSize is the maximum number of latency samples to keep
-	MaxLatencyListSize int
-
-	// TTL for cached metrics (default: 1 hour)
-	MetricsTTL time.Duration
-
-	// EnableTagFiltering enables tag-based deployment filtering
-	EnableTagFiltering bool
-}
-
-// DefaultRouterConfig returns sensible default router configuration.
-func DefaultRouterConfig() RouterConfig {
-	return RouterConfig{
-		Strategy:           StrategySimpleShuffle,
-		CooldownPeriod:     60 * time.Second,
-		LatencyBuffer:      0.1, // 10% buffer
-		MaxLatencyListSize: 10,
-		MetricsTTL:         1 * time.Hour,
-		EnableTagFiltering: false,
-	}
-}
-
-// RequestContext contains request-specific information for routing decisions.
-type RequestContext struct {
-	// Model is the requested model name
-	Model string
-
-	// IsStreaming indicates if this is a streaming request
-	IsStreaming bool
-
-	// Tags are request-level tags for tag-based routing
-	Tags []string
-
-	// EstimatedInputTokens for TPM/RPM calculations
-	EstimatedInputTokens int
-
-	// Metadata contains additional request metadata
-	Metadata map[string]string
-}
-
-// DeploymentConfig contains deployment-specific configuration for routing.
-type DeploymentConfig struct {
-	// Weight for weighted random selection (simple-shuffle)
-	Weight float64
-
-	// TPM limit for this deployment (0 = unlimited)
-	TPMLimit int64
-
-	// RPM limit for this deployment (0 = unlimited)
-	RPMLimit int64
-
-	// InputCostPerToken for cost-based routing
-	InputCostPerToken float64
-
-	// OutputCostPerToken for cost-based routing
-	OutputCostPerToken float64
-
-	// Tags for tag-based routing
-	Tags []string
-}
-
-// ExtendedDeployment wraps a deployment with routing-specific configuration.
-type ExtendedDeployment struct {
-	*provider.Deployment
-	Config DeploymentConfig
-}
-
-// ResponseMetrics contains metrics from a completed request.
-type ResponseMetrics struct {
-	// Latency is the total request duration
-	Latency time.Duration
-
-	// TimeToFirstToken is the time until first streaming chunk (for streaming requests)
-	TimeToFirstToken time.Duration
-
-	// TotalTokens is the total tokens used (input + output)
-	TotalTokens int
-
-	// InputTokens is the number of input tokens
-	InputTokens int
-
-	// OutputTokens is the number of output tokens
-	OutputTokens int
-
-	// Cost is the calculated cost of the request
-	Cost float64
-}
diff --git a/options.go b/options.go
index d67d482..c59a91c 100644
--- a/options.go
+++ b/options.go
@@ -7,7 +7,7 @@ import (
 	"github.com/blueberrycongee/llmux/internal/observability"
 	"github.com/blueberrycongee/llmux/internal/plugin"
 	"github.com/blueberrycongee/llmux/internal/resilience"
-	"github.com/blueberrycongee/llmux/internal/router"
+	"github.com/blueberrycongee/llmux/pkg/router"
 )
 
 // RateLimitKeyStrategy defines how to derive the rate limit key.
diff --git a/routers/base.go b/routers/base.go
index f6b5ad7..a40d5d3 100644
--- a/routers/base.go
+++ b/routers/base.go
@@ -7,7 +7,6 @@ import (
 	"sync"
 	"time"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	llmerrors "github.com/blueberrycongee/llmux/pkg/errors"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
@@ -55,7 +54,7 @@ type BaseRouter struct {
 	// statsStore is an optional distributed stats store.
 	// When nil, local stats map is used (backward compatible).
 	// When set, stats operations delegate to the store (distributed mode).
-	statsStore internalRouter.StatsStore
+	statsStore router.StatsStore
 }
 
 // NewBaseRouter creates a new base router with the given configuration.
@@ -73,7 +72,7 @@ func NewBaseRouter(config router.Config) *BaseRouter {
 
 // NewBaseRouterWithStore creates a new base router with a distributed stats store.
 // This enables multi-instance deployments to share routing statistics.
-func NewBaseRouterWithStore(config router.Config, store internalRouter.StatsStore) *BaseRouter {
+func NewBaseRouterWithStore(config router.Config, store router.StatsStore) *BaseRouter {
 	r := NewBaseRouter(config)
 	r.statsStore = store
 	return r
@@ -245,17 +244,8 @@ func (r *BaseRouter) ReportRequestEnd(deployment *provider.Deployment) {
 func (r *BaseRouter) ReportSuccess(deployment *provider.Deployment, metrics *router.ResponseMetrics) {
 	// Distributed mode: delegate to StatsStore
 	if r.statsStore != nil {
-		// Convert to internal metrics type
-		internalMetrics := &internalRouter.ResponseMetrics{
-			Latency:          metrics.Latency,
-			TimeToFirstToken: metrics.TimeToFirstToken,
-			TotalTokens:      metrics.TotalTokens,
-			InputTokens:      metrics.InputTokens,
-			OutputTokens:     metrics.OutputTokens,
-			Cost:             metrics.Cost,
-		}
 		// Fail-safe: ignore errors
-		_ = r.statsStore.RecordSuccess(context.Background(), deployment.ID, internalMetrics)
+		_ = r.statsStore.RecordSuccess(context.Background(), deployment.ID, metrics)
 		return
 	}
 
diff --git a/routers/base_statsstore_test.go b/routers/base_statsstore_test.go
index b9f75ce..126dbc1 100644
--- a/routers/base_statsstore_test.go
+++ b/routers/base_statsstore_test.go
@@ -5,7 +5,6 @@ import (
 	"testing"
 	"time"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 	"github.com/blueberrycongee/llmux/routers"
@@ -204,7 +203,7 @@ type mockStatsStore struct {
 
 	// Last call arguments
 	lastDeploymentID string
-	lastMetrics      *internalRouter.ResponseMetrics
+	lastMetrics      *router.ResponseMetrics
 	lastError        error
 
 	// Behavior control
@@ -212,8 +211,8 @@ type mockStatsStore struct {
 	cooldownUntil time.Time
 }
 
-func (m *mockStatsStore) GetStats(ctx context.Context, deploymentID string) (*internalRouter.DeploymentStats, error) {
-	return &internalRouter.DeploymentStats{}, nil
+func (m *mockStatsStore) GetStats(ctx context.Context, deploymentID string) (*router.DeploymentStats, error) {
+	return &router.DeploymentStats{}, nil
 }
 
 func (m *mockStatsStore) IncrementActiveRequests(ctx context.Context, deploymentID string) error {
@@ -234,7 +233,7 @@ func (m *mockStatsStore) DecrementActiveRequests(ctx context.Context, deployment
 	return nil
 }
 
-func (m *mockStatsStore) RecordSuccess(ctx context.Context, deploymentID string, metrics *internalRouter.ResponseMetrics) error {
+func (m *mockStatsStore) RecordSuccess(ctx context.Context, deploymentID string, metrics *router.ResponseMetrics) error {
 	m.successCalls++
 	m.lastDeploymentID = deploymentID
 	m.lastMetrics = metrics
diff --git a/routers/cost.go b/routers/cost.go
index d733274..cbb1108 100644
--- a/routers/cost.go
+++ b/routers/cost.go
@@ -5,7 +5,6 @@ import (
 	"fmt"
 	"sort"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/pricing"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
@@ -53,7 +52,7 @@ func NewCostRouterWithConfig(config router.Config) *CostRouter {
 }
 
 // newCostRouterWithStore creates a new cost router with optional distributed StatsStore.
-func newCostRouterWithStore(config router.Config, store internalRouter.StatsStore) *CostRouter {
+func newCostRouterWithStore(config router.Config, store router.StatsStore) *CostRouter {
 	config.Strategy = router.StrategyLowestCost
 	var base *BaseRouter
 	if store != nil {
diff --git a/routers/factory.go b/routers/factory.go
index 889c79f..c6749a2 100644
--- a/routers/factory.go
+++ b/routers/factory.go
@@ -3,7 +3,6 @@ package routers
 import (
 	"fmt"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
 
@@ -16,7 +15,7 @@ func New(config router.Config) (router.Router, error) {
 // NewWithStore creates a new router with a distributed stats store.
 // When store is nil, the router uses local in-memory stats (single-instance mode).
 // When store is provided, stats are shared across multiple instances (distributed mode).
-func NewWithStore(config router.Config, store internalRouter.StatsStore) (router.Router, error) {
+func NewWithStore(config router.Config, store router.StatsStore) (router.Router, error) {
 	switch config.Strategy {
 	case router.StrategySimpleShuffle, "":
 		return newShuffleRouterWithStore(config, store), nil
diff --git a/routers/latency.go b/routers/latency.go
index caa65a6..63daf05 100644
--- a/routers/latency.go
+++ b/routers/latency.go
@@ -4,7 +4,6 @@ import (
 	"context"
 	"sort"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
@@ -34,7 +33,7 @@ func NewLatencyRouterWithConfig(config router.Config) *LatencyRouter {
 }
 
 // newLatencyRouterWithStore creates a new latency router with optional distributed StatsStore.
-func newLatencyRouterWithStore(config router.Config, store internalRouter.StatsStore) *LatencyRouter {
+func newLatencyRouterWithStore(config router.Config, store router.StatsStore) *LatencyRouter {
 	config.Strategy = router.StrategyLowestLatency
 	var base *BaseRouter
 	if store != nil {
diff --git a/routers/leastbusy.go b/routers/leastbusy.go
index 9c0dfd8..20651cb 100644
--- a/routers/leastbusy.go
+++ b/routers/leastbusy.go
@@ -3,7 +3,6 @@ package routers
 import (
 	"context"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
@@ -36,7 +35,7 @@ func NewLeastBusyRouterWithConfig(config router.Config) *LeastBusyRouter {
 }
 
 // newLeastBusyRouterWithStore creates a new least busy router with optional distributed StatsStore.
-func newLeastBusyRouterWithStore(config router.Config, store internalRouter.StatsStore) *LeastBusyRouter {
+func newLeastBusyRouterWithStore(config router.Config, store router.StatsStore) *LeastBusyRouter {
 	config.Strategy = router.StrategyLeastBusy
 	var base *BaseRouter
 	if store != nil {
diff --git a/routers/shuffle.go b/routers/shuffle.go
index bb057b1..6a5b704 100644
--- a/routers/shuffle.go
+++ b/routers/shuffle.go
@@ -3,7 +3,6 @@ package routers
 import (
 	"context"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
@@ -32,7 +31,7 @@ func NewShuffleRouterWithConfig(config router.Config) *ShuffleRouter {
 }
 
 // newShuffleRouterWithStore creates a new shuffle router with optional distributed StatsStore.
-func newShuffleRouterWithStore(config router.Config, store internalRouter.StatsStore) *ShuffleRouter {
+func newShuffleRouterWithStore(config router.Config, store router.StatsStore) *ShuffleRouter {
 	config.Strategy = router.StrategySimpleShuffle
 	var base *BaseRouter
 	if store != nil {
diff --git a/routers/tagbased.go b/routers/tagbased.go
index 5027d22..e3cbcd5 100644
--- a/routers/tagbased.go
+++ b/routers/tagbased.go
@@ -3,7 +3,6 @@ package routers
 import (
 	"context"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
@@ -48,7 +47,7 @@ func NewTagBasedRouterWithConfig(config router.Config) *TagBasedRouter {
 }
 
 // newTagBasedRouterWithStore creates a new tag-based router with optional distributed StatsStore.
-func newTagBasedRouterWithStore(config router.Config, store internalRouter.StatsStore) *TagBasedRouter {
+func newTagBasedRouterWithStore(config router.Config, store router.StatsStore) *TagBasedRouter {
 	config.Strategy = router.StrategyTagBased
 	config.EnableTagFiltering = true
 	var base *BaseRouter
diff --git a/routers/tpmrpm.go b/routers/tpmrpm.go
index 1a97a0f..818b2a6 100644
--- a/routers/tpmrpm.go
+++ b/routers/tpmrpm.go
@@ -3,7 +3,6 @@ package routers
 import (
 	"context"
 
-	internalRouter "github.com/blueberrycongee/llmux/internal/router"
 	"github.com/blueberrycongee/llmux/pkg/provider"
 	"github.com/blueberrycongee/llmux/pkg/router"
 )
@@ -36,7 +35,7 @@ func NewTPMRPMRouterWithConfig(config router.Config) *TPMRPMRouter {
 }
 
 // newTPMRPMRouterWithStore creates a new TPM/RPM router with optional distributed StatsStore.
-func newTPMRPMRouterWithStore(config router.Config, store internalRouter.StatsStore) *TPMRPMRouter {
+func newTPMRPMRouterWithStore(config router.Config, store router.StatsStore) *TPMRPMRouter {
 	config.Strategy = router.StrategyLowestTPMRPM
 	var base *BaseRouter
 	if store != nil {
diff --git a/routers/types.go b/routers/types.go
index 47358a6..1886931 100644
--- a/routers/types.go
+++ b/routers/types.go
@@ -15,6 +15,13 @@ type (
 	RequestContext   = router.RequestContext
 	ResponseMetrics  = router.ResponseMetrics
 	Strategy         = router.Strategy
+	StatsStore       = router.StatsStore
+)
+
+// Re-export error variables
+var (
+	ErrStatsNotFound     = router.ErrStatsNotFound
+	ErrStoreNotAvailable = router.ErrStoreNotAvailable
 )
 
 // Re-export constants
diff --git a/tests/testutil/server.go b/tests/testutil/server.go
index 3aae67e..6bc7d04 100644
--- a/tests/testutil/server.go
+++ b/tests/testutil/server.go
@@ -16,8 +16,10 @@ import (
 	"github.com/blueberrycongee/llmux/internal/config"
 	"github.com/blueberrycongee/llmux/internal/metrics"
 	"github.com/blueberrycongee/llmux/internal/provider"
-	"github.com/blueberrycongee/llmux/internal/router"
+	pkgprovider "github.com/blueberrycongee/llmux/pkg/provider"
+	"github.com/blueberrycongee/llmux/pkg/router"
 	"github.com/blueberrycongee/llmux/providers/openai"
+	"github.com/blueberrycongee/llmux/routers"
 )
 
 // TestServer manages a LLMux server instance for testing.
@@ -160,7 +162,7 @@ func NewTestServer(opts ...ServerOption) (*TestServer, error) {
 	registry.RegisterFactory("openai", provider.AdaptFactory(openai.NewFromConfig))
 
 	// Initialize router with no cooldown for testing
-	simpleRouter := router.NewSimpleShuffleRouter(router.RouterConfig{
+	simpleRouter := routers.NewShuffleRouterWithConfig(router.Config{
 		CooldownPeriod: 0,
 	}) // No cooldown in tests
 
@@ -183,7 +185,7 @@ func NewTestServer(opts ...ServerOption) (*TestServer, error) {
 
 			// Register deployments for each model
 			for _, model := range p.Models {
-				deployment := &provider.Deployment{
+				deployment := &pkgprovider.Deployment{
 					ID:            fmt.Sprintf("%s-%s", p.Name, model),
 					ProviderName:  p.Name,
 					ModelName:     model,
@@ -213,7 +215,7 @@ func NewTestServer(opts ...ServerOption) (*TestServer, error) {
 
 		// Register deployments
 		for _, model := range options.models {
-			deployment := &provider.Deployment{
+			deployment := &pkgprovider.Deployment{
 				ID:            fmt.Sprintf("mock-openai-%s", model),
 				ProviderName:  "mock-openai",
 				ModelName:     model,
